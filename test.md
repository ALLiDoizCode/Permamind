{
  "input" : "@permamind what is arlink?",
  "mocks" : {
    "queryPermawebDocs" : [
      {
        "text" : "{\"query\":\"arlink\",\"results\":[{\"content\":\"# Permaweb Documentation Collection\\n\\nGenerated on: 2025-07-03T19:52:53.629Z\\nTotal documents: 65\\nTotal words: 21106\\n\\n## Table of Contents\\n\\n### Included Documents\\n\\n1. [Deploy and Register Atomic Assets using ArDrive CLI](https://cookbook.arweave.net/guides/smartweave/atomic-assets/ardrive-cli.html)\\n2. [Hello World (CLI)](https://cookbook.arweave.net/getting-started/quick-starts/hw-cli.html)\\n3. [Github Action](https://cookbook.arweave.net/guides/deployment/github-action.html)\\n4. [SmartWeave](https://cookbook.arweave.net/concepts/smartweave.html)\\n5. [Data Model](https://cookbook.arweave.net/concepts/arfs/data-model.html)\\n6. [Atomic Tokens](https://cookbook.arweave.net/guides/atomic-tokens/intro.html)\\n7. [arlocal](https://cookbook.arweave.net/guides/testing/arlocal.html)\\n8. [Overview](https://cookbook.arweave.net/concepts/psts.html)\\n9. [Querying Transactions](https://cookbook.arweave.net/concepts/queryTransactions.html)\\n10. [Warp (SmartWeave) SDK - Deploying Contracts](https://cookbook.arweave.net/guides/smartweave/warp/deploying-contracts.html)\\n11. [Warp (SmartWeave) SDK Intro](https://cookbook.arweave.net/guides/smartweave/warp/intro.html)\\n12. [Arseeding Js](https://cookbook.arweave.net/guides/deploying-manifests/arseeding-js.html)\\n13. [ArNS - Arweave Name System](https://cookbook.arweave.net/concepts/arns.html)\\n14. [Warp (SmartWeave) SDK - ReadState](https://cookbook.arweave.net/guides/smartweave/warp/readstate.html)\\n15. [Posting Transactions](https://cookbook.arweave.net/concepts/post-transactions.html)\\n16. [Vue Starter Kits](https://cookbook.arweave.net/kits/vue/index.html)\\n17. [Svelte Starter Kits](https://cookbook.arweave.net/kits/svelte/index.html)\\n18. [ServerSide DNS Integration](https://cookbook.arweave.net/guides/dns-integration/server-side.html)\\n19. [Transaction Metadata (Tags)](https://cookbook.arweave.net/concepts/tags.html)\\n20. [ar-gql](https://cookbook.arweave.net/guides/querying-arweave/ar-gql.html)\\n21. [Posting Transactions using arseedingjs](https://cookbook.arweave.net/guides/posting-transactions/arseeding-js.html)\\n22. [Querying Arweave with GraphQL](https://cookbook.arweave.net/guides/querying-arweave/queryingArweave.html)\\n23. [Bundling](https://cookbook.arweave.net/references/bundling.html)\\n24. [Wallets and Keys](https://cookbook.arweave.net/concepts/keyfiles-and-wallets.html)\\n25. [Posting Transactions using arweave-js](https://cookbook.arweave.net/guides/posting-transactions/arweave-js.html)\\n26. [React Starter Kits](https://cookbook.arweave.net/kits/react/index.html)\\n27. [arkb](https://cookbook.arweave.net/guides/deployment/arkb.html)\\n28. [Entity Types](https://cookbook.arweave.net/concepts/arfs/entity-types.html)\\n29. [Vouch](https://cookbook.arweave.net/concepts/vouch.html)\\n30. [Arweave peer HTTP API](https://cookbook.arweave.net/references/http-api.html)\\n31. [Warp WriteInteractions](https://cookbook.arweave.net/guides/smartweave/warp/write-interactions.html)\\n32. [Cooking with the Permaweb](https://cookbook.arweave.net/index.html)\\n33. [Permaweb Applications](https://cookbook.arweave.net/concepts/permawebApplications.html)\\n34. [Cooking with the Permaweb](https://cookbook.arweave.net/getting-started/index.html)\\n35. [Fetching Transaction Data](https://cookbook.arweave.net/guides/http-api.html)\\n36. [Vouch](https://cookbook.arweave.net/guides/vouch.html)\\n37. [Privacy](https://cookbook.arweave.net/concepts/arfs/privacy.html)\\n38. [Hello World (NodeJS)](https://cookbook.arweave.net/getting-started/quick-starts/hw-nodejs.html)\\n39. [Developing on the Permaweb](https://cookbook.arweave.net/getting-started/welcome.html)\\n40. [Gateways](https://cookbook.arweave.net/concepts/gateways.html)\\n41. [Atomic Assets](https://cookbook.arweave.net/guides/smartweave/atomic-assets/index.html)\\n42. [ArDB](https://cookbook.arweave.net/guides/querying-arweave/ardb.html)\\n43. [Path Manifests](https://cookbook.arweave.net/concepts/manifests.html)\\n44. [Welcome to the Permaweb](https://cookbook.arweave.net/concepts/permaweb.html)\\n45. [Permaweb Cookbook - Guides](https://cookbook.arweave.net/guides/index.html)\\n46. [Creating and Deploying Manifests](https://cookbook.arweave.net/guides/deploying-manifests/deployingManifests.html)\\n47. [Permaweb Cookbook - Core Concepts](https://cookbook.arweave.net/concepts/index.html)\\n48. [Bundling Services](https://cookbook.arweave.net/concepts/bundlers.html)\\n49. [ArFS Protocol A Decentralized File System on Arweave](https://cookbook.arweave.net/concepts/arfs/arfs.html)\\n50. [Ardrive](https://cookbook.arweave.net/guides/deploying-manifests/ardrive.html)\\n51. [Contributing Workflow](https://cookbook.arweave.net/getting-started/contributing.html)\\n52. [Permaweb Cookbook - Community](https://cookbook.arweave.net/community/index.html)\\n53. [Posting Transactions using Ardrive Turbo](https://cookbook.arweave.net/guides/posting-transactions/turbo.html)\\n54. [Permaweb Cookbook - References](https://cookbook.arweave.net/references/index.html)\\n55. [Posting a Transaction using Dispatch](https://cookbook.arweave.net/guides/posting-transactions/dispatch.html)\\n56. [Complete GraphQL Structure for Transactions](https://cookbook.arweave.net/references/gql.html)\\n57. [Schema Diagrams](https://cookbook.arweave.net/concepts/arfs/schema-diagrams.html)\\n58. [Hello World (No Code)](https://cookbook.arweave.net/getting-started/quick-starts/hw-no-code.html)\\n59. [Content Types](https://cookbook.arweave.net/concepts/arfs/content-types.html)\\n60. [Warp (SmartWeave) SDK - Evolve](https://cookbook.arweave.net/guides/smartweave/warp/evolve.html)\\n61. [Transaction Bundles](https://cookbook.arweave.net/concepts/bundles.html)\\n62. [Search Indexing Service](https://cookbook.arweave.net/guides/querying-arweave/search-indexing-service.html)\\n63. [Arweave App](https://cookbook.arweave.net/guides/deploying-manifests/arweave-app.html)\\n64. [Permaweb Cookbook - Legacy](https://cookbook.arweave.net/legacy/index.html)\\n65. [Starter Kits](https://cookbook.arweave.net/kits/index.html)\",\"domain\":\"arweave\",\"relevanceScore\":17,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/arweave-llms.txt\"},{\"content\":\"ANS-104: A standard for bundled data on Arweave, allowing for efficient storage of multiple pieces of data in a single transaction. It forms the basis for scalable data storage and retrieval on the network.\\nMessaging Patterns: Communication frameworks in AO that define how processes exchange information through message passing. These patterns include asynchronous fire-and-forget messaging (ao.send), request-response cycles (ao.send().receive()), message forwarding chains (msg.forward), and various blocking and non-blocking communication models. Messaging patterns provide structured approaches for inter-process communication, enabling complex distributed systems to coordinate effectively.\\nao.send: A non-blocking function in AO for asynchronous message sending that enables fire-and-forget communication between processes. It returns immediately after sending, allowing the sending process to continue execution without waiting for a response. The function returns a promise-like object that can be chained with .receive() for request-response patterns when needed.\\nao.send().receive(): A blocking message pattern in AO that combines sending a message and waiting for a specific reply, enabling request-response cycles between processes. It only matches messages linked by X-Reference and can specify a target process ID to indicate which process will reply. This pattern is essential for synchronous interactions in the otherwise asynchronous AO environment.\\nmsg.reply: A non-blocking function in AO handlers used to respond to incoming messages with automatic reference tracking. It enables asynchronous request-response patterns by automatically linking the response to the original message via X-Reference and setting the Target to the original sender or Reply-To address if specified.\\nmsg.forward: A non-blocking function in AO for message routing across multiple processes that creates a sanitized copy of the original message. It preserves Reply-To and X-Reference properties for complete message tracking and sets X-Origin to the original sender, enabling final services to reply directly to the originator. This function is key for creating multi-step processing pipelines (A → B → C → A patterns).\\nReceive: A blocking function in AO (with capital R) that s execution until any message matching a specified pattern arrives from any sender. Unlike the lowercase .receive() method, it's not bound to a specific conversation and can match messages from any process, making it useful for synchronous message processing flows or event listening.\\nHandlers.utils.reply: A utility function in AO that creates a handler function that automatically replies with a fixed response. It serves as a wrapper around msg.reply for common use cases, enabling concise creation of simple response handlers without writing full handler functions.\\nReference: A unique identifier automatically assigned to each message in AO that enables tracking message history and relationships. References form the foundation of AO's message correlation system, allowing processes to maintain conversation context across asynchronous communications.\\nReply-To: A message property in AO that explicitly specifies the destination for responses, overriding the default behavior of replying to the original sender. This enables more flexible routing patterns where responses can be directed to different processes than the message originator.\\nX-Reference: A special message property in AO that maintains conversation chains across replies and forwards. It links related messages together in a conversation, enabling processes to track which response corresponds to which request, even in complex multi-step processing pipelines.\\nX-Origin: A message property in AO that tracks the original sender of a message through forwarding chains. It enables final services in a processing pipeline to reply directly to the conversation originator without knowing its specific identity in advance, facilitating multi-step processing workflows.\\nBlocking Communication: A messaging approach in AO where the sending process s execution until a response is received. This pattern is implemented through functions like Receive and ao.send().receive(), creating synchronous interaction points in an otherwise asynchronous system. Blocking is useful for scenarios requiring guaranteed order of operations or direct response handling.\\nNon-blocking Communication: A messaging approach in AO where the sending process continues execution immediately after sending a message, without waiting for responses. Implemented through functions like ao.send, msg.reply, and msg.forward, this pattern enables parallel processing and higher throughput in distributed systems by avoiding execution s.\\nProcess: A computational unit in AO that can receive and send messages. Processes are represented by a log of interacting messages stored on Arweave, as well as an initialization data item.\\nMessage: The fundamental unit of communication in AO and the AO Core Protocol. In AO, messages are ANS-104 compliant data items that can be sent between processes or from users to processes. In the AO Core Protocol, every item on the permaweb is described as a Message, interpretable as either a map of named functions or as a concrete binary term. Messages can be called through the creation of another message, providing a map of arguments to the execution. Both systems use messages as the primary mechanism for communication and state management.\\nHandler: A function in an AO process that responds to specific message patterns. Handlers are registered to match particular message patterns and execute code when matching messages are received.\\nResolvers: Special tables in AO handlers where each key is a pattern matching table and its corresponding value is a function that executes when that pattern matches. Resolvers enable conditional execution based on additional pattern matching, creating switch/case-like structures where different functions are triggered based on which pattern matches the incoming message.\\nPattern Matching Tables: Declarative structures in AO that define how to match incoming messages based on their attributes. These tables support simple tag matching, wildcard matching with underscore ('_'), Lua string pattern matching (similar to regex), and function-based validation. They provide a flexible way to filter messages for processing by specific handlers.\\nHandlers.utils.hasMatchingData: A utility function in AO that returns a pattern matching function for checking if a message's Data field contains a specified string. This helper simplifies creating handlers that respond to specific message content rather than just tags, enabling content-based message routing and processing.\\nHandlers.utils.hasMatchingTag: A utility function in AO that returns a pattern matching function for checking if a message has a tag with a specified name and value. This helper enables precise matching of messages based on their tag attributes, facilitating targeted message handling and routing in AO processes.\\nHandlers.add: A core function in AO for registering message handlers that takes a name, pattern, and handler function as arguments. It adds or updates a handler in the process's handler list, enabling processes to respond to specific message patterns. The pattern can be a table, function, or string (in AOS 2.0+), and the handler can be a function or resolver table for conditional execution.\\nHandlers.once: A specialized handler registration function in AO that creates a handler which runs only once when its pattern is matched, then is automatically removed. This is equivalent to setting maxRuns = 1 and is useful for one-time initialization or handling unique events that should trigger only a single response.\\nHandler Execution Flow: The process by which AO determines which handlers to run when a message arrives. Handlers are executed in sequential order as they appear in the Handlers.list, with pattern functions determining whether to: skip the handler (false), process the message and continue checking subsequent handlers (\\\"continue\\\"), or process the message and checking further handlers (\\\"break\\\"). This flow control enables sophisticated message processing pipelines within AO processes.\\nArFS: A data modeling, storage, and retrieval protocol designed to emulate common file system operations on Arweave's permanent storage. ArFS solves key challenges by implementing a hierarchical file structure, metadata management, file permissions (public and private with encryption), file versioning, data deduplication, search capabilities, and interoperability with other decentralized applications. It works around Arweave's immutable nature by using an append-only transaction data model with tags in Arweave Transaction headers, enabling features like file renaming and organization despite the underlying permanent storage.\\nSpawn: The action of creating a new process in AO. Processes can spawn other processes, creating a hierarchical relationship.\\nActor Model: The computational paradigm that AO is based on, where each process (actor) is an independent unit with its own state that communicates exclusively through message passing.\\nHyperBEAM: The distributed execution engine that powers AO, built on Erlang/OTP. It provides distributed computation, deterministic execution, message scheduling, and WASM runtime capabilities.\\nArweave Wallet Kit: A modular toolkit that simplifies interactions between Arweave wallets and dApps by providing a unified API that supports any Arweave wallet. It's built around a core package as the foundation, with additional packages for React hooks, components, and styles. The kit uses 'strategies' as modular implementations for different wallet providers, currently supporting Wander.app, Arweave.app, Othent, and general browser wallets. This architecture enables users to interact with apps using their preferred wallet while giving developers a consistent interface across all wallet types.\\nCompute Unit: A node in the AO network responsible for executing process code and managing state. CUs offer the service of resolving process state in competition with one another.\\nScheduler Unit: A node in the AO network responsible for the single assignment of atomically incrementing slot numberings to messages sent to a process, ensuring deterministic ordering.\\nMessenger Unit: A node in the AO network that relays messages around the system, serving as the entry point for users to interact with processes within AO.\\nCron: A time-based job scheduler in AO that enables processes to execute functions on a schedule, allowing for autonomous contract execution.\\nHolographic State: A state management approach in AO where each process maintains its state independently of other processes, represented by a log of messages stored on Arweave. This independence allows processes to operate and be evaluated separately, facilitating faster interactions and improved scalability. The state is considered 'holographic' because it's implied by the associated message log rather than being explicitly stored, enabling true parallel processing while maintaining verifiability.\\nAOS: The AO operating system, which serves as a reference model for software creation on AO. It allows developers to initiate processes, install packages, and edit code within a live environment.\\nArweave: A decentralized storage network that allows users to store data permanently and sustainably. It serves as the foundation layer for the permaweb ecosystem and has its own native cryptocurrency called AR Token that powers the network's economic model.\\nBlockweave: Arweave's innovative data structure that improves upon traditional blockchain technology. Unlike sequential blockchain verification, it requires only random data verification for new additions, creating a 3D spider web-like structure. This approach dramatically reduces energy consumption while maintaining data immutability. The blockweave supports petabyte-scale storage and features continuous data validation (over 5,670 validations daily). It includes advanced features like bundled transactions (up to 1000 per block) and incentivized data replication, enabling efficient storage and verification of large datasets.\\nPermaweb: A permanent, decentralized web built on top of the Arweave network, allowing for the creation of websites and applications that are accessible forever. It represents the user-facing layer of the Arweave ecosystem.\\nAR.IO: The first permanent cloud network. No 404s, no lost dependencies, no subscriptions - just reliable access to applications and data for site and app hosting.\\nAR.IO Gateway: A server that provides access to data stored on the Arweave network. Part of the AR.IO network's decentralized infrastructure for serving permanent data and managing network traffic.\\nBundling: A feature in Arweave that groups multiple data transactions together, processing them as Layer-1 transactions to improve efficiency and reduce costs. This is implemented through the ANS-104 standard.\\nAO Token: The native token of the AO ecosystem, used for resource allocation, governance, and payments within the network. One AO is equal to one trillion (1,000,000,000,000) Armstrongs, which is the smallest unit of the token. When specifying token quantities in transactions, the amount is typically expressed in Armstrongs rather than AO (e.g., 1,000,000 Armstrongs = 0.0000001 AO).\\nPi Token (π): A token in the AO ecosystem representing ownership of key permaweb assets, including AO, AR, and fair-launch projects, serving as a default means of exchange.\\nAO-SQLite: A module for AO that combines the AO operating system with SQLite to create a lightweight but powerful indexer for the AOS experience.\\nArFleet: A framework in AO designed to facilitate the purchase of time-limited data storage from permissionless peers without requiring third-party enforcement.\\nTrusted Execution Environment: A secure area of a processor in AO that guarantees code and data loaded inside is protected with respect to confidentiality and integrity, similar to a CPU with the security of a hardware wallet.\\nArNS: A decentralized domain name system for the permaweb.\\nAssignments: A mechanism in AO that enables processes to access and share data from Arweave. Assignments allow processes to request specific data items by their transaction IDs and distribute messages to multiple processes. They are implemented through the ao module's assign and send functions, supporting features like data loading and cross-process message sharing.\\nao Module: A core library in AO that provides essential functionality for process communication and management. It includes functions for sending messages (ao.send), spawning processes (ao.spawn), managing assignments, and handling process state. The module maintains process environment information and manages message routing through an outbox system.\\nAO Computer: The AO Computer is an actor-oriented machine on the Arweave network, creating a unified computing environment across diverse nodes. It supports many parallel processes through an open message-passing layer, linking independent processes into a cohesive system, similar to how websites are interconnected via hyperlinks.\\nArweave Name Token: A token based on the AO Computer that is connected to each registered ArNS Name. Each ANT gives the owner the ability to update the subdomains and Arweave Transaction IDs used by the registered name as well as transfer ownership and other functions.\\nArweave Network Standards: Drafts and finalized standards for data formats, tag formats, data protocols, custom gateway features and anything that is built on top the Arweave Network. Specific standards are denoted by an associated number, e.g., ANS-###.\\nBase Layer Transaction: One of up to 1,000 transactions that make up a single Arweave block. These transactions form the foundation of the Arweave network's data structure and can contain bundled data items.\\nBundled Data Item: A data item or transaction nested within an ANS-104 bundled transaction. These items enable efficient storage and retrieval of multiple data pieces within the Arweave network.\\nBundler: A third-party service and gateway feature that bundles data files on a user's behalf.\\nChunk: A unit of data that is stored on the Arweave network. It represents a piece of a larger file that has been split into smaller, manageable segments for efficient storage and retrieval.\\nEpoch: A specific duration (e.g., one day) during which network activities and evaluations are conducted. It serves as a key time frame for processes such as observation duties, performance assessments, and reward distributions within the network's protocols.\\nEval: An onboard handler in AO processes that evaluates new code and verifies message origin. It processes incoming code, determines appropriate actions, and can be manually triggered to evaluate the Data field from messages. The Eval handler is also used internally by functions like .load to evaluate file contents.\\nGateway Address Registry: A decentralized directory maintained in the AR.IO smart contract. It serves as the authoritative list of all registered gateways on the AR.IO Network, facilitating discovery, health monitoring, and data sharing.\\nIndexing: The act of organizing transaction data tags into queryable databases.\\nLayer 2 Infrastructure: Technology / infrastructure stack built 'above' a base layer. In this use, the AR.IO Network would be considered Layer 2 infrastructure to the base Arweave protocol.\\nManifest: Special 'aggregate' files uploaded to Arweave that map user-definable sub-paths with other Arweave transaction IDs. This allows users to create logical groups of content, for example a directory of related files, or the files and assets that make up a web page or application.\\nMempool: Short for 'memory pool,' is a component of Arweave mining nodes that temporarily stores valid transactions that have been broadcasted to the network but have not yet been added to a block.\\nNative Address: The way public addresses are commonly (or by spec) represented in their native blockchain. Arweave keys are 43 character base64url representations of the public key, while Ethereum keys use a different hashing algorithm and start with 0x etc.\\nNormalized Address: 43 character base64url representation of the sha256 hash of a public key. Public keys for other chains can be normalized by this representation.\\nObserver: A gateway selected to evaluate the performance of peer gateways in resolving ArNS names. Observers assess and report on the operational efficacy of other gateways.\\nOptimistic Indexing: Indexing transaction or data item headers before the associated L1 transaction has been accepted and confirmed in a chain block.\\nPeriod: A predefined time span (e.g., a day) that serves as a cycle for network activities such as dynamic pricing. It is a fundamental unit of time for operational and protocol processes within the network.\\nPermanent Cloud Network: A decentralized network that securely stores, distributes, and serves data and applications in a timeless, tamper-proof, and universally accessible way. Unlike traditional clouds, it ensures data permanence and user sovereignty by eliminating reliance on centralized providers and creating a resilient, censorship-resistant infrastructure.\\nProcess ID: Every process in AO is assigned a unique immutable identifier code.\\nProtocol Balance: The primary sink and source of ARIO tokens circulating through the AR.IO Network. This balance functions as a programmatically encoded vault in the network's smart contract for managing ArNS revenue and incentive rewards distribution.\\nProtocol Rewards: ARIO Token incentive rewards distributed by the AR.IO protocol to eligible users and gateway operators, forming part of the network's economic incentive structure.\\nPublic Key: The publicly known keys for a signer (wallet). Public keys are different byte lengths depending on the signer type (e.g. Arweave vs. Ethereum (ECDSA), vs Solana, etc.)\\nSeeding: The act of propagating new data throughout the network. Miner nodes seed Arweave base layer transaction data to other miners, while gateways ensure that the transactions they receive reach the Arweave nodes. Both gateways and Arweave nodes seed base layer transactions and data chunks.\\nStaking: The process of locking ARIO tokens into a protocol-facilitated vault, temporarily removing them from circulation until unlocked. This action represents an opportunity cost for the gateway operator and serves as a motivator to prioritize the network's collective interests.\\nStake Redelegation: The process by which stakers move their delegated tokens from one gateway to another.\\nStake Redemption: A feature allowing stakers to use their staked tokens for ArNS-related activities, such as purchasing names, extending leases, or increasing undername capacity.\\nTransaction ID: Every transaction and data file uploaded to Arweave is assigned a unique identifier code known as the Transaction ID.\\nTrust-minimization: Relates to enacting network security by minimizing the number of entities and the degree to which they must be trusted to achieve reliable network interactions. A network with trust-minimizing mechanisms means that it has reduced exposure to undesirable third-party actions and built-in incentives to reward good behavior while punishing bad behavior.\\nVault: Token vaults are protocol level mechanisms used to contain staked tokens over time. Each vault contains a starting timestamp, ending timestamp (if applicable), along with a balance of tokens.\\nWayfinder Protocol: The Wayfinder protocol provides applications with a pattern for dynamically switching / routing between network gateways. It also allows for abstraction of top level domain names from Arweave data and verifies the responses from AR.IO Gateways. It forms the basis of the ar:// schema, so users can seamlessly access ArNS names, Arweave base layer transactions, and bundled data items without the user providing a top-level domain.\\nBootloader: A feature in AO that enables users to include a script to evaluate when spawning a process. The script can be included either with the Data property or with a txId specified on the On-Boot Tag.\\nCoroutine: A programming feature in AO similar to async/await or generators, allowing processes to execution (yield) and resume later. Used for handling asynchronous operations and message passing.\\nHandler Pattern: A mechanism in AO for matching and processing incoming messages based on their attributes. Can be defined using strings, tables, or functions to match message properties like Action tags.\\nResolver: A special table in AO handlers where each key is a pattern matching table and its corresponding value is a function that executes when that pattern matches, enabling conditional execution based on message attributes.\\nWeaveDrive: A protocol (AOP-5) that provides virtual file system support for AO processes to efficiently read data directly from Arweave. It enables processes to access and manipulate data stored on Arweave through a standardized file system interface, with support for lazy loading, caching, and deterministic data access patterns. The protocol includes mechanisms for data attestation, availability verification, and process boot loading.\\nAttestor: An Arweave wallet address authorized to create attestation data items that grant access to specific Arweave data items using WeaveDrive.\\nToken Subledger: A process type in AO that implements the full messaging protocol of token contracts and enables splitting tokens from a parent process into a child process while maintaining fungibility.\\nMessage Reference: A unique identifier assigned to messages in AO that enables tracking message chains and responses across processes, facilitating request-response patterns and message forwarding.\\nMessage Forward: A mechanism in AO that enables passing messages between multiple processes while maintaining reference chains and origin information, useful for creating multi-step processing pipelines.\\nAssignment Check: A security feature in AO that provides whitelisted assignment protection for processes by verifying the trustworthiness of message assignments.\\nAO Environment: The core runtime environment in AO that manages process state, message handling, and execution context. It includes process identification, module references, authorities, and outbox management for coordinating process communication.\\nProcess Authority: A security mechanism in AO that defines which entities (identified by their addresses) are trusted to interact with a process. Authorities can send messages that will be accepted and processed by the process.\\nProcess Outbox: A component of an AO process that temporarily stores outgoing messages, spawns, and assignments before they are processed by the network. The outbox manages the process's communication with other processes.\\nMessage Tags: Metadata attached to AO messages that provide context and instructions for message processing. Tags can be extractable or non-extractable, and forwardable or non-forwardable, affecting how messages are processed and routed.\\nMessage Normalization: The process of standardizing message format in AO by extracting tags into a consistent structure and ensuring proper handling of message attributes. This enables reliable message processing across the network.\\nMessage Sanitization: A security feature in AO that removes non-forwardable tags from messages before forwarding, ensuring that sensitive or system-specific information is not inappropriately propagated through the network.\\nProcess Inbox: A component of an AO process that stores incoming messages for processing. The inbox has a maximum capacity and implements overflow protection to prevent memory issues while maintaining message order.\\nProcess Module: A reusable component in AO that defines the behavior and capabilities of a process. Modules can be referenced and reused across different processes, providing standardized functionality and interfaces.\\nProcess Result: The output of an AO process execution, including any generated messages, spawned processes, assignments, and error information. Results are used to coordinate process communication and manage state changes.\\nSingle System Image: A unified computing environment in AO that creates a cohesive system across diverse nodes in a distributed network. This allows processes to operate independently while maintaining a unified user experience, similar to how websites on different servers are connected via hyperlinks.\\nStake-Exclusivity Period: A time window in AO during which staked collateral is exclusively reserved for a specific message transmission, preventing it from being reused. This mechanism ensures stake availability for potential slashing if discrepancies are found in the message.\\nTime Value of Stake: The economic cost in AO associated with locking stake capital for a specific duration to secure message transmission. This value is calculated based on the opportunity cost of the locked capital and expected annual return rates.\\nPermissionless Ecosystem Funding: A token issuance mechanism in AO that allows developers to fund their applications by receiving AO tokens from users who bridge assets into their protocols. The funding rate automatically adjusts with the token emission curve.\\nPermaweb Ecosystem Development Guild: An alliance of organizations and builders in AO that develop and maintain core infrastructure, funded through native yield generated by bridged assets. The funding naturally decreases with token emission rate.\\nPushing: The process in AO where Messenger Units relay messages through the network, coordinating with Scheduler Units for message ordering and Compute Units for state calculation. This continues recursively until there are no more messages to process.\\nState Attestation: A cryptographic assurance mechanism in AO that verifies the correctness of computations and states. In AO's state management, attestations are cryptographically signed statements by Compute Units verifying process state computation results. In the AO Core Protocol, attestations provide cryptographic assurance that verifies the correctness of an output, representing proof that a specific computation produced the expected result when combined with the message and hashpath. Both forms enable trustless verification without requiring full recomputation.\\nBridge: A system that enables assets from other networks to be represented and used within AO. Bridges consist of smart contracts on both the native chain and AO network, generating derivative tokens that represent the bridged assets. They facilitate cross-chain interoperability and asset transfer between different blockchain networks.\\nAOX: A general-purpose bridge that facilitates the transfer of assets from traditional exchanges and Ethereum to the AO network. It enables users to bring their assets from external networks into the AO ecosystem, expanding the range of assets available for use within AO.\\nQuantum: A bridge developed by Astro Protocol that enables the transfer of AR tokens from the Arweave network to the AO network. It facilitates seamless movement of Arweave's native token into the AO ecosystem, enabling users to leverage their AR holdings within AO-based applications and protocols.\\nCast Message: A type of message in AO that is sent to a Scheduler Unit without waiting for a response, enabling one-way communication patterns. This allows for more efficient message passing when immediate responses aren't needed.\\naoconnect: A JavaScript/TypeScript library that enables interaction with the AO system from Node.js or browser environments. It provides a comprehensive set of tools and utilities for developers to interact with AO processes, send messages, and manage AO resources programmatically.\\nAO Dev-Cli: A command-line tool for building AO WebAssembly modules, supporting both Lua and C/C++ development. It provides a streamlined workflow for initializing projects, building modules, and deploying them to the AO network. The tool features configurable build settings, external library support, and predefined memory presets for different use cases.\\naos console: A command-line interface tool for interacting with AO networks. It allows developers to deploy processes, send messages, and manage AO resources through terminal commands.\\nDataItemSigner: A function used to cryptographically sign data items before they're submitted to the AO network, ensuring authenticity and ownership.\\ndryrun Parameter: An API parameter that allows execution of operations without committing them to the blockchain, useful for testing and validation.\\nRelay Operator: An entity that facilitates message transmission in the AO network, often requiring payment in the form of tokens to relay messages.\\nTag System: The structured protocol for message routing in AO using key-value pairs. Common patterns include Action-Target-Quantity for token transfers and other operations.\\nBalances Implementation: The pattern for implementing token balances within AO processes, typically using a Lua table with wallet addresses as keys.\\nBlueprints: Templates in AOS that streamline the development of distributed applications by providing a framework for creating consistent and efficient processes across the AO network.\\nAR Token: The native cryptocurrency of the Arweave network used for transaction fees, data storage payments, and mining rewards. AR tokens are required to store data on the network, incentivize miners to maintain the blockweave, and participate in network governance. The token has a fixed maximum supply and its value is tied to the network's storage capacity. One AR is equal to one trillion (1,000,000,000,000) Winstons, which is the smallest unit of the token.\\nAtomic Assets: A unique data type on Arweave, linking data, metadata, and a smart contract under one identifier.\\nArweave Gateway: Nodes that provide HTTP access to data stored on the Arweave network.\\nProof of Access: Arweave's consensus mechanism that combines proof of work with proof of storage.\\nSmartWeave: A smart contract platform for Arweave that uses lazy-evaluation.\\nSuccinct Proof of Random Access: Arweave's current mining mechanism that combines proof of work with proof of storage.\\nUniversal Data License: Arweave's on-chain content licensing system.\\nWallet: A software application that allows users to store, send, and receive AR tokens, as well as interact with Arweave applications.\\nWarp: A smart contract execution environment for Arweave.\\nWeave: The collective data stored on the Arweave network.\\nWildfire: Arweave's self-sustaining network participation reward system.\\nAO Package Manager: A package manager for AO, enabling developers to manage and distribute AO modules and dependencies.\\nAO Ventures: A venture capital firm investing in projects within the AO ecosystem.\\nArlink: Arlink is a one click deployment platform built on Arweave, enabling developers to permanently host front-end applications such as React, Next.js, or static websites with a single click. It functions like Vercel but leverages the Arweave Permaweb for censorship-resistant, immutable, and durable hosting.\\nAutonomous Finance: A decentralized finance initiative utilizing AO technologies for autonomous financial operations.\\nBazar: A fully decentralized atomic asset exchange built on the permaweb, leveraging the Universal Content Marketplace (UCM) protocol and AO. It enables content creators to trade digital assets with real-world rights through a trustless orderbook process. As the first user interface operating on the UCM protocol, Bazar facilitates the exchange of various digital content types, from images and music to applications, while ensuring secure and efficient trading through AO processes.\\nBetterIDEa: A native web-based IDE for AO development, featuring Lua language server integration and AO Package Manager support.\\nBotega: A decentralized finance platform built on AO that facilitates asset exchanges through multiple models including AMMs and order books. Its unique architecture treats each liquidity pool as an independent process with localized state, enabling high-throughput transactions, autonomous execution of advanced orders, and AI integration. Botega is fully decentralized with its frontend on Arweave and execution logic in AO processes.\\nCommunity Labs: A collaborative group dedicated to advancing the permaweb ecosystem.\\nLiquidOps: An over collateralised lending and borrowing protocol built on Arweave and AO.\\nDexi: An autonomous application that aggregates real-time financial data from events within the AO network, including asset prices, token swaps, and liquidity fluctuations. The Dexi Terminal provides a web interface similar to TradingView for monitoring assets and trading tokens. AO applications can subscribe to Dexi's data agents to receive verifiable real-time data, eliminating the need for external oracles.\\nForward Research: A research organization exploring and expanding the capabilities of the AO network.\\nLongview Labs: An organization focused on developing decentralized technologies and applications.\\nOdysee: A decentralized video platform utilizing blockchain for content distribution. Currently being migrated to Arweave/AO via ArFleet.\\nPermaswap: A decentralized exchange built on the Arweave network, supporting transactions within the permaweb ecosystem.\\nRedstone: A decentralized data analytics platform built on Arweave.\\nTrackgood: A decentralized supply chain management system using Arweave for data integrity.\\nWander: A non-custodial Arweave and AO native wallet with extensive features. Wander is available as a browser extension, mobile application, and embedded smart account.\\nBeacon: The most secure wallet for AO and AR networks, offering features such as two-factor authentication (2FA), multi-signature capabilities, passkeys, recovery options, and more to ensure robust security.\\nGraphQL for Arweave: A query language used to interact with Arweave data in a flexible and efficient manner, allowing developers to specify exactly what data they need.\\nMulti-sig: A multi-signature wallet or smart contract that requires multiple signatures or approvals from different parties before executing a transaction or action. This enhances security and control by distributing decision-making authority.\\nPermaweb Journal: An on-chain publication and knowledge hub focused on the Arweave and AO ecosystem. It serves as a central resource for news, insights, and developments within the permaweb community.\\nLua: A lightweight, embeddable scripting language used in AO for process development. Lua is utilized within the AO environment for writing process code and interacting with the AO system.\\nUnder_names: ANT owners and controllers can configure multiple subdomains for their registered ArNS name known as \\\"under_names\\\" or more easily written \\\"undernames\\\". These undernames are assigned individually at the time of registration or can be added on to any registered name at any time. Under_names use an underscore \\\"_\\\" in place of a more typically used dot \\\".\\\" to separate the subdomain from the main ArNS domain.\\nPermaweb Glossary: A comprehensive reference guide for the Arweave and AO ecosystem, providing detailed explanations of key terms, concepts, and technologies. It serves as an educational resource for developers, users, and community members to better understand the permaweb ecosystem. A key feature is its portability - the glossary can be embedded in other websites and applications via iframes, making it easily accessible across the permaweb.\\nProtocol.Land: Decentralized, source controlled, code collaboration where you own your code.\\nStablecoin: A type of cryptocurrency designed to maintain a stable value relative to a specific asset or basket of assets, typically a fiat currency like the US dollar. Stablecoins provide price stability while leveraging the benefits of blockchain technology, making them useful for trading, payments, and as a store of value in the crypto ecosystem.\\nUSDA: Astro USD, or USDA, is an AO-native stablecoin used to transact, facilitate, and interact with various other DeFi protocols in the ecosystem, while acting as a store of value, a medium of exchange, and a unit of account. USDA is a stablecoin backed by assets within the Arweave and AO ecosystems. It leverages Arweave's decentralized storage and AO's smart contract-like processes for secure and efficient value exchange, balancing transparency and decentralization.\\nAstro Protocol: A blockchain-based framework that enables the minting of stablecoins and synthetic assets, backed by on-chain collateral stored and managed by processes developed within the AO compute environment. Astro introduces dynamic liquidation processes and stability modules to enhance system resilience and user trust.\\nData Item: A standardized data format defined in the ANS-104 bundle specification, used as the preferred storage format for Arweave bundles. Data items are signed pieces of data that can be bundled together for efficient storage on Arweave. In AO, messages are implemented as data items with specific protocol tags, making them the foundation of process communication and state management.\\nAO Link: A fully decentralized transaction explorer for the AO network (accessible at aolink.ar.io or ao.link) that provides real-time visualization and interaction with on-chain messages and computations. It features message exploration, graphical visualization of message relationships, manual message interaction capabilities, and real-time streaming of network activity. The platform enables users to view token balances, process inboxes, and inspect process code, offering comprehensive insight into the AO network's operations.\\nAO Teal: A typed version of Lua for AO development, similar to TypeScript for JavaScript, that adds static type checking and annotations to enhance code safety and reliability. It includes features for secure coding practices and integrates with a biginteger implementation for precise handling of large numbers in financial applications. Teal enables developers to write more robust and maintainable code for AO processes.\\nAO Form: A declarative infrastructure management tool for the AO ecosystem, similar to Terraform, that enables developers to deploy and upgrade multiprocess systems through a single configuration file. It provides state management for deployments, efficient component upgrades, and streamlined infrastructure maintenance, making it essential for managing complex AO process networks and optimizing development workflows.\\nBasejump: A social gaming platform built on AO that features the Action substrate. It provides a no-code environment for creating, discovering, and monetizing interoperable games and assets. Basejump is consumer-focused with an avatar-centric interface, featuring a PVP battle game for hyperobjects that runs in Discord and on the web.\\nAO Core Protocol: The foundational execution protocol of AO that enables decentralized computing and communication between nodes. AO Core provides a framework into which any number of different computational models, encapsulated as primitive devices, can be attached. It serves as the base layer for the AO ecosystem, offering universal primitives for decentralized computations including hashpaths for state-space referencing, unified data structures for program states, attestation mechanisms, and a meta-VM for executing various computational models. HyperBEAM is a modular system that implements this protocol, while AO builds an actor-based environment on top of it. The protocol is designed to be computer-native to internet technologies, particularly compatible with HTTP protocols. Every item on the permaweb is described as a Message, which can be interpreted as either a map of named functions or as a concrete binary term. Messages can be called through the creation of another message, providing a map of arguments to the execution. The protocol does not enforce any forms of consensus or particular virtual machine, instead focusing on offering the simplest possible representation of data and computation.\\nAsynchronous Message Passing: A communication paradigm where senders don't wait for receivers to be ready, allowing for non-blocking operations and better scalability. This enables efficient process communication in distributed systems.\\nCheckpoint: A saved state of a process that can be used to resume execution from a known point, used for persistence and recovery. This enables process state preservation and restoration capabilities.\\nDecentralized Execution: The ability to run processes across a distributed network without centralized control or coordination. This enables truly distributed computing capabilities in the HyperBEAM ecosystem.\\nErlang: The programming language used to implement the HyperBEAM core, known for its robustness and support for building distributed, fault-tolerant applications. It provides the foundation for HyperBEAM's reliable message passing and process management.\\n~json-iface@1.0: A device in HyperBEAM that offers a translation layer between the JSON-encoded message format used by legacy versions and HyperBEAM's native HTTP message format.\\nHTTP Message Signature: A standardized method (RFC9421) for creating, encoding, and verifying digital signatures within HTTP messages, crucial for the security model of HyperBEAM and the AO Core protocol. In HyperBEAM, HTTP Signed Messages are used to authenticate node configuration changes, verify message integrity, and establish trust between nodes. The hb_message module in HyperBEAM provides functionality to convert between different message formats, including HTTP Signed Messages, enabling secure communication across the decentralized network. This signature mechanism is fundamental to the attestation system that allows nodes to cryptographically prove the correctness of computations without requiring full recomputation.\\n~p4@1.0: A device that runs as a pre-processor and post-processor in HyperBEAM, enabling a framework for node operators to sell usage of their machine's hardware to execute AO Core devices.\\nSam Williams: The founder of Arweave and the creator of the AO Computer concept. He pioneered the development of the permaweb and decentralized storage technologies, leading the creation of both the Arweave network and the AO ecosystem. His vision has been instrumental in advancing decentralized computing and permanent data storage solutions.\\nMetalinks: A decentralized link hub platform that enables users to create and manage their web presence without intermediaries. Metalinks provides a web3-native solution where users can own their content and control their digital presence, embodying the principles of decentralization and user sovereignty. The platform allows users to create their own decentralized link hub, offering pure web3 freedom without traditional gatekeepers or centralized control.\\nFair Launch: A cryptocurrency distribution method that ensures equal access and sustainable funding for projects without traditional gatekeepers or early investor advantages. In the AO ecosystem, fair launches enable projects to receive funding through the Permaweb Index, where users can allocate their AO yield to support development. This model promotes transparency and decentralization by avoiding pre-allocation, pre-mining, or insider advantages, while ensuring builders can secure sustainable funding. Projects launched this way receive tokens based on community allocation of yield, creating a more equitable system where success is tied directly to community support and utility rather than speculative investment.\\nConverge Protocol: The codename for AO Core Protocol. See AO Core Protocol for the complete definition.\\nDevice: A component in the AO Core Protocol that specifies how a message should be interpreted. Each message may optionally state a Device which should be used by Converge-compatible systems to interpret its contents. If no Device is explicitly stated, it must be inferred as 'Message'. Every Device must implement functions with the names 'ID' and 'Keys'.\\nHashpath: A cryptographic memoization of the tree of executions that were the source of a given piece of data in the AO Core Protocol. It is derived by cryptographically mixing two prior commitments, resulting in a short (32 bytes with SHA2-256) but verifiable reference to the entire computation tree.\\nStack Device: A special device in HyperBEAM that combines a series of devices on a message into a single 'stack' of executable transformations. When added as the highest Device tag on a message, it scans the remainder of the message's tags looking for and loading other messages it finds, passing through each element of the stack in turn during execution.\\nAttestation: A cryptographic assurance mechanism in AO that verifies the correctness of computations and states. In AO's state management, attestations are cryptographically signed statements by Compute Units verifying process state computation results. In the AO Core Protocol, attestations provide cryptographic assurance that verifies the correctness of an output, representing proof that a specific computation produced the expected result when combined with the message and hashpath. Both forms enable trustless verification without requiring full recomputation.\\nPath: A way to reference messages in the AO Core Protocol that starts from a given message and applies a series of additional messages on top of it. Each resulting message must have an ID resolvable via its device, enabling additional paths to be described atop the intermediate message. Paths are designed to be compatible with HTTP protocols.\\nPermissionlessness: A property of computation systems where no actor in the ecosystem may be denied the ability to use the network by any other actor or group. This is a fundamental property of decentralized computation machines, though it can only be offered in degrees rather than absolutely.\\nTrustlessness: A property of computation systems where users can participate in the network without needing to trust other parties are not acting maliciously. Like permissionlessness, this is a fundamental property of decentralized computation machines that can only be offered in degrees rather than absolutely.\\n~meta@1.0: A configuration device in HyperBEAM that provides an interface for specifying a node's hardware, supported devices, metering, and payments information. It allows external clients to find and validate node configurations in the network, serving as the primary configuration mechanism for HyperBEAM nodes.\\n~relay@1.0: A device in HyperBEAM used to relay messages between nodes and the wider HTTP network. It provides an interface for sending and receiving messages using various execution strategies, facilitating communication across the network.\\n~wasm64@1.0: A device in HyperBEAM that executes WebAssembly code using the Web Assembly Micro-Runtime (WAMR). It enables the execution of WASM modules from any other device and supports devices written in languages like Rust, C, and C++.\\n~compute-lite@1.0: A lightweight device in HyperBEAM that wraps a local WASM executor, used for executing legacy AO processes. It enables compatibility with older AO network processes within the HyperBEAM environment.\\n~snp@1.0: A device in HyperBEAM used to generate and validate proofs that a node is executing inside a Trusted Execution Environment (TEE). It enables trust-minimized attestations of AO Core executions using ephemeral key pairs that exist only within the TEE.\\nTrustless Execution: A property of computation systems where users can participate in the network without needing to trust other parties are not acting maliciously. Like permissionlessness, this is a fundamental property of decentralized computation machines that can only be offered in degrees rather than absolutely.\\n~simple-pay@1.0: A simple, flexible pricing device in HyperBEAM that can be used with p4@1.0 to offer flat-fees for the execution of AO Core messages. It provides a straightforward mechanism for monetizing node services.\\n~faff@1.0: A simple pricing and ledger device for p4@1.0 in HyperBEAM that allows nodes to offer access to their services only to a specific set of users. It's useful for personal use or servicing specific applications.\\nscheduler@1.0: A device in HyperBEAM used to assign a linear hashpath to an execution, ensuring deterministic ordering that all users can access. It enables the creation of executions that mirror traditional smart contracting networks when used with other AO Core devices.\\nstack@1.0: A device in HyperBEAM used to execute an ordered set of devices over the same inputs. It allows users to create complex combinations of other devices and apply them as a single unit with a single hashpath.\\n~process@1.0: A device in HyperBEAM that enables users to create persistent, shared executions accessible by multiple users. It allows customization of execution and scheduler devices, supporting various execution patterns. It includes a push key for moving messages from a process's execution outbox into another execution's schedule.\\nARIO Token: The native cryptocurrency of the AR.IO Network that powers the world's first Permanent Cloud. With a fixed supply of 1 billion tokens, ARIO serves multiple functions: powering ecosystem operations, facilitating transactions, and incentivizing network participation. It enables critical operations like gateway staking, ArNS domain registrations, and protocol rewards distribution. The token follows a non-inflationary model where network revenue is recycled to fund rewards rather than minting new tokens. Each ARIO is divisible into 1,000,000 micro-ARIO (µARIO) units to support a wide range of transactions.\\nArmstrong: The smallest unit of the AO Token. One AO is equal to one trillion (1,000,000,000,000) Armstrongs. When specifying token quantities in AO transactions, the amount is typically expressed in Armstrongs rather than AO tokens.\\nWinston: The smallest unit of the AR Token (Arweave). One AR is equal to one trillion (1,000,000,000,000) Winstons. This denomination is similar to satoshis in Bitcoin and is used for precise calculations and transactions within the Arweave network.\\nmicro-ARIO: The smallest unit of the ARIO Token. One ARIO is equal to one million (1,000,000) micro-ARIO. This subunit allows for precise calculations and transactions within the AR.IO Network, enabling granular token operations for services like gateway staking and ArNS domain registrations.\\nAction: A decentralized framework built on AO and Arweave designed for creating self-sovereign games, digital assets, and communities. Action introduces 'hyperobjects' (interoperable game assets like avatars, items, and environments) that can be used across different games and platforms. The ecosystem is powered by the ACTION token, which enables hyperobject creation, governance participation, and economic coordination. Action emphasizes originality, collaboration, and sustainability, with a portion of fees directed toward real-world environmental initiatives like rainforest protection.\\nACTION Token: The native utility token of the Action ecosystem with a fixed supply of 10 billion tokens. It enables hyperobject creation, marketplace transactions, governance participation, and ecosystem growth. Unlike traditional tokens, ACTION is designed primarily for platform utility rather than speculation, with 75% of the supply allocated to community distribution. The token follows a non-inflationary model and incorporates treasury mechanisms to fund environmental and social initiatives through community governance.\\nHyperobject: The foundational digital assets of the Action ecosystem that serve as building blocks for game, commerce, and creativity. Hyperobjects include avatars ('Action Figures'), game assets ('Action Items'), and game environments ('Worlds'). Unlike traditional NFTs, hyperobjects can be large applications due to AO's scalability, and they exhibit context-dependent behavior, adapting dynamically to the specific game or world in which they're used. Each hyperobject runs as a unique AO process with corresponding Arweave data representing its underlying assets.\\nMerkle Root: The result of hashing all transactions in a block, pairing those hashes, and hashing them again until a single hash remains. In blockchain systems like Arweave and AO, Merkle roots enable efficient verification of transaction data without requiring nodes to process every transaction individually. This technique significantly reduces the computational resources needed to verify blocks and is central to maintaining blockchain integrity. In the AO Core Protocol, HashPaths use Merkle tree principles to create a cryptographic history of all operations that led to a particular state, allowing verification of the entire computation chain with a single hash.\\nAO Yield: A customizable reward system in the AO ecosystem that allows users to earn tokens based on their Arweave (AR) holdings and deposited assets (like stETH and DAI). Users can allocate their yield to different options: PI (receiving diversified exposure through the Permaweb Index), AO (continuing to earn AO tokens), AR (converting yield to Arweave tokens), or specific fair launch projects. When allocated to ecosystem projects, the yield helps fund development while users receive project tokens in return. The system operates through trustless, audited contracts on Ethereum for bridged assets, with deposits remaining withdrawable at any time.\\nPermaweb Index: A fair launch funding mechanism for the permaweb ecosystem, launched on Pi Day (3.14). It introduces a new model for funding crypto projects where users can allocate their AO yield to support ecosystem development. The index is represented by the PI token, which provides diversified exposure to key permaweb assets: 33.3% AO, 33.3% Arweave (AR), and 33.3% fair launch projects. This structure enables builders to secure sustainable development funding while allowing community members to support projects they believe in, all while maintaining broad exposure to the ecosystem's growth through a single token.\\nArDrive: A decentralized storage application built on Arweave that enables users to store and manage their files permanently. It offers a pay-once, store-forever model, eliminating the need for recurring subscription fees. ArDrive provides true data ownership, ensuring files remain accessible even if the company disappears, and supports both private and public data storage options. The platform is optimized for storing various types of data, including photography, digital art, and NFTs.\\nArDrive CLI: A command-line interface tool for interacting with ArDrive, enabling users to upload, download, and manage files on the Arweave network programmatically. It provides a powerful way to automate file operations and integrate ArDrive functionality into scripts and applications.\\nArDrive Turbo: An enhanced version of ArDrive that provides faster upload speeds and improved performance for large files. It optimizes the upload process by implementing advanced bundling and chunking techniques, making it more efficient for handling substantial amounts of data.\\nArDrive Core: The fundamental library that provides the core functionality for interacting with ArDrive. It includes essential features for file management, data encryption, and Arweave integration, serving as the foundation for both the ArDrive web application and CLI tools.\\nRug Pull: A malicious practice in cryptocurrency where project developers abandon the project and take investors' funds, or deliberately manipulate token value before selling their holdings, leaving other investors with worthless tokens. AO's Fair Launch model prevents rug pulls by eliminating pre-mining, pre-allocation, and insider advantages. Instead, projects receive funding through the Permaweb Index based on community-allocated yield, ensuring that token distribution is tied directly to community support and utility rather than speculative investment. This model removes the ability for developers to suddenly dump large token holdings, as tokens are earned gradually through actual development and community backing.\\nLegacynet: A development and testing network for the AO computer launched on February 27, 2024. It allows developers and early adopters to interact with the AO computer without fees, providing a sandbox environment for testing and building applications before deploying to mainnet. The network includes features like the aos console for process management and native community chat servers for developer collaboration. It serves as a crucial platform for exploring AO's hyper parallel computing capabilities and building towards mainnet deployment.\\nRandAO: A novel decentralized random number generation protocol built on AO that achieves robust security through an 'any honest' guarantee - ensuring randomness integrity as long as at least one participant remains honest. It integrates Verifiable Delay Functions (VDFs) with a commit-reveal scheme to prevent denial of service attacks and manipulation attempts while maintaining unpredictable output generation.\\nVerifiable Delay Function: A cryptographic primitive that requires a specified number of sequential steps to evaluate (and hence a theoretical amount of time), yet produces a unique output that can be efficiently verified. VDFs feature sequential computation that cannot be significantly accelerated even with parallel processing, while allowing for efficient verification of results. In RandAO, VDFs prevent denial of service attacks by ensuring that once commitments are made, the final entropy value is deterministically locked but requires time to calculate.\\nCommit Reveal Scheme: A decentralized protocol for generating verifiable random numbers where providers first commit to entropy values by submitting their hashes, then reveal the original values which are verified against the commitments. The naive implementation suffers from a critical any-malicious denial of service vulnerability that RandAO addresses through VDF integration.\\n$RNG Token: The native utility token of the RandAO ecosystem that captures the value of decentralized random number generation. All randomness requests must be paid for in $RNG tokens, establishing continuous demand. The token economy creates a marketplace where providers compete on service quality and earn rewards through both direct service payments and staking mechanisms.\\nPietrzak's VDF: An elegant implementation of a Verifiable Delay Function using the RSW timelock puzzle. The construction is defined by an RSA modulus (N), input value (x), number of squaring operations (T), and number of checkpoints (n). It creates checkpoints that split verification into equal segments that can be verified independently, reducing verification time through parallelization.\\nRSW Timelock Puzzle: A cryptographic construction developed by Rivest, Shamir, and Wagner that requires sequential computation to solve, making it impossible to significantly accelerate even with parallel processing. In RandAO, it serves as the foundation for Pietrzak's VDF implementation, creating a deterministic time delay between commitment and revelation phases to prevent denial of service attacks in randomness generation.\\nParallel Processing: A fundamental capability in AO where an arbitrary number of processes can execute simultaneously and independently. Unlike traditional blockchain networks where computation happens in a single global state, AO processes maintain independent states and can operate separately, sharing available computational resources efficiently. During idle times, resources are automatically reallocated to active processes, maximizing throughput and enabling scalability mechanics similar to traditional web2 systems. This parallel architecture, combined with holographic state management, forms the foundation of AO's hyper parallel computing capabilities.\\nHyper Parallel Computing: The ability to execute multiple processes simultaneously and independently, allowing for parallel execution of tasks and improved overall system performance. This is a key feature of AO's architecture, enabling efficient utilization of computational resources and faster processing times.\\nDeflationary: A property of Arweave's tokenomics where the effective token supply decreases over time due to the storage endowment mechanism. As users pay for permanent storage, tokens are locked in the endowment, and due to the storage cost decline rate (historically averaging 38% per year), fewer tokens need to be released from the endowment than were initially deposited. This creates a natural token sink that reduces circulating supply proportionate to network usage, strengthening the token's value proposition without extracting value from users.\\nEndowment Simulation: A computational model that projects the behavior of Arweave's storage endowment over time based on variables like storage cost decline rate, token prices, and network usage. This tool helps users understand the long-term sustainability of the permanent storage economics system.\\nNon-value Extractive Mechanism: A tokenomic design where costs to users represent actual value (in Arweave's case, storage security) rather than rent extraction. This creates alignment between user interests and protocol economics, as users pay for reliability rather than arbitrary fees. This mechanism is fundamental to Arweave's Permanent Storage Economics model.\\nPermanent Storage Economics: The economic model underlying Arweave's permanent storage solution, where upfront payments create a Storage Endowment that generates returns to fund ongoing storage costs. This model, as a non-value extractive mechanism, treats storage costs not as extraction but as purchased security, with higher costs correlating to greater reliability and permanence. The system's sustainability is projected through the endowment simulation tool.\\nStorage Cost Decline Rate: The annual percentage decrease in data storage costs due to technological advancements. Historically averaging around 38% per year over the past 50 years (as of 2025), this rate directly impacts Arweave's tokenomics by reducing the number of tokens that need to be released from the Storage Endowment over time to maintain stored data.\\nStorage Endowment: A financial mechanism in Arweave where users pay upfront for long-term storage (approximately 200 years at current prices). These tokens are removed from circulation and held in the endowment, only to be released if/when needed to pay for ongoing storage costs. This creates a natural token sink that reduces circulating supply proportionate to network usage.\\nToken Sink: A mechanism in the Arweave ecosystem that removes tokens from circulation, effectively reducing the available supply. In Arweave's case, the Storage Endowment functions as a token sink, as tokens paid for storage are locked away for extended periods. This mechanism contributes to the deflationary nature of the AR token.\\nTabM: A unified data format in HyperBEAM standing for 'Type Annotated Binary Messages'. TabM serves as the intermediary representation that all message formats can convert to and from, enabling seamless communication between different encoding schemes. It includes type annotations that preserve the data types of fields (such as integers, strings, and binaries), allowing for accurate message transformation across the system. TabM is a core component of HyperBEAM's message processing pipeline, bridging the gap between various message formats while preserving data integrity.\\nCodecs: Modular components in HyperBEAM that convert between different message formats and the unified TabM format. Codecs solve interoperability challenges in distributed systems by using TabM as an intermediate format, reducing complexity from N² direct conversions to N codecs for N formats. This maintains data integrity and type safety while supporting various encodings (ANS-104, HTTP Signatures, JSON, flat formats). The architecture makes the system extensible - adding a new format only requires one codec that converts to/from TabM, rather than creating conversions for every existing format. This enables HyperBEAM to seamlessly communicate across different systems while maintaining consistent message semantics.\\nFlat Format: A simple key-value encoding scheme in HyperBEAM where data is represented as a series of 'key: value' pairs, each on a new line. This format is particularly used in configuration files and certain message types due to its simplicity and human readability. For example, a flat format message might look like 'Content-Type: text/plain\\\\nAction: update\\\\nValue: 123'. Unlike nested formats like JSON, flat formats have no hierarchy or nesting capabilities, making them easier to parse but less expressive. In HyperBEAM, flat formats are often used for configuration files and basic message headers where nested structures aren't necessary.\\nPATCH: A method in HyperBEAM that enables efficient state updates by modifying only specific parts of data structures rather than replacing entire objects. When sent with method=patch in AO, it allows processes to update their state incrementally, significantly reducing latency and computational overhead. PATCH works with HyperBEAM's caching system to provide high-performance state management even for legacy processes. This mechanism is particularly valuable for applications handling large datasets or requiring frequent small updates, as it minimizes network traffic and processing time by transmitting only the changed portions of state.\",\"domain\":\"permaweb-glossary\",\"relevanceScore\":15,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/permaweb-glossary-llms.txt\"},{\"content\":\"# 36. Intro to AO-Core - HyperBEAM - Documentation\\n\\nDocument Number: 36\\nSource: https://hyperbeam.arweave.net/build/introduction/what-is-ao-core.html\\nWords: 435\\nQuality Score: 0.440\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nWhat is AO-Core? Your browser does not support the video tag.AO-Core is a protocol and standard for distributed computation that forms the foundation of the AO Computer. Inspired by and built upon concepts from the Erlang language, AO-Core embraces the actor model for concurrent, distributed systems. It defines a minimal, generalized model for decentralized computation built around standard web technologies like HTTP.Think of it as a way to interpret the Arweave permaweb not just as static storage, but as a dynamic, programmable, and infinitely scalable computing environment. Unlike traditional blockchain systems, AO-Core defines a flexible, powerful computation protocol that enables a wide range of applications beyond just running Lua programs.Core Concepts AO-Core revolves around three fundamental components: Messages Modular Data Packets Messages are cryptographically linked, forming a verifiable computation graph. Devices Extensible Execution Engines AO-Core introduces a modular architecture centered around Devices. These are pluggable components—typically implemented as modules—that define specific computational logic, such as executing WASM, managing state, or relaying data. Devices interpret and process messages, allowing for flexible and extensible computation. This design enables developers to extend the system by creating custom Devices to fit their specific needs, making the network highly adaptable and composable. Paths Composable Pipelines Paths in AO-Core are structures that link messages over time, creating a verifiable history of computations. They allow users to navigate the computation graph and access specific states or results. AO-Core leverages HashPaths —cryptographic fingerprints representing the sequence of operations leading to a specific message state—ensuring traceability and integrity. This pathing mechanism enables developers to compose complex, verifiable data pipelines and interact with processes and data in a flexible, trustless manner.Key Features AO-Core is inherently resilient, running across a global network of machines that eliminates any single point of failure. Its computations are permanent, immutably stored on Arweave so they can be recalled—or continued—at any time. The protocol remains permissionless, meaning anyone can participate. And it is trustless, with every state mathematically verifiable so no central authority is required.The Actor Model in AO Inspired by Erlang, AO-Core implements the actor model to provide a foundation for inherently concurrent, distributed, and scalable systems. In this model, computation is performed by independent actors (or processes). These actors communicate exclusively by passing messages to one another, and each can make local decisions, send more messages, and create new actors.Beyond Processes While AO Processes (smart contracts built using the AO-Core protocol) are a powerful application, AO-Core itself enables a much broader range of computational patterns:Serverless functions with trustless guarantees Hybrid applications combining smart contracts and serverless functionality Custom execution environments through new devices Composable systems using the path language\",\"domain\":\"hyperbeam\",\"relevanceScore\":14,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/hyperbeam-llms.txt\"},{\"content\":\"# 4. SmartWeave  Cooking with the Permaweb\\n\\nDocument Number: 4\\nSource: https://cookbook.arweave.net/concepts/smartweave.html\\nWords: 891\\nQuality Score: 0.574\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nSmartWeave ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.What is SmartWeave?SmartWeave is the name given to the dominant SmartContract paradigm on Arweave. A unique property of SmartWeave contracts is that the current state of the contract is provided by a process of \\\"lazy evaluation\\\". This means that instead of Arweave mining nodes constantly evaluating the current state of all contracts, a client reading a contract evaluates the state for themselves.Why is SmartWeave important?The state and logic of decentralized applications need to be as censorship-resistant, permanent, and verifiable as the rest of their data. SmartWeave enables developers to write smart contracts that encapsulate their apps state and logic on-chain and execute it in a trustless verifiable way. This is no small feat as the Arweave protocol does not include incentives for nodes to evaluate smart contract state for the network.SmartWeave provides an immutable append-only pattern for contract interactions that leverage permanent storage to hold onto their state. The result is a fully decentralized on-chain state machine that can give protocols and applications dynamic functionality in a permissionless and trustless way. By using SmartWeave, developers can create smart contracts that are stored on Arweave and are guaranteed not to change over time. This allows them to build Permaweb applications with dynamic functionality that can be used in a permissionless and trustless manner.There are several reasons why developers might choose to use SmartWeave to implement the logic for their permaweb applications:Decentralized storage: SmartWeave is built on Arweave, which means that applications created using SmartWeave will be stored on a distributed network of nodes rather than on a centralized server. This can make them more resistant to censorship, tampering, and other forms of interference.Lazy evaluation: The lazy evaluation feature of SmartWeave contracts allows for efficient and scaleable execution. Instead of Arweave nodes constantly evaluating the state of a contract, the client reading the contract is responsible for evaluating the state, leveraging the users processing power instead of the networks nodes.Language support: SmartWeave supports a range of programming languages, including JavaScript, TypeScript, Rust, Go, AssemblyScript, and WASM (WebAssembly). This allows developers to use the language they are most familiar with when creating SmartWeave applications.Data durability: Arweave is designed to store data in a way that makes it highly durable and long-lasting. This can be useful for applications that need to store data over a long period of time, such as historical records or scientific data.Economic model: Arweave uses a unique economic model based on the concept of permanent storage that incentivizes miners to store data indefinitely. This can help ensure the long-term viability and durability of permaweb applications created using SmartWeave.How does SmartWeave Work?SmartWeave contracts, at their core, are built from an initial contract state, with edits, additions, and subtractions using transaction tags.SmartWeave SDK's such as Warp (previously RedStone), are used to query for these transactions to build contract state locally, modifying the contract state with each transaction. The Evaluator (Warp) uses tags to query for a contracts transactions; It knows a transaction is part of the contract by way of the App-Name tag, and the Contract tag.Here is an example of a contract interaction.The App-Name says its a Smartweave ACTION.The Contract tag gives the specific transaction ID of the initial contract state.The Input tag gives the contract its function to execute and any other data it needs:[\\n{\\nname:\\\"App-Name\\\"\\nvalue:\\\"SmartWeaveAction\\\"\\n},\\n{\\nname:\\\"App-Version\\\"\\nvalue:\\\"3.0\\\"\\n},\\n{\\nname:\\\"Contract\\\"\\nvalue:\\\"pyM5amizQRN2VlcVBVaC7QzlguUB0p3O3xx9JmbNW48\\\"\\n},\\n{\\nname:\\\"Input\\\"\\nvalue:\\\"{\\n\\\"function\\\":\\\"setRecord\\\",\\n\\\"subDomain\\\":\\\"@\\\",\\n\\\"transactionId\\\":\\\"lfaFgcoBT8auBrFJepLV1hyiUjtlKwVwn5MTjPnTDcs\\\"\\n}\\\"\\n}\\n] And here is an example of a contract.The App-Name says its a Smartweave CONTRACT The Contract-Src tag points to the source code of the contract:[\\n{\\nkey:\\\"App-Name\\\"\\nvalue:\\\"SmartWeaveContract\\\"\\n},\\n{\\nkey:\\\"App-Version\\\"\\nvalue:\\\"3.0\\\"\\n},\\n{\\nkey:\\\"Contract-Src\\\"\\nvalue:\\\"JIIB01pRbNK2-UyNxwQK-6eknrjENMTpTvQmB8ZDzQg\\\"\\n},\\n{\\nkey:\\\"SDK\\\"\\nvalue:\\\"RedStone\\\"\\n},\\n{\\nkey:\\\"Content-Type\\\"\\nvalue:\\\"application/json\\\"\\n}\\n] The resulting state is the current contract state, which the SDK on the client side can use to calculate user balances, contract owners, and other contract specific details. Once the caller has a validated contract state they can build an interaction for the user to deploy to the chain, which upon mining or indexing on a Gateway will be included the next time someone builds the contract state.For a comprehensive overview of the SmartWeave Protocol, its leading implementation Warp Contracts, and more, head to Warp Academy. Dive into step-by-step tutorials, explore advanced concepts, and uncover how SmartWeave power up the permaweb!Smartweave ecosystem projects There's quite a few ecosystem projects leveraging SmartWeave SmartContracts, but here are some of notes:Implementations Warp | Main provider of SmartWeave SDK's, tutorials, and helps maintain the SmartWeave protocol.MEM | Molecular Execution Machine (MEM) is a developer platform that powers the creation and usage of highly available and highly performant applications within a decentralized environment.Tools SonAr | SmartWeave contract explorer, created and hosted by Warp.Resources Warp Academy | A one- shop for all things SmartWeave Apps Permapages | Permanent webpage creation tool, ArNS purchase portal, and ANT creation portal. Your profile on the permaweb.ArNS | Arweave Name System WeaveDB | NoSQL Database as a Smart Contract.KwilDB | SQL Database as a Smart Contract.ArDrive Inferno | Get PST's for uploading thru Ardrive.FirstBatch | FirstBatch aids developers and enterprises in creating personalized, private, and distortion-free AI applications.Othent | Web3 transactions with existing traditional social logins.BazAR | Digital content marketplace with real-world rights.Alex the Archieve | A decentralized archival platform utilizing Arweave's immutable storage.and so much more.\",\"domain\":\"arweave\",\"relevanceScore\":13,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/arweave-llms.txt\"},{\"content\":\"# Permaweb Documentation Collection\\n\\nGenerated on: 2025-07-03T19:50:48.588Z\\nTotal documents: 90\\nTotal words: 36805\\n\\n## Table of Contents\\n\\n### Included Documents\\n\\n1. [Processes](https://cookbook_ao.arweave.net/concepts/processes.html)\\n2. [HyperBEAM from AO Connect](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/ao-connect.html)\\n3. [Editor setup](https://cookbook_ao.arweave.net/guides/aos/editor.html)\\n4. [Editor setup](https://cookbook_ao.arweave.net/references/editor-setup.html)\\n5. [Legacynet HyperBEAM](https://cookbook_ao.arweave.net/welcome/legacynet-info/index.html)\\n6. [Creating a Pingpong Process in aos](https://cookbook_ao.arweave.net/guides/aos/pingpong.html)\\n7. [Reading Dynamic State](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/reading-dynamic-state.html)\\n8. [AO Processes](https://cookbook_ao.arweave.net/welcome/ao-processes.html)\\n9. [Connecting to specific ao nodes](https://cookbook_ao.arweave.net/guides/aoconnect/connecting.html)\\n10. [Understanding the Inbox](https://cookbook_ao.arweave.net/guides/aos/inbox-and-handlers.html)\\n11. [Automated Responses](https://cookbook_ao.arweave.net/tutorials/bots-and-games/attacking.html)\\n12. [ao](https://cookbook_ao.arweave.net/guides/aos/modules/ao.html)\\n13. [Fetching Game State](https://cookbook_ao.arweave.net/tutorials/bots-and-games/game-state.html)\\n14. [DataItem Signers](https://cookbook_ao.arweave.net/guides/aoconnect/signers.html)\\n15. [Handlers (Version 005)](https://cookbook_ao.arweave.net/references/handlers.html)\\n16. [Messages](https://cookbook_ao.arweave.net/concepts/messages.html)\\n17. [BetterIDEa](https://cookbook_ao.arweave.net/references/betteridea/index.html)\\n18. [Connecting to HyperBEAM with aos](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/aos-with-hyperbeam.html)\\n19. [Customizing the Prompt in aos](https://cookbook_ao.arweave.net/guides/aos/prompt.html)\\n20. [Messaging in ao](https://cookbook_ao.arweave.net/tutorials/begin/messaging.html)\\n21. [Crafting a Token](https://cookbook_ao.arweave.net/tutorials/begin/token.html)\\n22. [Building a Token in ao](https://cookbook_ao.arweave.net/guides/aos/token.html)\\n23. [Troubleshooting using aolink](https://cookbook_ao.arweave.net/guides/aos/troubleshooting.html)\\n24. [Preparations](https://cookbook_ao.arweave.net/tutorials/begin/preparations.html)\\n25. [Exposing Process State to HyperBEAM](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/exposing-process-state.html)\\n26. [How ao messaging works](https://cookbook_ao.arweave.net/concepts/how-it-works.html)\\n27. [Get started in 5 minutes](https://cookbook_ao.arweave.net/welcome/getting-started.html)\\n28. [Sending a Message to a Process](https://cookbook_ao.arweave.net/guides/aoconnect/sending-messages.html)\\n29. [aoconnect](https://cookbook_ao.arweave.net/guides/aoconnect/aoconnect.html)\\n30. [JSON](https://cookbook_ao.arweave.net/guides/aos/modules/json.html)\\n31. [Getting started with SQLite](https://cookbook_ao.arweave.net/guides/snacks/sqlite.html)\\n32. [AO Dev-Cli 01](https://cookbook_ao.arweave.net/guides/dev-cli/index.html)\\n33. [Meet Web Assembly](https://cookbook_ao.arweave.net/references/wasm.html)\\n34. [Building a Chatroom in aos](https://cookbook_ao.arweave.net/tutorials/begin/chatroom.html)\\n35. [A whistle stop tour of Lua](https://cookbook_ao.arweave.net/concepts/lua.html)\\n36. [Pretty](https://cookbook_ao.arweave.net/guides/aos/modules/pretty.html)\\n37. [aos Brief Tour](https://cookbook_ao.arweave.net/concepts/tour.html)\\n38. [FAQ](https://cookbook_ao.arweave.net/guides/aos/faq.html)\\n39. [Reading results from an ao Process](https://cookbook_ao.arweave.net/guides/aoconnect/reading-results.html)\\n40. [Bots and Games](https://cookbook_ao.arweave.net/tutorials/bots-and-games/index.html)\\n41. [Eval](https://cookbook_ao.arweave.net/concepts/eval.html)\\n42. [Introduction](https://cookbook_ao.arweave.net/guides/aos/intro.html)\\n43. [Lets Play A Game](https://cookbook_ao.arweave.net/tutorials/bots-and-games/ao-effect.html)\\n44. [Chatroom Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/chatroom.html)\\n45. [Using WeaveDrive](https://cookbook_ao.arweave.net/guides/snacks/weavedrive.html)\\n46. [Tokengating the Chatroom](https://cookbook_ao.arweave.net/tutorials/begin/tokengating.html)\\n47. [Units](https://cookbook_ao.arweave.net/concepts/units.html)\\n48. [Calling DryRun](https://cookbook_ao.arweave.net/guides/aoconnect/calling-dryrun.html)\\n49. [Token Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/token.html)\\n50. [Tutorials](https://cookbook_ao.arweave.net/tutorials/index.html)\\n51. [Community Resources](https://cookbook_ao.arweave.net/references/community.html)\\n52. [Installing ao connect](https://cookbook_ao.arweave.net/guides/aoconnect/installing-connect.html)\\n53. [Lua Optimization Guide for AO Platform](https://cookbook_ao.arweave.net/references/lua-optimization.html)\\n54. [Meet Lua](https://cookbook_ao.arweave.net/references/lua.html)\\n55. [aos AO Operating System](https://cookbook_ao.arweave.net/guides/aos/index.html)\\n56. [ao Module](https://cookbook_ao.arweave.net/references/ao.html)\\n57. [Messaging Patterns in ao](https://cookbook_ao.arweave.net/references/messaging.html)\\n58. [Introduction to AO-Core](https://cookbook_ao.arweave.net/welcome/ao-core-introduction.html)\\n59. [ao Specs](https://cookbook_ao.arweave.net/concepts/specs.html)\\n60. [Voting Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/voting.html)\\n61. [ao Token and Subledger Specification](https://cookbook_ao.arweave.net/references/token.html)\\n62. [Spawning a Process](https://cookbook_ao.arweave.net/guides/aoconnect/spawning-processes.html)\\n63. [LLMs Documentation](https://cookbook_ao.arweave.net/llms-explanation.html)\\n64. [Mechanics of the Arena](https://cookbook_ao.arweave.net/tutorials/bots-and-games/arena-mechanics.html)\\n65. [References](https://cookbook_ao.arweave.net/references/index.html)\\n66. [Welcome to ao](https://cookbook_ao.arweave.net/welcome/index.html)\\n67. [Begin An Interactive Tutorial](https://cookbook_ao.arweave.net/tutorials/begin/index.html)\\n68. [Accessing Data from Arweave with ao](https://cookbook_ao.arweave.net/references/data.html)\\n69. [Home](https://cookbook_ao.arweave.net/)\\n70. [Expanding the Arena](https://cookbook_ao.arweave.net/tutorials/bots-and-games/build-game.html)\\n71. [Utils](https://cookbook_ao.arweave.net/guides/aos/modules/utils.html)\\n72. [Base64](https://cookbook_ao.arweave.net/guides/aos/modules/base64.html)\\n73. [Bringing it Together](https://cookbook_ao.arweave.net/tutorials/bots-and-games/bringing-together.html)\\n74. [Staking Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/staking.html)\\n75. [Interpreting Announcements](https://cookbook_ao.arweave.net/tutorials/bots-and-games/announcements.html)\\n76. [Strategic Decisions](https://cookbook_ao.arweave.net/tutorials/bots-and-games/decisions.html)\\n77. [Blueprints](https://cookbook_ao.arweave.net/guides/aos/blueprints/index.html)\\n78. [Load Lua Files with load filename](https://cookbook_ao.arweave.net/guides/aos/load.html)\\n79. [CRED Utils Blueprint](https://cookbook_ao.arweave.net/guides/aos/blueprints/cred-utils.html)\\n80. [Installing aos](https://cookbook_ao.arweave.net/guides/aos/installing.html)\\n81. [Monitoring Cron](https://cookbook_ao.arweave.net/guides/aoconnect/monitoring-cron.html)\\n82. [Guides](https://cookbook_ao.arweave.net/guides/index.html)\\n83. [Concepts](https://cookbook_ao.arweave.net/concepts/index.html)\\n84. [Cron Messages](https://cookbook_ao.arweave.net/references/cron.html)\\n85. [CLI](https://cookbook_ao.arweave.net/guides/aos/cli.html)\\n86. [Modules](https://cookbook_ao.arweave.net/guides/aos/modules/index.html)\\n87. [Why Migrate to HyperBEAM](https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/why-migrate.html)\\n88. [crypto](https://cookbook_ao.arweave.net/guides/aos/modules/crypto.html)\\n89. [Release Notes](https://cookbook_ao.arweave.net/releasenotes/index.html)\\n90. [Sending an Assignment to a Process](https://cookbook_ao.arweave.net/guides/aoconnect/assign-data.html)\",\"domain\":\"ao\",\"relevanceScore\":13,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ao-llms.txt\"},{\"content\":\"# 14. Intro to HyperBEAM - HyperBEAM - Documentation\\n\\nDocument Number: 14\\nSource: https://hyperbeam.arweave.net/build/introduction/what-is-hyperbeam.html\\nWords: 307\\nQuality Score: 0.509\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nSkip to content\\nWhat is HyperBEAM? HyperBEAM is the primary, production-ready implementation of the AO-Core protocol, built on the robust Erlang/OTP framework. It serves as a decentralized operating system, powering the AO Computer —a scalable, trust-minimized, distributed supercomputer built on permanent storage of Arweave.Implementing AO-Core HyperBEAM transforms the abstract concepts of AO-Core—Messages, Devices, and Paths—into a concrete, operational system. It provides the runtime environment and essential services to execute these\\ncomputations across a network of distributed nodes. Messages Modular Data Packets In HyperBEAM, every interaction within the AO Computer is handled as a message. A message is a binary item or a map of functions. These cryptographically-linked data units are the foundation for communication, allowing processes to trigger computations, query state, and transfer value. HyperBEAM nodes are responsible for routing and processing these messages according to the rules of the AO-Core protocol. Devices Extensible Execution Engines HyperBEAM introduces a uniquely modular architecture centered around Devices. These pluggable components are Erlang modules that define specific computational logic—like running WASM, managing state, or relaying data—allowing for unprecedented flexibility. This design allows developers to extend the system by creating custom Devices to fit their specific computational needs. Paths Composable Pipelines HyperBEAM exposes a powerful HTTP API that uses structured URL patterns to interact with processes and data. This pathing mechanism allows developers to create verifiable data pipelines, composing functionality from multiple devices into a single, atomic request. The URL bar effectively becomes a command-line interface for AO's trustless compute environment.A Robust and Scalable Foundation Built on the Erlang/OTP framework, HyperBEAM provides a robust and secure foundation that leverages the BEAM virtual machine for exceptional concurrency, fault tolerance, and scalability. This abstracts away underlying hardware, allowing diverse nodes to contribute resources without compatibility issues. The system governs how nodes coordinate and interact, forming a decentralized network that is resilient and permissionless.\",\"domain\":\"hyperbeam\",\"relevanceScore\":13,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/hyperbeam-llms.txt\"},{\"content\":\"# Permaweb Documentation Collection\\n\\nGenerated on: 2025-07-03T19:52:09.983Z\\nTotal documents: 125\\nTotal words: 78208\\n\\n## Table of Contents\\n\\n### Included Documents\\n\\n1. [ariowayfinder-react](https://docs.ar.io/wayfinder/react)\\n2. [AO Compute Unit (CU)](https://docs.ar.io/gateways/cu)\\n3. [Arns Viewer](https://docs.ar.io/build/guides/arns-viewer)\\n4. [Arns Viewer](https://docs.ar.io/guides/arns-viewer)\\n5. [Wayfinder Protocol](https://docs.ar.io/concepts/wayfinder)\\n6. [Deploy a dApp with ArDrive web](https://docs.ar.io/guides/ardrive-web)\\n7. [Deploy a dApp with ArDrive web](https://docs.ar.io/build/guides/ardrive-web)\\n8. [Permaweb Deploy](https://docs.ar.io/guides/permaweb-deploy)\\n9. [Permaweb Deploy](https://docs.ar.io/build/guides/permaweb-deploy)\\n10. [Linux Setup](https://docs.ar.io/gateways/linux-setup)\\n11. [Advanced](https://docs.ar.io/gateways/advanced)\\n12. [Advanced Config](https://docs.ar.io/gateways/ar-io-node/advanced-config.html)\\n13. [Telemetry](https://docs.ar.io/wayfinder/telemetry)\\n14. [Upgrading](https://docs.ar.io/gateways/upgrading)\\n15. [Uploading to Arweave](https://docs.ar.io/guides/uploading-to-arweave)\\n16. [Gql](https://docs.ar.io/guides/gql)\\n17. [Gql](https://docs.ar.io/build/guides/gql)\\n18. [ARIO Gateway Grafana](https://docs.ar.io/gateways/grafana)\\n19. [Windows Setup](https://docs.ar.io/gateways/windows-setup)\\n20. [Gateway Network](https://docs.ar.io/gateways/gateway-network)\\n21. [Getting Started](https://docs.ar.io/wayfinder/getting-started)\\n22. [Managing Undernames](https://docs.ar.io/guides/managing-undernames)\\n23. [Managing Undernames](https://docs.ar.io/build/guides/managing-undernames)\\n24. [Admin](https://docs.ar.io/gateways/admin)\\n25. [useWayfinder](https://docs.ar.io/wayfinder/react/use-wayfinder)\\n26. [Introduction](https://docs.ar.io/introduction)\\n27. [Parquet and ClickHouse Usage Guide](https://docs.ar.io/gateways/parquet)\\n28. [Observation and Incentives](https://docs.ar.io/gateways/observer)\\n29. [Core](https://docs.ar.io/wayfinder/core)\\n30. [wayfinderrequest()](https://docs.ar.io/wayfinder/core/request)\\n31. [Bundler](https://docs.ar.io/gateways/bundler)\\n32. [Importing SQLite Database Snapshots](https://docs.ar.io/gateways/snapshots)\\n33. [Arlink Deploy](https://docs.ar.io/build/guides/arlink)\\n34. [Arlink Deploy](https://docs.ar.io/guides/arlink)\\n35. [ARIO Network Testnet](https://docs.ar.io/guides/testnet)\\n36. [Ar Io Sdk](https://docs.ar.io/ar-io-sdk)\\n37. [Get Arns Returned Name](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-name)\\n38. [Arweave Name System (ArNS)](https://docs.ar.io/arns)\\n39. [Get Arns Returned Names](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-returned-names)\\n40. [ARIO Node Release Notes](https://docs.ar.io/gateways/release-notes)\\n41. [ARIO Node Release Notes](https://docs.ar.io/gateways/release-notes#)\\n42. [Join the Gateway Network](https://docs.ar.io/gateways/join-network)\\n43. [Staking](https://docs.ar.io/staking)\\n44. [getTokenCost](https://docs.ar.io/ar-io-sdk/ario/arns/get-token-cost)\\n45. [Manifests](https://docs.ar.io/concepts/manifests)\\n46. [Register an IP Asset on Arweave](https://docs.ar.io/guides/story)\\n47. [Browser Sandboxing](https://docs.ar.io/concepts/sandboxing)\\n48. [Wayfinder](https://docs.ar.io/wayfinder)\\n49. [Content Moderation](https://docs.ar.io/gateways/moderation)\\n50. [Release Name](https://docs.ar.io/ar-io-sdk/ants/release-name)\\n51. [Gateway Architecture](https://docs.ar.io/gateways)\\n52. [upgradeRecord](https://docs.ar.io/ar-io-sdk/ario/arns/upgrade-record)\\n53. [setUndernameRecord](https://docs.ar.io/ar-io-sdk/ants/set-undername-record)\\n54. [ANT Configuration](https://docs.ar.io/ar-io-sdk/ants/configuration)\\n55. [setBaseNameRecord](https://docs.ar.io/ar-io-sdk/ants/set-base-name-record)\\n56. [getDemandFactorSettings](https://docs.ar.io/ar-io-sdk/ario/arns/get-demand-factor-settings)\\n57. [Get Demand Factor](https://docs.ar.io/ar-io-sdk/ario/arns/get-demand-factor)\\n58. [SimpleCacheRoutingStrategy](https://docs.ar.io/wayfinder/routing-strategies/simple-cache)\\n59. [getRecords](https://docs.ar.io/ar-io-sdk/ants/get-records)\\n60. [setRecord](https://docs.ar.io/ar-io-sdk/ants/set-record)\\n61. [getRegistrationFees](https://docs.ar.io/ar-io-sdk/ario/arns/get-registration-fees)\\n62. [The ARIO Token](https://docs.ar.io/token)\\n63. [Gateway Troubleshooting FAQ](https://docs.ar.io/gateways/troubleshooting)\\n64. [getState](https://docs.ar.io/ar-io-sdk/ants/get-state)\\n65. [Glossary](https://docs.ar.io/glossary)\\n66. [Gateway Providers](https://docs.ar.io/wayfinder/core/gateway-providers)\\n67. [getInfo](https://docs.ar.io/ar-io-sdk/ants/get-info)\\n68. [getArNSRecord](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-record)\\n69. [Cache](https://docs.ar.io/wayfinder/core/gateway-providers/cache)\\n70. [Remove Primary Names](https://docs.ar.io/ar-io-sdk/ants/remove-primary-names)\\n71. [Reassign Name](https://docs.ar.io/ar-io-sdk/ants/reassign-name)\\n72. [getBalance](https://docs.ar.io/ar-io-sdk/ants/get-balance)\\n73. [Gateway ArNS Resolution](https://docs.ar.io/gateways/arns-resolution)\\n74. [Getting Started](https://docs.ar.io/ar-io-sdk/getting-started)\\n75. [getBalances](https://docs.ar.io/ar-io-sdk/ants/get-balances)\\n76. [removeRecord](https://docs.ar.io/ar-io-sdk/ants/remove-record)\\n77. [getArNSReservedNames](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-reserved-names)\\n78. [getArNSReservedName](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-reserved-name)\\n79. [removeUndernameRecord](https://docs.ar.io/ar-io-sdk/ants/remove-undername-record)\\n80. [ARIO Configuration](https://docs.ar.io/ar-io-sdk/ario/configuration)\\n81. [Managing Primary Names](https://docs.ar.io/guides/primary-names)\\n82. [Preferred With Fallback](https://docs.ar.io/wayfinder/routing-strategies/preferred-with-fallback)\\n83. [ARIO Node Filtering System](https://docs.ar.io/gateways/filters)\\n84. [Normalized Addresses](https://docs.ar.io/concepts/normalized-addresses)\\n85. [Hash Verification Strategy](https://docs.ar.io/wayfinder/verification-strategies/hash-verification)\\n86. [Network](https://docs.ar.io/wayfinder/core/gateway-providers/network)\\n87. [Extend Lease](https://docs.ar.io/ar-io-sdk/ario/arns/extend-lease)\\n88. [Static](https://docs.ar.io/wayfinder/core/gateway-providers/static)\\n89. [addController](https://docs.ar.io/ar-io-sdk/ants/add-controller)\\n90. [Events](https://docs.ar.io/wayfinder/core/events)\\n91. [Optimizing Data Handling in ARIO Gateway](https://docs.ar.io/gateways/optimize-data)\\n92. [transfer](https://docs.ar.io/ar-io-sdk/ants/transfer)\\n93. [getCostDetails](https://docs.ar.io/ar-io-sdk/ario/arns/get-cost-details)\\n94. [setKeywords](https://docs.ar.io/ar-io-sdk/ants/set-keywords)\\n95. [approvePrimaryNameRequest](https://docs.ar.io/ar-io-sdk/ants/approve-primary-name-request)\\n96. [wayfinderresolveUrl()](https://docs.ar.io/wayfinder/core/resolve-url)\\n97. [ANTgetLogo()](https://docs.ar.io/ar-io-sdk/ants/get-logo)\\n98. [ARIO Gateway Environment Variables](https://docs.ar.io/gateways/env)\\n99. [getControllers](https://docs.ar.io/ar-io-sdk/ants/get-controllers)\\n100. [Random](https://docs.ar.io/wayfinder/routing-strategies/random)\\n101. [StaticRoutingStrategy](https://docs.ar.io/wayfinder/routing-strategies/static)\\n102. [Increase Undername Limit](https://docs.ar.io/ar-io-sdk/ario/arns/increase-undername-limit)\\n103. [ARIO SDK Release Notes](https://docs.ar.io/ar-io-sdk/release-notes)\\n104. [Remove Controller](https://docs.ar.io/ar-io-sdk/ants/remove-controller)\\n105. [ARIO Documentation](https://docs.ar.io/)\\n106. [Signature Verification Strategy](https://docs.ar.io/wayfinder/verification-strategies/signature-verification)\\n107. [Data Root Verification Strategy](https://docs.ar.io/wayfinder/verification-strategies/data-root-verification)\\n108. [Set Logo](https://docs.ar.io/ar-io-sdk/ants/set-logo)\\n109. [ANTs on Bazar](https://docs.ar.io/guides/ants-on-bazar)\\n110. [ANTs on Bazar](https://docs.ar.io/learn/guides/ants-on-bazar)\\n111. [getOwner](https://docs.ar.io/ar-io-sdk/ants/get-owner)\\n112. [getArNSRecords](https://docs.ar.io/ar-io-sdk/ario/arns/get-arns-records)\\n113. [Set Name](https://docs.ar.io/ar-io-sdk/ants/set-name)\\n114. [Set Ticker](https://docs.ar.io/ar-io-sdk/ants/set-ticker)\\n115. [Set Description](https://docs.ar.io/ar-io-sdk/ants/set-description)\\n116. [getHandlers](https://docs.ar.io/ar-io-sdk/ants/get-handlers)\\n117. [Gateway Apex Domain Content Resolution](https://docs.ar.io/gateways/apex)\\n118. [buyRecord](https://docs.ar.io/ar-io-sdk/ario/arns/buy-record)\\n119. [ARIO Network Composition](https://docs.ar.io/network-composition)\\n120. [Quick Start Guides](https://docs.ar.io/guides)\\n121. [Round Robin](https://docs.ar.io/wayfinder/routing-strategies/round-robin)\\n122. [ARIO Smart Contract](https://docs.ar.io/ario-contract)\\n123. [Fastest Ping](https://docs.ar.io/wayfinder/routing-strategies/fastest-ping)\\n124. [Routing Strategies](https://docs.ar.io/wayfinder/routing-strategies)\\n125. [Verification Strategies](https://docs.ar.io/wayfinder/verification-strategies)\",\"domain\":\"ario\",\"relevanceScore\":12,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ario-llms.txt\"},{\"content\":\"# 40. ARIO Node Release Notes - ARIO Docs\\n\\nDocument Number: 40\\nSource: https://docs.ar.io/gateways/release-notes\\nWords: 7692\\nQuality Score: 0.489\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nAR.IO Release Notes Overview Welcome to the documentation page for the AR.IO gateway release notes. Here, you will find detailed information about each version of the AR.IO gateway, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO gateway. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO gateway change logs. Stay updated with the continuous improvements and advancements in the AR.IO gateway by referring to this page for all release-related information.[Release 41] - 2025-06-30 Added Added preferred chunk GET node URLs configuration via PREFERRED_CHUNK_GET_NODE_URLS environment variable to enable chunk-specific peer prioritization. Preferred URLs receive a weight of 100 for prioritization and the system selects 10 peers per attempt by default.Added hash validation for peer data fetching by including X-AR-IO-Expected-Digest header in peer requests when hash is available, validating peer responses against expected hash, and immediately rejecting mismatched data.Added DOCKER_NETWORK_NAME environment variable to configure the Docker network name used by Docker Compose.Added draft guide for running a community gateway.Added draft data verification architecture document.Changed Removed trusted node fallback for chunk retrieval. Chunks are now retrieved exclusively from peers, with the retry count increased from 3 to 50 to ensure reliability without the trusted node fallback.Fixed Fixed inverted logic preventing symlink creation in FsChunkDataStore.Fixed Content-Length header for range requests and 304 responses, properly setting header for single and multipart range requests and removing entity headers from 304 Not Modified responses per RFC 7232.Fixed MaxListenersExceeded warnings by adding setMaxListeners to read-through data cache.Fixed potential memory leaks in read-through data cache by using once instead of on for error and end event listeners.[Release 40] - 2025-06-23 This is an optional release that primarily improves caching when data is fetched from peers.Added Added experimental flush-to-stable script for manual database maintenance. This script allows operators to manually flush stable chain and data item tables, mirroring the logic of StandaloneSqliteDatabase.flushStableDataItems. WARNING: This script is experimental and directly modifies database contents. Use with caution and ensure proper backups before running.Changed Replaced yesql with custom SQL loader that handles comments better, improving SQL file parsing and maintenance.Switched to SPDX license headers to reduce LLM token usage, making the codebase more efficient for AI-assisted development.Improved untrusted data handling and hash validation in cache operations. The cache now allows caching when a hash is available for validation even for untrusted data sources, but only finalizes the cache when the computed hash matches a known trusted hash. This prevents cache poisoning while still allowing data caching from untrusted sources when the data can be validated.[Release 39] - 2025-06-17 This release enhances observability and reliability with new cache metrics, improved data verification capabilities, and automatic failover between chain data sources. The addition of ArNS-aware headers enables better data prioritization across the gateway network. This is a recommended but not urgent upgrade.Added Added filesystem cache metrics with cycle-based tracking. Two new Prometheus metrics track cache utilization: cache_objects_total (number of objects in cache) and cache_size_bytes (total cache size in bytes). Both metrics include store_type and data_type labels to differentiate between cache types (e.g., headers, contiguous_data). Metrics are updated after each complete cache scan cycle, providing accurate visibility into filesystem cache usage.Added X-AR-IO-Data-Id header to all data responses. This header shows the actual data ID being served, whether from a direct ID request or manifest path resolution, providing transparency about the content being delivered.Added automatic data item indexing when data verification is enabled. When ENABLE_BACKGROUND_DATA_VERIFICATION is set to true, the system now automatically enables data item indexing (ANS104_UNBUNDLE_FILTER) with an always: true filter if no filter is explicitly configured. This ensures bundles are unbundled to verify that data items are actually contained in the bundle associated with the Arweave transaction's data root.Added ArNS headers to outbound gateway requests to enable data prioritization. The generateRequestAttributes function now includes ArNS context headers (X-ArNS-Name, X-ArNS-Basename, X-ArNS-Record) in requests to other gateways and Arweave nodes, allowing downstream gateways to effectively prioritize ArNS data requests.Added configurable Docker Compose host port environment variables (CORE_PORT, ENVOY_PORT, CLICKHOUSE_PORT, CLICKHOUSE_PORT_2, CLICKHOUSE_PORT_3, OBSERVER_PORT) to allow flexible port mapping while maintaining container-internal port compatibility and security.Added Envoy aggregate cluster configuration for automatic failover between primary and fallback chain data sources. The primary cluster (default: arweave.net:443) uses passive outlier detection while the fallback cluster (default: peers.arweave.xyz:1984) uses active health checks. This enables zero-downtime failover between HTTPS and HTTP endpoints with configurable FALLBACK_NODE_HOST and FALLBACK_NODE_PORT environment variables.Changed Streamlined background data retrieval to reduce reliance on centralized sources. The default BACKGROUND_RETRIEVAL_ORDER now only includes chunks,s3, removing trusted-gateways and tx-data from the default configuration. This prioritizes verifiable chunk data and S3 storage for background operations like unbundling.Removed ar-io.net from default trusted gateways list and removed TRUSTED_GATEWAY_URL default value to reduce load on ar-io.net now that P2P data retrieval is re-enabled. Existing deployments with TRUSTED_GATEWAY_URL explicitly set will continue to work for backwards compatibility.[Release 38] - 2025-06-09 This release focuses on data integrity and security improvements, introducing trusted data verification and enhanced header information for data requests. Upgrading to this release is recommended but not urgent.Added Added X-AR-IO-Trusted header to indicate data source trustworthiness in responses. This header helps clients understand whether data comes from a trusted source and works alongside the existing X-AR-IO-Verified header to provide data integrity information. The system now filters peer data by requiring peers to indicate their content is either verified or trusted, protecting against misconfigured peers that may inadvertently serve unintended content (e.g., provider default landing pages) instead of actual Arweave data.Added If-None-Match header support for HTTP conditional requests enabling better client-side caching efficiency. When clients send an If-None-Match header that matches the ETag, the gateway returns a 304 Not Modified response with an empty body, reducing bandwidth usage and improving performance.Added digest and hash headers for data HEAD requests to enable client-side data integrity verification.Added EC2 IMDS (instance-profile) credential support for S3 data access, improving AWS authentication in cloud environments.Added trusted data flag to prevent caching of data from untrusted sources, ensuring only verified and reliable content is stored locally while still allowing serving of untrusted data when necessary.Changed Re-enabled ar-io-peers as fallback data source in configuration for improved data availability.Updated trusted node configuration to use arweave.net as the default trusted node URL.Updated ETag header format to use properly quoted strings (e.g., \\\"hash\\\" instead of hash) following HTTP/1.1 specification standards for improved compatibility with caching proxies and clients.[Release 37] - 2025-06-03 This is a recommended release due to the included observer robustness improvements. It also adds an important new feature - data verification for preferred ArNS names. When preferred ArNS names are set, the bundles containing the data they point to will be locally unbundled (verifying data item signatures), and the data root for the bundle will be compared to the data root in the Arweave chain (establishing that the data is on Arweave). To enable this feature, set your preferred ArNS names, turn on unbundling by setting ANS104_DOWNLOAD_WORKERS and ANS104_UNBUNDLE_WORKERS both to 1, and set your ANS104_INDEX_FILTER to a filter that will match the data items for your preferred names. If you don't know the filter, use {\\\"always\\\": true}, but be aware this will index the entire bundle for the IDs related to your preferred names.Note: this release contains migrations to data.db. If your node appears unresponsive please check core service logs to determine whether migrations are running and wait for them to finish.Added Added prioritized data verification system for preferred ArNS names, focusing computational resources on high-priority content while enabling flexible root transaction discovery through GraphQL fallback support.Added verification retry prioritization system with tracking of retry counts, priority levels, and attempt timestamps to ensure bundles do not get stuck retrying forever.Added improved observer functionality with best-of-2 observations and higher compression for more reliable network monitoring.Added MAX_VERIFICATION_RETRIES environment variable (default: 5) to limit verification retry attempts and prevent infinite loops for consistently failing data items.Added retry logic with exponential backoff for GraphQL queries to handle rate limiting (429) and server errors with improved resilience when querying trusted gateways for root bundle IDs.Changed Updated dependencies: replaced deprecated express-prometheus-middleware with the actively maintained express-prom-bundle library and updated prom-client to v15.1.3 for better compatibility and security.Updated Linux setup documentation to use modern package installation methods, replacing apt-key yarn installation with npm global install and updating Node.js/nvm versions.Improved route metrics normalization with explicit whitelist function for better granularity and proper handling of dynamic segments.Fixed Fixed docker-compose configuration to use correct NODE_MAX_OLD_SPACE_SIZE environment variable name.Fixed production TypeScript build configuration to exclude correct \\\"test\\\" directory path.Fixed Parquet exporter to properly handle data item block_transaction_index exports, preventing NULL value issues.Fixed bundles system to copy root_parent_offset when flushing data items to maintain data integrity.Fixed ClickHouse auto-import script to handle Parquet export not_started status properly.Fixed docker-compose ClickHouse configuration to not pass conflicting PARQUET_PATH environment variable to container scripts.Fixed verification process for data items that have not been unbundled by adding queue bundle support and removing bundle join constraint to ensure proper verification of data items without indexed root parents.[Release 36] - 2025-05-27 This is a recommended but not essential upgrade. The most important changes are the preferred ArNS caching feature for improved performance on frequently accessed content and the observer's 80% failure threshold to prevent invalid reports during network issues.Added Added preferred ArNS caching functionality that allows configuring lists of ArNS names to be cached longer via PREFERRED_ARNS_NAMES and PREFERRED_ARNS_BASE_NAMES environment variables. When configured, these names will be cleaned from the filesystem cache after PREFERRED_ARNS_CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD instead of the standard cleanup threshold (CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD). This is accomplished by maintaining an MRU (Most Recently Used) list of ArNS names in the contiguous metadata cache. When filesystem cleanup runs, it checks this list to determine which cleanup threshold to apply. This feature enables gateway operators to ensure popular or important ArNS names remain cached longer, improving performance for frequently accessed content.Added ArNS headers to responses: X-ArNS-Name, X-ArNS-Basename, and X-ArNS-Record to help identify which ArNS names were used in the resolution.Changed Updated observer to prevent report submission when failure rate exceeds 80%. This threshold helps guard against both poorly operated observers and widespread network issues. In the case of a widespread network issue, the assumption is that most gateway operators are well intentioned and will work together to troubleshoot and restore both observations and network stability, rather than submitting reports that would penalize functioning gateways.Updated default trusted gateway in docker-compose Envoy configuration to ar-io.net for improved robustness and alignment with core service configuration.Improved range request performance by passing ranges directly to getData implementations rather than streaming all data and extracting ranges.Fixed Fixed missing cache headers (X-Cache and other data headers) in range request responses to ensure consistent cache header behavior across all request types.Fixed async streaming for multipart range requests by using async iteration instead of synchronous reads, preventing potential data loss.Fixed ArNS resolution to properly exclude www subdomain from resolution logic.Fixed test reliability issues by properly awaiting stream completion before making assertions.Fixed chunk broadcasting to not await peer broadcasts, as they are best-effort operations.[Release 35] - 2025-05-19 This is a low upgrade priority release. It contains a small caching improvement and routing fix. Upgrading to help test it is appreciated but not essential.Changed Adjusted filesystem data expiration to be based on last request times rather than file access times which may be inaccurate.Adjusted CORS headers to include content-* headers.Fixed Fixed regex used to expose /api-docs when an apex ArNS name is set.[Release 34] - 2025-05-05 Given the resilience provided by adding a second trusted gateway URL, it is recommended that everyone upgrade to this release.Added Added peer list endpoints for retrieving information about Arweave peers and ar.io gateway peers.Added ar-io.net as a secondary trusted gateway to increase data retrieval resilience by eliminating a single point of failure.Added circuit breaker for Arweave peer chunk posting.Changed Created directories for DuckDB and Parquet to help avoid permission issues by the directories being created by containers.Fixed Fixed GraphQL ClickHouse error when returning block ID and timestamp.Fixed the tx-chunks-data-source to throw a proper error (resulting in a 404) when the first chunk is missing rather than streaming a partial response.[Release 33] - 2025-05-05 Added Added a [Parquet and ClickHouse usage guide]. Using ArDrive as an example, it provides step by step instructions about how to bulk load Parquet and configure continuous ingest of bundled data items into ClickHouse. This allows the ar-io-node to support performant GraphQL queries on larger data sets and facilitates sharing indexing work across gateways via distribution of Parquet files.Added support for configurable ArNS 404 pages using either:ARNS_NOT_FOUND_TX_ID: Transaction ID for custom 404 content ARNS_NOT_FOUND_ARNS_NAME: ArNS name to resolve for 404 content Added experimental /chunk/ GET route for serving chunk data by absolute offset either the local cache.Added support for AWS_SESSION_TOKEN in the S3 client configuration.Expanded ArNS OTEL tracing to improve resolution behavior observability.Added support for setting a ClickHouse username and password via the CLICKHOUSE_USERNAME and CLICKHOUSE_PASSWORD environment variable. When using ClickHouse, CLICKHOUSE_PASSWORD should always be set. However, CLICKHOUSE_USERNAME can be left unset. The username default will be used in that case.Added support for configuring the port used to connect to ClickHouse via the CLICKHOUSE_PORT environment variable.Changed Disabled ClickHouse import timing logging by default. It can be enabled via environment variable - DEBUG when running the service standalone or CLICKHOUSE_DEBUG when using Docker Compose Upgraded to ClickHouse 25.4.Fixed Ensure .env is read in clickhouse-import script.[Release 32] - 2025-04-22 Changed Reenabled parallel ArNS resolution with removal of misplaced global limit. Refer to release 30 notes for more details on configuration and rationale.Added a timeout for the last ArNS resolver in ARNS_RESOLVER_PRIORITY_ORDER. It defaults to 30 seconds and is configurable using ARNS_COMPOSITE_LAST_RESOLVER_TIMEOUT_MS. This helps prevent promise build up if the last resolver stalls.Fixed Fixed apex ArNS name handling when a subdomain is present in ARNS_ROOT_HOST.Fixed a case where fork recovery could stall due to early flushing of unstable chain data.Restored observer logs by removing unintentional default log level override in docker-compose.yaml.[Release 31] - 2025-04-11 Changed Improved peer TX header fetching by fetching from a wider range of peers and up/down weighting peers based on success/failure.Fixed Rolled back parallel ArNS resolution changes that were causing ArNS resolution to slow down over time.[Release 30] - 2025-04-04 Added Added support for filtering Winston logs with a new LOG_FILTER environment variable.Example filter: {\\\"attributes\\\":{\\\"class\\\":\\\"ArweaveCompositeClient\\\"}} to only show logs from that class.Use CORE_LOG_FILTER environment variable when running with docker-compose.Added parallel ArNS resolution capability.Configured via ARNS_MAX_CONCURRENT_RESOLUTIONS (default: 1).This foundation enables future enhancements to ArNS resolution and should generally not be adjusted at present.Changed Improved ClickHouse auto-import script with better error handling and continuous operation through errors.Reduced maximum header request rate per second to trusted node to load on community gateways.Optimized single owner and recipient queries on ClickHouse with specialized sorted tables.Used ID sorted ClickHouse table for ID queries to improve performance.Fixed Fixed data alignment in Parquet file name height boundaries to ensure consistent import boundaries.Removed trailing slashes from AO URLs to prevent issues when passing them to the SDK.Only prune SQLite data when ClickHouse import succeeds to prevent data loss during exports.[Release 29] - 2025-03-21 Changed Temporarily default to trusted gateway ArNS resolution to reduce CU load as much possible. On-demand CU resolution is still available as a fallback and the order can be modified by setting ARNS_RESOLVER_PRIORITY_ORDER.Remove duplicate network process call in on-demand resolver.Don't wait for network process debounces in the on-demand resolver.Slow network process dry runs no longer block fallback to next resolver.Added Added support for separate CUs URLs for the network and ANT processes via the NETWORK_AO_CU_URL and ANT_AO_CU_URL process URLs respectively. If either is missing the AO_CU_URL is used instead with a fallback to the SDK default URL if AO_CU_URL is also unspecified.Added CU URLs to on-demand ArNS resolver logs.Added circuit breakers for AR.IO network process CU dry runs. By default they use a 1 minute timeout and open after 30% failure over a 10 minute window and reset after 20 minutes.Fixed Owners in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.[Release 28] - 2025-03-17 Changed Raised name not found name list refresh interval to 2 minutes to reduce load on CUs. This increases the maximum amount of time a user may wait for a new name to be available. Future releases will introduce other changes to mitigate this delay.Adjusted composite ArNS resolver to never timeout resolutions from the last ArNS resolver in the resolution list.Added Added support for serving a given ID or ArNS name from the apex domain of a gateway. If using an ID, set the APEX_TX_ID environment variable. If using an ArNS name, set the APEX_ARNS_NAME environment variable.Added BUNDLE_REPAIR_UPDATE_TIMESTAMPS_INTERVAL_SECONDS, BUNDLE_REPAIR_BACKFILL_INTERVAL_SECONDS, and BUNDLE_REPAIR_FILTER_REPROCESS_INTERVAL_SECONDS environment variables to control the interval for retrying failed bundles, backfilling bundle records, and reprocessing bundles after a filter change. Note: the latter two are rarely used. Queuing bundles for reprocessing via the /ar-io/admin/queue-bundle endpoint is usually preferable to automatic reprocessing as it is faster and offers more control over the reprocessing behavior.Fixed Signatures in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.Adjusted exported Parquet file names to align with expectations of ClickHouse import script.Ensured that bundle indexing status is properly reset when bundles are manually queued after an unbundling filter change has been made.[Release 27] - 2025-02-20 Changed Set process IDs for mainnet.Increase default AO CU WASM memory limit to 17179869184 to support mainnet\\nprocess.[Release 26] - 2025-02-13 Added Added a per resolver timeout in the composite ArNS resolver. When the\\ncomposite resolver attempts resolution it is applied to each resolution\\nattempt. It is configurable via the ARNS_COMPOSITE_RESOLVER_TIMEOUT_MS and\\ndefaults to 3 seconds in order to allow a fallback attempt before the default\\nobserver timeout of 5 seconds.Added a TURBO_UPLOAD_SERVICE_URL environment variable to support\\nconfiguration of the bundler used by the observer (TurboSDK defaults are\\nused if not set).Added a REPORT_DATA_SINK environment variable that enables switching the\\nmethod used to post observer reports. With the default, turbo, it sends\\ndata items via a Turbo compatible bundler. Switching it to arweave will\\npost base layer transactions directly to Arweave instead.Added a /ar-io/admin/bundle-status/ endpoint that returns the counters\\nand timestamps from the bundles row in data.db. This can be used for\\nmonitoring unbundling progress and scripting (e.g., to skip requeuing already\\nqueued bundles).Added more complete documentation for filters.Changed Use arweave.net as the default GraphQL URL for AO CUs since most gateways\\nwill not have a complete local AO data item index.Use a default timeout of 5 seconds when refreshing Arweave peers to prevent\\nstalled peer refreshes.Cache selected gateway peer weights for the amount of time specified by the GATEWAY_PEERS_WEIGHTS_CACHE_DURATION_MS environment variable with a default\\nof 5 seconds to avoid expensive peer weight recomputation on each request.Chunk broadcasts to primary nodes occur in parallel with a concurrency limit\\ndefaulting to 2 and configurable via the CHUNK_POST_CONCURRENCY_LIMIT environment variable.Added circuit breakers for primary chunk node POSTs to avoid overwhelming\\nchunk nodes when they are slow to respond.Fixed Properly cleanup timeout and event listener when terminating the data\\nroot computation worker.Count chunk broadcast exceptions as errors in the arweave_chunk_broadcast_total metric.[Release 25] - 2025-02-07 Added Added support for indexing and querying ECDSA signed Arweave transactions.Expanded the OpenAPI specification to cover the entire gateway API and\\ncommonly used Arweave node routes.ArNS undername record count limits are now enforced. Undernames are sorted\\nbased on their ANT configured priority with a fallback to name comparisons\\nwhen priorities conflict or are left unspecified. Enforcement is enabled by\\ndefault but can be disabled by setting the ARNS_RESOLVER_ENFORCE_UNDERNAME_LIMIT to false.Changed Renamed the ario-peer data source to ar-io-peers for consistency and\\nclarity. ario-peer will continue to work for backwards compatibility but is\\nconsidered deprecated.Use AR.IO gateway peers from the ar.io gateway address registry (GAR) as the\\nlast fallback for fetching data when responding to client data requests. This\\nhas the benefit of making the network more resilient to trusted gateway\\ndisruptions, but it can also result in nodes serving data from less trusted\\nsources if it is not found in the trusted gateway. This can be disabled by\\nusing a custom ON_DEMAND_RETRIEVAL_ORDER that does not include ar-io-peers.Arweave data chunk requests are sent to the trusted node first with a\\nfallback to Arweave peers when chunks are unavailable on the trusted node.\\nThis provides good performance by default with a fallback in case there are\\nissues retrieving chunks from the trusted node.Increased the observer socket timeout to 5 seconds to accommodate initial\\nslow responses for uncached ArNS resolutions.Disabled writing base layer Arweave signatures to the SQLite DB by default to\\nsave disk space. When signatures are required to satisfy GraphQL requests,\\nthey are retrieved from headers on the trusted node.Fixed Updated dependencies to address security issues.Improved reliability of failed bundle indexing retries.Fixed failure to compute data roots for verification for base layer data\\nlarger than 2GiB.Fixed observer healthcheck by correcting node.js path in healthcheck script.[Release 24] - 2025-02-03 Added Added a ARNS_ANT_STATE_CACHE_HIT_REFRESH_WINDOW_SECONDS environment\\nvariable that determines the number of seconds before the end of the TTL at\\nwhich to start attempting to refresh the ANT state.Added a TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS environment that defaults to\\n10,000 and sets the number of milliseconds to wait before timing out request\\nto trusted gateways.Added BUNDLE_REPAIR_RETRY_INTERVAL_SECONDS and BUNDLE_REPAIR_RETRY_BATCH_SIZE environment variables to control the time\\nbetween queuing batches of bundle retries and the number of data items\\nretrieved when constructing batches of bundles to retry.Added support for configuring the ar.io SDK log level via the AR_IO_SDK_LOG_LEVEL environment variable.Added a request_chunk_total Prometheus counter with status, source (a\\nURL) and source_type (trusted or peer) labels to track success/failure\\nof chunk retrieval in the Arweave network per source.Added a get_chunk_total Prometheus metric to count chunk retrieval\\nsuccess/failure per chunk.Added arns_cache_hit_total and arns_cache_miss_total Prometheus counters\\nto track ArNS cache hits and misses for individual names respectively.Added arns_name_cache_hit_total and arns_name_cache_miss_total Prometheus\\ncounters to track ArNS name list cache hits and misses\\nrespectively.Added a arns_resolution_duration_ms Prometheus metric that tracks summary\\nstatistics for the amount of time it takes to resolve ArNS names.Changed In addition to the trusted node, the Arweave network is now searched for\\nchunks by default. All chunks retrieved are verified against data roots\\nindexed from a trusted Arweave node to ensure their validity.Default to a 24 hour cache TTL for the ArNS name cache. Record TTLs still\\noverride this, but in cases where resolution via AO CU is slow or fails, the\\ncache will be used. In the case of slow resolution, CU based resolution will\\nproceed in the background and update the cache upon completion.Switched to the ioredis library for better TLS support.Updated minor dependency minor versions (more dependencies will be updated in\\nthe next release).Bundles imports will no longer be re-attempted for bundles that have already\\nbeen fully unbundled using the current filters if they are matched or\\nmanually queued again.Replaced references docker-compose in the docs with the more modern docker compose.Fixed Ensure duplicate data item IDs are ignored when comparing counts to determine\\nif a bundle has been fully unbundled.Fixed worker threads failing to shut down properly when the main processped.Ensure bundle import attempt counts are incremented when bundles are skipped\\nto avoid repeatedly attempting to import skipped bundles.Use observe that correctly ensure failing gateways are penalized in the AR.IO\\nAO process.[Release 23] - 2025-01-13 Added Added FS_CLEANUP_WORKER_BATCH_SIZE,FS_CLEANUP_WORKER_BATCHDURATION, and FS_CLEANUP_WORKER_RESTARTDURATION environment variables to allow\\nconfiguration of number of contiguous data files cleaned up per batch, the between each batch, and the before restarting the entire cleanup\\nprocess again.Added data_items_unbundled_total Prometheus metric that counts the total\\nnumber of data items unbundled, including those that did not match the\\nunbundling filter.Added a parent_type label that can be one of transaction or data_item to data item indexing metrics.Added a files_cleaned_total total Prometheus metric to enable monitoring of\\ncontiguous data cleanup.Added support for specifying the admin API via a file specified by the ADMIN_API_KEY_FILE environment variable.Added experimental support for posting chunks in a non-blocking way to\\nsecondary nodes specified via a comma separate list in the SECONDARY_CHUNK_POST_URLS environment variable.Changed Renamed the parent_type lable to contiguous_data_type on bundle metrics\\nto more accurately reflect the meaning of the label.Reduced the maximum time to refresh the ArNS name list to 10 seconds to\\nminimize delays in ArNS availability after a new name is registered.Changed /ar-io/admin/queue-bundle to wait for bundles rows to be written\\nto the DB before responding to ensure that errors that occur due to DB\\ncontention are not silently ignored.Data items are now flushed even when block indexing is ped. This allows\\nfor indexing batches of data items using the admin API with block indexing\\ndisabled.Adjust services in docker-compose to use unless-ped as their restart\\npolicy. This guards against missing restarts in the case where service\\ncontainers exit with a success status even when they shouldn't.Fixed Added missing created_at field in blocked_names table.Fixed broken ArNS undername resolution.[Release 22] - 2024-12-18 Added Added the ability to block and unblock ArNS names (e.g., to comply with hosting provider TOS). To block a name, POST { \\\"name\\\": \\\"\\\" } to /ar-io/admin/block-name. To unblock a name, POST { \\\"name\\\": \\\"\\\" } to /ar-io/admin/unblock-name.Changed Return an HTTP 429 response to POSTs to /ar-io/admin/queue-bundle when the bundle data import queue is full so that scripts queuing bundles can wait rather than overflowing it.Fixed Adjust ArNS length limit from <= 48 to <= 51 to match the limit enforced by the AO process.[Release 21] - 2024-12-05 Added Added a ClickHouse auto-import service. When enabled, it calls the Parquet export API, imports the exported Parquet into ClickHouse, moves the Parquet files to an imported subdirectory, and deletes data items in SQLite up to where the Parquet export ended. To use it, run Docker Compose with the clickhouse profile, set the CLICKHOUSE_URL to http://clickhouse:8123, and ensure you have set an ADMIN_KEY. Using this configuration, the core service will also combine results from ClickHouse and SQLite when querying transaction data via GraphQL. Note: if you have a large number of data items in SQLite, the first export and subsequent delete may take an extended period. Also, this functionality is considered experimental. We expect there are still bugs to be found in it and we may make breaking changes to the ClickHouse schema in the future. If you choose to use it in production (not yet recommended), we suggest backing up copies of the Parquet files found in data/parquet/imported so that they can be reimported if anything goes wrong or future changes require it.Added a background data verification process that will attempt to recompute data roots for bundles and compare them to data roots indexed from Arweave nodes. When the data roots match, all descendant data items will be marked as verified. This enables verification of data initially retrieived from sources, like other gateways, that serve contiguous data instead of verifiable chunks. Data verification can be enabled by setting the ENABLE_BACKGROUND_DATA_VERIFICATION environment variable to true. The interval between attempts to verify batches of bundles is configurable using the BACKGROUND_DATA_VERIFICATION_INTERVAL_SECONDS environment variable.Added a CHUNK_POST_MIN_SUCCESS_COUNT environment variable to configure how many Arweave nodes must accept a chunk before a chunk broadcast is considered successful.Added arweave_chunk_post_total and arweave_chunk_broadcast_total Prometheus metrics to respectively track the number of successful chunk POSTs to Arweave nodes and the number of chunks successfully broadcast.When resolving ArNS names, the entire list of names is now cached instead of individually checking whether each name exists. This reduces the load on AO CUs since the entire list can be reused across multiple requests for different names. Note: due to the default 5 minute interval between name list refreshes, newly registered may now take longer to resolver after initial registration. We intend to make further caching refinements to address this in the future.Added support for multiple prioritized trusted gateways configurable by setting the TRUSTED_GATEWAYS_URLS environment variable to a JSON value containing a mapping of gateway hosts to priorities. Data requests are sent to other gateways in ascending priority order. If multiple gateways share the same priority, all the gateways with the same priority are tried in a random order before continuing on to the next priority.Added support for caching contiguous data in S3. It is enabled by default when the AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_CONTIGUOUS_DATA_PREFIX environment variables are set.Changed trusted-gateway was changed to trusted-gateways in ON_DEMAND_RETRIEVAL_ORDER and BACKGROUND_RETRIEVAL_ORDER.Renamed the S3 contiguous environment variables - AWS_S3_BUCKET to AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_PREFIX to AWS_S3_CONTIGUOUS_DATA_PREFIX.[Release 20] - 2024-11-15 Added Exposed the core service chunk POST endpoint via Envoy. It accepts a Arweave data chunk and broadcasts it to either the comma separated list of URLs specified by the CHUNK_POST_URLs environment variable or, if none are specified, the /chunk path on URL specified by the TRUST_GATEWAY_URL environment variable.Added a X-AR-IO-Root-Transaction-Id HTTP header to data responses containing the root base layer transaction ID for the ID in question if it's been indexed.Added a X-AR-IO-Data-Item-Data-Offset HTTP header containing the offset of the data item relative to the root bundle base layer transaction for it. In conjunction with X-AR-IO-Root-Transaction-Id, it enables retrieving data for data item IDs from base layer data using first a HEAD request to retrieve the root ID and data offset followed by a range request into the root bundle. This greatly increases the likelihood of retriving data item data by ID since only an index into the base layer and Arweave chunk availability is needed for this access method to succeed.Added an experimental ClickHouse service to docker-compose.yaml (available via the clickhouse profile). This will be used as a supplemental GraphQL DB in upcoming releases.Added a data item indexing healthcheck that can be enabled by setting the RUN_AUTOHEAL environment variable to true. When enabled, it will restart the core service if no data items have been indexed since the value specified by the MAX_EXPECTED_DATA_ITEM_INDEXING_INTERVAL_SECONDS environment variable.[Release 19] - 2024-10-21 Fixed Adjusted data item flushing to use the bundle DB worker instead of the core DB worker to prevent write contention and failed flushes under heavy unbundling load.Added Added X-AR-IO-Digest, X-AR-IO-Stable, X-AR-IO-Verified, and ETag headers. X-AR-IO-Digest contains a base64 URL encoded representation of the SHA-256 hash of the data item data. It may be empty if the gateway has not previously cached the data locally. X-AR-IO-Stable contains either true or false depending on whether the associated Arweave transaction is more than 18 blocks old or not. X-AR-IO-Verified contains either true if the gateway has verified the data root of the L1 transaction or the L1 root parent of the data item or false if it has not. ETag contains the same value a X-AR-IO-Digest and is used to improve HTTP caching efficiency.Added support for using a different data source for on-demand and background data retrieval. Background data retrieval is used when unbundling. The background retrieval data source order is configurable using the BACKGROUND_RETRIEVAL_ORDER environment variable and defaults to chunks,s3,trusted-gateway,tx-data. Priority is given to chunk retrieval since chunks are verifiable.Added an /ar-io/admin/export-parquet/status to support monitoring of in-progress Parquet export status.Added sqlite_in_flight_ops Prometheus metric with worker (core, bundles, data, or moderation) and role (read or write) labels to support monitoring the number of in-flight DB operations.Added experimental Grafana and Prometheus based observability stack. See the \\\"Monitoring and Observability\\\" section of the README for more details.Changed Bundle data is now retrieved as chunks from Arweave nodes by default so that data roots can be compared against the chain (see entry about background retrieval above).Changed observer configuration to use 8 instead of 5 chosen names. These are combined with 2 names prescribed from the contract for a total of 10 names observed each epoch to provide increased ArNS observation coverage.Verification status is set on data items when unbundling a parent that has already been verified.[Release 18] - 2024-10-01 Fixed Improved performance of data attributes query that was preventing data.db WAL flushing.Added Added WAL sqlite_wal_checkpoint_pages Prometheus metric to help monitor WAL flushing.Added a POST /ar-io/admin/export-parquet endpoint that can be used to export the contents of the SQLite3 core and bundle DBs as Parquet. To trigger an export, POST JSON containing outputDir, startHeight, endHeight, and maxFileRows keys. The resulting Parquet files can then be queried directly using DuckDB or loaded into another system (e.g. ClickHouse). Scripts will be provided to help automate the latter in a future release.Added ARNS_RESOLVER_OVERRIDE_TTL_SECONDS that can be used to force ArNS names to refresh before their TTLs expire.Added a GET /ar-io/resolver/:name endpoint that returns an ArNS resolution for the given name.Changed Removed ArNS resolver service in favor of integrated resolver. If a standalone resolver is still desired, the core service can be run with the START_WRITERS environment variable set to false. This will disable indexing while preserving resolver functionality.Deduplicated writes to data.db to improve performance and reduce WAL growth rate.[Release 17] - 2024-09-09 Notes This release includes a LONG RUNNING MIGRATION. Your node may appear unresponsive while it is running. It is best to wait for it to complete. If it fails or is interrupted, removing your SQLite DBs (in data/sqlite by default) should resolve the issue, provided you are willing to lose your GraphQL index and let your node rebuild it.Fixed Use the correct environment variable to populate WEBHOOK_BLOCK_FILTER in docker-compose.yaml.Don't cache data regions retrieved to satisfy range requests to avoid unnecessary storage overhead and prevent inserting invalid ID to hash mappings into the data DB.Added Added a new ClickHouse based DB backend. It can be used in combination with the SQLite DB backend to enable batch loading of historical data from Parquet. It also opens up the possibility of higher DB performance and scalability. In its current state it should be considered a technology preview. It won't be useful to most users until we either provide Parquet files to load into it or automate flushing of the SQLite DB to it (both are planned in future release). It is not intended to be standalone solution. It supports bulk loading and efficient GraphQL querying of transactions and data items, but it relies on SQLite (or potentially another OLTP in the future) to index recent data. These limitations allow greatly simplified schema and query construction. Querying the new ClickHouse DB for transaction and data items via GraphQL is enabled by setting the CLICKHOUSE_URL environment variable.Added the ability to skip storing transaction signatures in the DB by setting WRITE_TRANSACTION_DB_SIGNATURES to false. Missing signatures are fetched from the trusted Arweave node when needed for GraphQL results.Added a Redis backed signature cache to support retrieving optimistically indexed data item signatures in GraphQL queries when writing data items signatures to the DB has been disabled.Added on-demand and composite ArNS resolvers. The on-demand resolver fetches results directly from an AO CU. The composite resolver attempts resolution in the order specified by the ARNS_RESOLVER_PRIORITY_ORDER environment variable (defaults to on-demand,gateway).Added a queue_length Prometheus metric to fasciliate monitoring queues and inform future optimizations Added SQLite WAL cleanup worker to help manage the size of the data.db-wal file. Future improvements to data.db usage are also planned to further improve WAL management.Changed Handle data requests by ID on ArNS sites. This enables ArNS sites to use relative links to data by ID.Replaced ARNS_RESOLVER_TYPE with ARNS_RESOLVER_PRIORITY_ORDER (defaults to on-demand,gateway).Introduced unbundling back pressure. When either data item data or GraphQL indexing queue depths are more than the value specified by the MAX_DATA_ITEM_QUEUE_SIZE environment variable (defaults to 100000), unbundling is d until the queues length falls bellow that threshold. This prevents the gateway from running out of memory when the unbundling rate exceeds the indexing rate while avoiding wasteful bundle reprocessing.Prioritized optimistic data item indexing by inserting optimistic data items at the front of the indexing queues.Prioritized nested bundle indexing by inserting nested bundles at the front of the unbundling queue.[Release 16] - 2024-08-09 Fixed Fixed promise leak caused by missing await when saving data items to the DB.Modified ArNS middleware to not attempt resolution when receiving requests for a different hostname than the one specified by ARNS_ROOT_HOST.Added Added support for returning Content-Encoding HTTP headers based on user specified Content-Encoding tags.Added isNestedBundle filter enables that matches any nested bundle when indexing. This enables composite unbundling filters that match a set of L1 tags and bundles nested under them.Added ability to skip writing ANS-104 signatures to the DB and load them based on offsets from the data instead. This significantly reduces the size of the bundles DB. It can be enabled by setting the WRITE_ANS104_DATA_ITEM_DB_SIGNATURES environment variable to false.Added data_item_data_indexed_total Prometheus counter to count data items with data attributes indexed.Changed Queue data attributes writes when serving data rather than writing them syncronously.Reduced the default data indexer count to 1 to lessen the load on the data DB.Switched a number of overly verbose info logs to debug level.Removed docker-compose on-failure restart limits to ensure that services restart no matter how many times they fail.Modified the data_items_indexed_total Prometheus counter to count data items indexed for GraphQL querying instead of data attributes.Increased aggressiveness of contiguous data cleanup. It now s 5 seconds instead of 10 seconds per batch and runs every 4 hours instead of every 24 hours.[Release 15] - 2024-07-19 Fixed Fixed query error that was preventing bundles from being marked as fully imported in the database.Added Adjusted data item indexing to record data item signature types in the DB. This helps distinguish between signatures using different key formats, and will enable querying by signature type in the future.Adjusted data item indexing to record offsets for data items within bundles and signatures and owners within data items. In the future this will allow us to avoid saving owners and signatures in the DB and thus considerably reduce the size of the bundles DB.Added ARNS_CACHE_TTL_MS environment variable to control the TTL of ARNS cache entries (defaults to 1 hour).Added support for multiple ranges in a single HTTP range request.Added experimental chunk POST endpoint that broadcasts chunks to the comma-separate list of URLS in the CHUNK_BROADCAST_URLS environment variable. It is available at /chunk on the internal gateway service port (4000 by default) but is not yet exposed through Envoy.Added support for running an AO CU adjacent to the gateway (see README.md for details).Added X-ArNS-Process-Id to ArNS resolved name headers.Added a set of AO_... environment variables for specifying which AO URLs should be used (see docker-compose.yaml for the complete list). The AO_CU_URL is of particular use since the core and resolver services only perform AO reads and only the CU is needed for reads.Changed Split the monolithic docker-compose.yaml into docker-compose.yaml, docker-compose.bundler.yaml, and docker-compose.ao.yaml (see README for details).Replaced references to 'docker-compose' with 'docker compose' in the docs since the former is mostly deprecated.Reduce max fork depth from 50 to 18 inline to reflect Arweave 2.7.2 protocol changes.Increased the aggressiveness of bundle reprocessing by reducing reprocessing interval from 10 minutes to 5 minutes and raising reprocessing batch size from 100 to 1000.Use a patched version of Litestream to work around insufficient S3 multipart upload size in the upstream version.[Release 14] - 2024-06-26 Fixed Correctly handle manifest index after paths.[Release 13] - 2024-06-24 Added Added support for optimistically reading data items uploaded using the integrated Turbo bundler via the LocalStack S3 interface.Added X-AR-IO-Origin-Node-Release header to outbound data requests.Added hops, origin, and originNodeRelease query params to outbound data requests.Added support for fallback in v0.2 manifests that is used if no path in the manifest is matched.Changed Updated Observer to read prescribed names from and write observations to the ar.io AO network process.Updated Resolver to read from the ar.io AO network process.Fixed Modified optimistic indexing of data items to use a null parent_id when inserting into the DB instead of a placeholder value. This prevents unexpected non-null bundledIn values in GraphQL results for optimistically indexed data items.Modified GraphQl query logic to require an ID for single block GraphQL queries. Previously queries missing an ID were returning an internal SQLite error. This represents a small departure from arweave.net's query logic which returns the latest block for these queries. We recommend querying blocks instead of block in cases where the latest block is desired.Adjusted Observer health check to reflect port change to 5050.Security Modified docker-compose.yaml to only expose Redis, PostgreSQL, and LocalStack ports internally. This protects gateways that neglect to deploy behind a firewall, reverse proxy, or load balancer.[Release 12] - 2024-06-05 Added Added /ar-io/admin/queue-data-item endpoint for queuing data item headers for indexing before the bundles containing them are processed. This allows trusted bundlers to make their data items quickly available to be queried via GraphQL without having to wait for bundle data submission or unbundling.Added experimental support for retrieving contiguous data from S3. See AWS_* environment variables documentation for configuration details. In conjuction with a local Turbo bundler this allows optimistic bundle (but not yet data item) retrieval.Add experimental support for fetching data from gateway peers. It can be enabled by adding ario-peer to ON_DEMAND_RETRIEVAL_ORDER. Note: do not expect this work reliably yet! This functionality is in active development and will be improved in future releases.Add import_attempt_count to bundle records to enable future bundle import retry optimizations.Changed Removed version from docker-compose.yaml to avoid warnings with recent versions of docker-compose.Switched default observer port from 5000 to 5050 to avoid conflict on OS X. Since Envoy is used to provide external access to the observer API this should have no user visible effect.[Release 11] - 2024-05-21 Added Added arweave_tx_fetch_total Prometheus metric to track counts of transaction headers fetched from the trusted node and Arweave network peers.Changed Revert to using unnamed bind mounts due to cross platform issues with named s.[Release 10] - 2024-05-20 Added Added experimental support for streaming SQLite backups to S3 (and compatible services) using Litestream. Start the service using the docker-compose \\\"litestream\\\" profile to use it, and see the AR_IO_SQLITE_BACKUP_* environment variables documentation for further details.Added /ar-io/admin/queue-bundle endpoint for queueing bundles for import for import before they're in the mempool. In the future this will enable optimistic indexing when combined with a local trusted bundler.Added support for triggering webhooks when blocks are imported matching the filter specified by the WEBHOOK_BLOCK_FILTER environment variable.Added experimental support for indexing transactions and related data items from the mempool. Enable it by setting ENABLE_MEMPOOL_WATCHER to 'true'.Made on-demand data caching circuit breakers configurable via the GET_DATA_CIRCUIT_BREAKER_TIMEOUT_MS environment variable. This allows gateway operators to decide how much latency they will tolerate when serving data in exchange for more complete data indexing and caching.Rename cache header from X-Cached to X-Cache to mimic typical CDN practices.Add X-AR-IO-Hops and X-AR-IO-Origin headers in preparation for future peer-to-peer functionality.Upgrade to Node.js v20 and switch to native test runner.[Release 9] - 2024-04-10 Added Added experimental Farcaster Frames support, enabling simple Arweave based Frames with button navigation. Transaction and data item data is now served under /local/farcaster/frame/. /local is used as a prefix to indicate this functionality is both experimental and local to a particular gateway rather than part of the global gateway API. Both GET and POST requests are supported.Added an experimental local ArNS resolver. When enabled it removes dependence on arweave.net for ArNS resolution! Enable it by setting RUN_RESOLVER=TRUE, TRUSTED_ARNS_RESOLVER_TYPE=resolver, and TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 in your .env file.Added an X-Cached header to data responses to indicate when data is served from the local cache rather than being retrieved from an external source. This is helpful for interfacing with external systems, debugging, and end-to-end testing.Save hashes for unbundled data items during indexing. This enables reduction in data storage via hash based deduplication as well as more efficient peer-to-peer data retrieval in the future.[Release 8] - 2024-03-14 Added Added GraphQL SQL query debug logging to support trouble-shooting and performance optimization.Added support for indexing data items (not GraphQL querying) based solely on tag name. (example use case: indexing all IPFS CID tagged data items).Changes Observer data sampling now uses randomized ranges to generate content hashes.Reference gateway ArNS resolutions are now cached to improve report generation performance.Contract interactions are now tested before posting using dryWrite to avoid submitting interactions that would fail./ar-io/observer/info now reports INVALID for wallets that fail to load.Fixed Fix data caching failure caused by incorrect method name in getData circuit breakers.Fix healthcheck when ARNS_ROOT_HOST includes a subdomain.[Release 7] - 2024 - 02 - 14 Added Add support for notifying other services of transactions and data items using webhooks (see README for details).Add support for filter negation (particularly useful for excluding large bundles from indexint).Improve unbundling throughput by decoupling data fetching from unbundling.Add Envoy and core service ARM builds.Changed Improve resouce cleanup and shutdown behavior.Don't save Redis data to disk by default to help prevent memory issues on startup for small gateways.Reduce the amount of data sampled from large files by the observer.Ensure block poa2 field is not chached to reduce memory consumption.[Release 6] - 2024-01-29 Fixed Update observer to improve reliability of contract state synchronization and evaluation.[Release 5] - 2024-01-25 Added Added transaction offset indexing to support future data retrieval capabilities.Enabled IPv6 support in Envoy config.Added ability to configure observer report generation interval via the REPORT_GENERATION_INTERVAL_MS environmental variable. (Intended primarily for development and testing) Changed Updated observer to properly handle FQDN conflicts.Renamed most created_at columns to index to indexed_at for consistency and clarity.Fixed Updated LMDB version to remove Buffer workaround and fix occasional block cache errors.[Release 4] - 2024-01-11 Added Added circuit breakers around data index access to reduce impact of DB access contention under heavy requests loads.Added support for configuring data source priority via the ON_DEMAND_RETRIEVAL_ORDER environment variable.Updated observer to a version that retrieves epoch start and duration from contract state.Changed Set the Redis max memory eviction policy to allkeys-lru.Reduced default Redis max memory from 2GB to 256MB.Improved predictability and performance of GraphQL queries.Eliminated unbundling worker threads when filters are configured to skip indexing ANS-104 bundles.Reduced the default number of ANS-104 worker threads from 2 to 1 when unbundling is enabled to conserve memory.Increased nodejs max old space size to 8GB when ANS-104 workers > 1.Fixed Adjusted paths for chunks indexed by data root to include the full data root.[Release 3] - 2023-12-05 Added Support range requests (PR 61, PR 64) Note: serving multiple ranges in a single request is not yet supported.Release number in /ar-io/info response.Redis header cache implementation (PR 62).New default header cache (replaces old FS cache).LMDB header cache implementation (PR 60).Intended for use in development only.Enable by setting CHAIN_CACHE_TYPE=lmdb.Filesystem header cache cleanup worker (PR 68).Enabled by default to cleanup old filesystem cache now that Redis is the new default.Support for parallel ANS-104 unbundling (PR 65).Changed Used pinned container images tags for releases.Default to Redis header cache when running via docker-compose.Default to LMDB header cache when running via yarn start.Fixed Correct GraphQL pagination for transactions with duplicate tags.\",\"domain\":\"ario\",\"relevanceScore\":12,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ario-llms.txt\"},{\"content\":\"# 41. ARIO Node Release Notes - ARIO Docs\\n\\nDocument Number: 41\\nSource: https://docs.ar.io/gateways/release-notes#\\nWords: 7692\\nQuality Score: 0.489\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nAR.IO Release Notes Overview Welcome to the documentation page for the AR.IO gateway release notes. Here, you will find detailed information about each version of the AR.IO gateway, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO gateway. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO gateway change logs. Stay updated with the continuous improvements and advancements in the AR.IO gateway by referring to this page for all release-related information.[Release 41] - 2025-06-30 Added Added preferred chunk GET node URLs configuration via PREFERRED_CHUNK_GET_NODE_URLS environment variable to enable chunk-specific peer prioritization. Preferred URLs receive a weight of 100 for prioritization and the system selects 10 peers per attempt by default.Added hash validation for peer data fetching by including X-AR-IO-Expected-Digest header in peer requests when hash is available, validating peer responses against expected hash, and immediately rejecting mismatched data.Added DOCKER_NETWORK_NAME environment variable to configure the Docker network name used by Docker Compose.Added draft guide for running a community gateway.Added draft data verification architecture document.Changed Removed trusted node fallback for chunk retrieval. Chunks are now retrieved exclusively from peers, with the retry count increased from 3 to 50 to ensure reliability without the trusted node fallback.Fixed Fixed inverted logic preventing symlink creation in FsChunkDataStore.Fixed Content-Length header for range requests and 304 responses, properly setting header for single and multipart range requests and removing entity headers from 304 Not Modified responses per RFC 7232.Fixed MaxListenersExceeded warnings by adding setMaxListeners to read-through data cache.Fixed potential memory leaks in read-through data cache by using once instead of on for error and end event listeners.[Release 40] - 2025-06-23 This is an optional release that primarily improves caching when data is fetched from peers.Added Added experimental flush-to-stable script for manual database maintenance. This script allows operators to manually flush stable chain and data item tables, mirroring the logic of StandaloneSqliteDatabase.flushStableDataItems. WARNING: This script is experimental and directly modifies database contents. Use with caution and ensure proper backups before running.Changed Replaced yesql with custom SQL loader that handles comments better, improving SQL file parsing and maintenance.Switched to SPDX license headers to reduce LLM token usage, making the codebase more efficient for AI-assisted development.Improved untrusted data handling and hash validation in cache operations. The cache now allows caching when a hash is available for validation even for untrusted data sources, but only finalizes the cache when the computed hash matches a known trusted hash. This prevents cache poisoning while still allowing data caching from untrusted sources when the data can be validated.[Release 39] - 2025-06-17 This release enhances observability and reliability with new cache metrics, improved data verification capabilities, and automatic failover between chain data sources. The addition of ArNS-aware headers enables better data prioritization across the gateway network. This is a recommended but not urgent upgrade.Added Added filesystem cache metrics with cycle-based tracking. Two new Prometheus metrics track cache utilization: cache_objects_total (number of objects in cache) and cache_size_bytes (total cache size in bytes). Both metrics include store_type and data_type labels to differentiate between cache types (e.g., headers, contiguous_data). Metrics are updated after each complete cache scan cycle, providing accurate visibility into filesystem cache usage.Added X-AR-IO-Data-Id header to all data responses. This header shows the actual data ID being served, whether from a direct ID request or manifest path resolution, providing transparency about the content being delivered.Added automatic data item indexing when data verification is enabled. When ENABLE_BACKGROUND_DATA_VERIFICATION is set to true, the system now automatically enables data item indexing (ANS104_UNBUNDLE_FILTER) with an always: true filter if no filter is explicitly configured. This ensures bundles are unbundled to verify that data items are actually contained in the bundle associated with the Arweave transaction's data root.Added ArNS headers to outbound gateway requests to enable data prioritization. The generateRequestAttributes function now includes ArNS context headers (X-ArNS-Name, X-ArNS-Basename, X-ArNS-Record) in requests to other gateways and Arweave nodes, allowing downstream gateways to effectively prioritize ArNS data requests.Added configurable Docker Compose host port environment variables (CORE_PORT, ENVOY_PORT, CLICKHOUSE_PORT, CLICKHOUSE_PORT_2, CLICKHOUSE_PORT_3, OBSERVER_PORT) to allow flexible port mapping while maintaining container-internal port compatibility and security.Added Envoy aggregate cluster configuration for automatic failover between primary and fallback chain data sources. The primary cluster (default: arweave.net:443) uses passive outlier detection while the fallback cluster (default: peers.arweave.xyz:1984) uses active health checks. This enables zero-downtime failover between HTTPS and HTTP endpoints with configurable FALLBACK_NODE_HOST and FALLBACK_NODE_PORT environment variables.Changed Streamlined background data retrieval to reduce reliance on centralized sources. The default BACKGROUND_RETRIEVAL_ORDER now only includes chunks,s3, removing trusted-gateways and tx-data from the default configuration. This prioritizes verifiable chunk data and S3 storage for background operations like unbundling.Removed ar-io.net from default trusted gateways list and removed TRUSTED_GATEWAY_URL default value to reduce load on ar-io.net now that P2P data retrieval is re-enabled. Existing deployments with TRUSTED_GATEWAY_URL explicitly set will continue to work for backwards compatibility.[Release 38] - 2025-06-09 This release focuses on data integrity and security improvements, introducing trusted data verification and enhanced header information for data requests. Upgrading to this release is recommended but not urgent.Added Added X-AR-IO-Trusted header to indicate data source trustworthiness in responses. This header helps clients understand whether data comes from a trusted source and works alongside the existing X-AR-IO-Verified header to provide data integrity information. The system now filters peer data by requiring peers to indicate their content is either verified or trusted, protecting against misconfigured peers that may inadvertently serve unintended content (e.g., provider default landing pages) instead of actual Arweave data.Added If-None-Match header support for HTTP conditional requests enabling better client-side caching efficiency. When clients send an If-None-Match header that matches the ETag, the gateway returns a 304 Not Modified response with an empty body, reducing bandwidth usage and improving performance.Added digest and hash headers for data HEAD requests to enable client-side data integrity verification.Added EC2 IMDS (instance-profile) credential support for S3 data access, improving AWS authentication in cloud environments.Added trusted data flag to prevent caching of data from untrusted sources, ensuring only verified and reliable content is stored locally while still allowing serving of untrusted data when necessary.Changed Re-enabled ar-io-peers as fallback data source in configuration for improved data availability.Updated trusted node configuration to use arweave.net as the default trusted node URL.Updated ETag header format to use properly quoted strings (e.g., \\\"hash\\\" instead of hash) following HTTP/1.1 specification standards for improved compatibility with caching proxies and clients.[Release 37] - 2025-06-03 This is a recommended release due to the included observer robustness improvements. It also adds an important new feature - data verification for preferred ArNS names. When preferred ArNS names are set, the bundles containing the data they point to will be locally unbundled (verifying data item signatures), and the data root for the bundle will be compared to the data root in the Arweave chain (establishing that the data is on Arweave). To enable this feature, set your preferred ArNS names, turn on unbundling by setting ANS104_DOWNLOAD_WORKERS and ANS104_UNBUNDLE_WORKERS both to 1, and set your ANS104_INDEX_FILTER to a filter that will match the data items for your preferred names. If you don't know the filter, use {\\\"always\\\": true}, but be aware this will index the entire bundle for the IDs related to your preferred names.Note: this release contains migrations to data.db. If your node appears unresponsive please check core service logs to determine whether migrations are running and wait for them to finish.Added Added prioritized data verification system for preferred ArNS names, focusing computational resources on high-priority content while enabling flexible root transaction discovery through GraphQL fallback support.Added verification retry prioritization system with tracking of retry counts, priority levels, and attempt timestamps to ensure bundles do not get stuck retrying forever.Added improved observer functionality with best-of-2 observations and higher compression for more reliable network monitoring.Added MAX_VERIFICATION_RETRIES environment variable (default: 5) to limit verification retry attempts and prevent infinite loops for consistently failing data items.Added retry logic with exponential backoff for GraphQL queries to handle rate limiting (429) and server errors with improved resilience when querying trusted gateways for root bundle IDs.Changed Updated dependencies: replaced deprecated express-prometheus-middleware with the actively maintained express-prom-bundle library and updated prom-client to v15.1.3 for better compatibility and security.Updated Linux setup documentation to use modern package installation methods, replacing apt-key yarn installation with npm global install and updating Node.js/nvm versions.Improved route metrics normalization with explicit whitelist function for better granularity and proper handling of dynamic segments.Fixed Fixed docker-compose configuration to use correct NODE_MAX_OLD_SPACE_SIZE environment variable name.Fixed production TypeScript build configuration to exclude correct \\\"test\\\" directory path.Fixed Parquet exporter to properly handle data item block_transaction_index exports, preventing NULL value issues.Fixed bundles system to copy root_parent_offset when flushing data items to maintain data integrity.Fixed ClickHouse auto-import script to handle Parquet export not_started status properly.Fixed docker-compose ClickHouse configuration to not pass conflicting PARQUET_PATH environment variable to container scripts.Fixed verification process for data items that have not been unbundled by adding queue bundle support and removing bundle join constraint to ensure proper verification of data items without indexed root parents.[Release 36] - 2025-05-27 This is a recommended but not essential upgrade. The most important changes are the preferred ArNS caching feature for improved performance on frequently accessed content and the observer's 80% failure threshold to prevent invalid reports during network issues.Added Added preferred ArNS caching functionality that allows configuring lists of ArNS names to be cached longer via PREFERRED_ARNS_NAMES and PREFERRED_ARNS_BASE_NAMES environment variables. When configured, these names will be cleaned from the filesystem cache after PREFERRED_ARNS_CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD instead of the standard cleanup threshold (CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD). This is accomplished by maintaining an MRU (Most Recently Used) list of ArNS names in the contiguous metadata cache. When filesystem cleanup runs, it checks this list to determine which cleanup threshold to apply. This feature enables gateway operators to ensure popular or important ArNS names remain cached longer, improving performance for frequently accessed content.Added ArNS headers to responses: X-ArNS-Name, X-ArNS-Basename, and X-ArNS-Record to help identify which ArNS names were used in the resolution.Changed Updated observer to prevent report submission when failure rate exceeds 80%. This threshold helps guard against both poorly operated observers and widespread network issues. In the case of a widespread network issue, the assumption is that most gateway operators are well intentioned and will work together to troubleshoot and restore both observations and network stability, rather than submitting reports that would penalize functioning gateways.Updated default trusted gateway in docker-compose Envoy configuration to ar-io.net for improved robustness and alignment with core service configuration.Improved range request performance by passing ranges directly to getData implementations rather than streaming all data and extracting ranges.Fixed Fixed missing cache headers (X-Cache and other data headers) in range request responses to ensure consistent cache header behavior across all request types.Fixed async streaming for multipart range requests by using async iteration instead of synchronous reads, preventing potential data loss.Fixed ArNS resolution to properly exclude www subdomain from resolution logic.Fixed test reliability issues by properly awaiting stream completion before making assertions.Fixed chunk broadcasting to not await peer broadcasts, as they are best-effort operations.[Release 35] - 2025-05-19 This is a low upgrade priority release. It contains a small caching improvement and routing fix. Upgrading to help test it is appreciated but not essential.Changed Adjusted filesystem data expiration to be based on last request times rather than file access times which may be inaccurate.Adjusted CORS headers to include content-* headers.Fixed Fixed regex used to expose /api-docs when an apex ArNS name is set.[Release 34] - 2025-05-05 Given the resilience provided by adding a second trusted gateway URL, it is recommended that everyone upgrade to this release.Added Added peer list endpoints for retrieving information about Arweave peers and ar.io gateway peers.Added ar-io.net as a secondary trusted gateway to increase data retrieval resilience by eliminating a single point of failure.Added circuit breaker for Arweave peer chunk posting.Changed Created directories for DuckDB and Parquet to help avoid permission issues by the directories being created by containers.Fixed Fixed GraphQL ClickHouse error when returning block ID and timestamp.Fixed the tx-chunks-data-source to throw a proper error (resulting in a 404) when the first chunk is missing rather than streaming a partial response.[Release 33] - 2025-05-05 Added Added a [Parquet and ClickHouse usage guide]. Using ArDrive as an example, it provides step by step instructions about how to bulk load Parquet and configure continuous ingest of bundled data items into ClickHouse. This allows the ar-io-node to support performant GraphQL queries on larger data sets and facilitates sharing indexing work across gateways via distribution of Parquet files.Added support for configurable ArNS 404 pages using either:ARNS_NOT_FOUND_TX_ID: Transaction ID for custom 404 content ARNS_NOT_FOUND_ARNS_NAME: ArNS name to resolve for 404 content Added experimental /chunk/ GET route for serving chunk data by absolute offset either the local cache.Added support for AWS_SESSION_TOKEN in the S3 client configuration.Expanded ArNS OTEL tracing to improve resolution behavior observability.Added support for setting a ClickHouse username and password via the CLICKHOUSE_USERNAME and CLICKHOUSE_PASSWORD environment variable. When using ClickHouse, CLICKHOUSE_PASSWORD should always be set. However, CLICKHOUSE_USERNAME can be left unset. The username default will be used in that case.Added support for configuring the port used to connect to ClickHouse via the CLICKHOUSE_PORT environment variable.Changed Disabled ClickHouse import timing logging by default. It can be enabled via environment variable - DEBUG when running the service standalone or CLICKHOUSE_DEBUG when using Docker Compose Upgraded to ClickHouse 25.4.Fixed Ensure .env is read in clickhouse-import script.[Release 32] - 2025-04-22 Changed Reenabled parallel ArNS resolution with removal of misplaced global limit. Refer to release 30 notes for more details on configuration and rationale.Added a timeout for the last ArNS resolver in ARNS_RESOLVER_PRIORITY_ORDER. It defaults to 30 seconds and is configurable using ARNS_COMPOSITE_LAST_RESOLVER_TIMEOUT_MS. This helps prevent promise build up if the last resolver stalls.Fixed Fixed apex ArNS name handling when a subdomain is present in ARNS_ROOT_HOST.Fixed a case where fork recovery could stall due to early flushing of unstable chain data.Restored observer logs by removing unintentional default log level override in docker-compose.yaml.[Release 31] - 2025-04-11 Changed Improved peer TX header fetching by fetching from a wider range of peers and up/down weighting peers based on success/failure.Fixed Rolled back parallel ArNS resolution changes that were causing ArNS resolution to slow down over time.[Release 30] - 2025-04-04 Added Added support for filtering Winston logs with a new LOG_FILTER environment variable.Example filter: {\\\"attributes\\\":{\\\"class\\\":\\\"ArweaveCompositeClient\\\"}} to only show logs from that class.Use CORE_LOG_FILTER environment variable when running with docker-compose.Added parallel ArNS resolution capability.Configured via ARNS_MAX_CONCURRENT_RESOLUTIONS (default: 1).This foundation enables future enhancements to ArNS resolution and should generally not be adjusted at present.Changed Improved ClickHouse auto-import script with better error handling and continuous operation through errors.Reduced maximum header request rate per second to trusted node to load on community gateways.Optimized single owner and recipient queries on ClickHouse with specialized sorted tables.Used ID sorted ClickHouse table for ID queries to improve performance.Fixed Fixed data alignment in Parquet file name height boundaries to ensure consistent import boundaries.Removed trailing slashes from AO URLs to prevent issues when passing them to the SDK.Only prune SQLite data when ClickHouse import succeeds to prevent data loss during exports.[Release 29] - 2025-03-21 Changed Temporarily default to trusted gateway ArNS resolution to reduce CU load as much possible. On-demand CU resolution is still available as a fallback and the order can be modified by setting ARNS_RESOLVER_PRIORITY_ORDER.Remove duplicate network process call in on-demand resolver.Don't wait for network process debounces in the on-demand resolver.Slow network process dry runs no longer block fallback to next resolver.Added Added support for separate CUs URLs for the network and ANT processes via the NETWORK_AO_CU_URL and ANT_AO_CU_URL process URLs respectively. If either is missing the AO_CU_URL is used instead with a fallback to the SDK default URL if AO_CU_URL is also unspecified.Added CU URLs to on-demand ArNS resolver logs.Added circuit breakers for AR.IO network process CU dry runs. By default they use a 1 minute timeout and open after 30% failure over a 10 minute window and reset after 20 minutes.Fixed Owners in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.[Release 28] - 2025-03-17 Changed Raised name not found name list refresh interval to 2 minutes to reduce load on CUs. This increases the maximum amount of time a user may wait for a new name to be available. Future releases will introduce other changes to mitigate this delay.Adjusted composite ArNS resolver to never timeout resolutions from the last ArNS resolver in the resolution list.Added Added support for serving a given ID or ArNS name from the apex domain of a gateway. If using an ID, set the APEX_TX_ID environment variable. If using an ArNS name, set the APEX_ARNS_NAME environment variable.Added BUNDLE_REPAIR_UPDATE_TIMESTAMPS_INTERVAL_SECONDS, BUNDLE_REPAIR_BACKFILL_INTERVAL_SECONDS, and BUNDLE_REPAIR_FILTER_REPROCESS_INTERVAL_SECONDS environment variables to control the interval for retrying failed bundles, backfilling bundle records, and reprocessing bundles after a filter change. Note: the latter two are rarely used. Queuing bundles for reprocessing via the /ar-io/admin/queue-bundle endpoint is usually preferable to automatic reprocessing as it is faster and offers more control over the reprocessing behavior.Fixed Signatures in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse.Adjusted exported Parquet file names to align with expectations of ClickHouse import script.Ensured that bundle indexing status is properly reset when bundles are manually queued after an unbundling filter change has been made.[Release 27] - 2025-02-20 Changed Set process IDs for mainnet.Increase default AO CU WASM memory limit to 17179869184 to support mainnet\\nprocess.[Release 26] - 2025-02-13 Added Added a per resolver timeout in the composite ArNS resolver. When the\\ncomposite resolver attempts resolution it is applied to each resolution\\nattempt. It is configurable via the ARNS_COMPOSITE_RESOLVER_TIMEOUT_MS and\\ndefaults to 3 seconds in order to allow a fallback attempt before the default\\nobserver timeout of 5 seconds.Added a TURBO_UPLOAD_SERVICE_URL environment variable to support\\nconfiguration of the bundler used by the observer (TurboSDK defaults are\\nused if not set).Added a REPORT_DATA_SINK environment variable that enables switching the\\nmethod used to post observer reports. With the default, turbo, it sends\\ndata items via a Turbo compatible bundler. Switching it to arweave will\\npost base layer transactions directly to Arweave instead.Added a /ar-io/admin/bundle-status/ endpoint that returns the counters\\nand timestamps from the bundles row in data.db. This can be used for\\nmonitoring unbundling progress and scripting (e.g., to skip requeuing already\\nqueued bundles).Added more complete documentation for filters.Changed Use arweave.net as the default GraphQL URL for AO CUs since most gateways\\nwill not have a complete local AO data item index.Use a default timeout of 5 seconds when refreshing Arweave peers to prevent\\nstalled peer refreshes.Cache selected gateway peer weights for the amount of time specified by the GATEWAY_PEERS_WEIGHTS_CACHE_DURATION_MS environment variable with a default\\nof 5 seconds to avoid expensive peer weight recomputation on each request.Chunk broadcasts to primary nodes occur in parallel with a concurrency limit\\ndefaulting to 2 and configurable via the CHUNK_POST_CONCURRENCY_LIMIT environment variable.Added circuit breakers for primary chunk node POSTs to avoid overwhelming\\nchunk nodes when they are slow to respond.Fixed Properly cleanup timeout and event listener when terminating the data\\nroot computation worker.Count chunk broadcast exceptions as errors in the arweave_chunk_broadcast_total metric.[Release 25] - 2025-02-07 Added Added support for indexing and querying ECDSA signed Arweave transactions.Expanded the OpenAPI specification to cover the entire gateway API and\\ncommonly used Arweave node routes.ArNS undername record count limits are now enforced. Undernames are sorted\\nbased on their ANT configured priority with a fallback to name comparisons\\nwhen priorities conflict or are left unspecified. Enforcement is enabled by\\ndefault but can be disabled by setting the ARNS_RESOLVER_ENFORCE_UNDERNAME_LIMIT to false.Changed Renamed the ario-peer data source to ar-io-peers for consistency and\\nclarity. ario-peer will continue to work for backwards compatibility but is\\nconsidered deprecated.Use AR.IO gateway peers from the ar.io gateway address registry (GAR) as the\\nlast fallback for fetching data when responding to client data requests. This\\nhas the benefit of making the network more resilient to trusted gateway\\ndisruptions, but it can also result in nodes serving data from less trusted\\nsources if it is not found in the trusted gateway. This can be disabled by\\nusing a custom ON_DEMAND_RETRIEVAL_ORDER that does not include ar-io-peers.Arweave data chunk requests are sent to the trusted node first with a\\nfallback to Arweave peers when chunks are unavailable on the trusted node.\\nThis provides good performance by default with a fallback in case there are\\nissues retrieving chunks from the trusted node.Increased the observer socket timeout to 5 seconds to accommodate initial\\nslow responses for uncached ArNS resolutions.Disabled writing base layer Arweave signatures to the SQLite DB by default to\\nsave disk space. When signatures are required to satisfy GraphQL requests,\\nthey are retrieved from headers on the trusted node.Fixed Updated dependencies to address security issues.Improved reliability of failed bundle indexing retries.Fixed failure to compute data roots for verification for base layer data\\nlarger than 2GiB.Fixed observer healthcheck by correcting node.js path in healthcheck script.[Release 24] - 2025-02-03 Added Added a ARNS_ANT_STATE_CACHE_HIT_REFRESH_WINDOW_SECONDS environment\\nvariable that determines the number of seconds before the end of the TTL at\\nwhich to start attempting to refresh the ANT state.Added a TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS environment that defaults to\\n10,000 and sets the number of milliseconds to wait before timing out request\\nto trusted gateways.Added BUNDLE_REPAIR_RETRY_INTERVAL_SECONDS and BUNDLE_REPAIR_RETRY_BATCH_SIZE environment variables to control the time\\nbetween queuing batches of bundle retries and the number of data items\\nretrieved when constructing batches of bundles to retry.Added support for configuring the ar.io SDK log level via the AR_IO_SDK_LOG_LEVEL environment variable.Added a request_chunk_total Prometheus counter with status, source (a\\nURL) and source_type (trusted or peer) labels to track success/failure\\nof chunk retrieval in the Arweave network per source.Added a get_chunk_total Prometheus metric to count chunk retrieval\\nsuccess/failure per chunk.Added arns_cache_hit_total and arns_cache_miss_total Prometheus counters\\nto track ArNS cache hits and misses for individual names respectively.Added arns_name_cache_hit_total and arns_name_cache_miss_total Prometheus\\ncounters to track ArNS name list cache hits and misses\\nrespectively.Added a arns_resolution_duration_ms Prometheus metric that tracks summary\\nstatistics for the amount of time it takes to resolve ArNS names.Changed In addition to the trusted node, the Arweave network is now searched for\\nchunks by default. All chunks retrieved are verified against data roots\\nindexed from a trusted Arweave node to ensure their validity.Default to a 24 hour cache TTL for the ArNS name cache. Record TTLs still\\noverride this, but in cases where resolution via AO CU is slow or fails, the\\ncache will be used. In the case of slow resolution, CU based resolution will\\nproceed in the background and update the cache upon completion.Switched to the ioredis library for better TLS support.Updated minor dependency minor versions (more dependencies will be updated in\\nthe next release).Bundles imports will no longer be re-attempted for bundles that have already\\nbeen fully unbundled using the current filters if they are matched or\\nmanually queued again.Replaced references docker-compose in the docs with the more modern docker compose.Fixed Ensure duplicate data item IDs are ignored when comparing counts to determine\\nif a bundle has been fully unbundled.Fixed worker threads failing to shut down properly when the main processped.Ensure bundle import attempt counts are incremented when bundles are skipped\\nto avoid repeatedly attempting to import skipped bundles.Use observe that correctly ensure failing gateways are penalized in the AR.IO\\nAO process.[Release 23] - 2025-01-13 Added Added FS_CLEANUP_WORKER_BATCH_SIZE,FS_CLEANUP_WORKER_BATCHDURATION, and FS_CLEANUP_WORKER_RESTARTDURATION environment variables to allow\\nconfiguration of number of contiguous data files cleaned up per batch, the between each batch, and the before restarting the entire cleanup\\nprocess again.Added data_items_unbundled_total Prometheus metric that counts the total\\nnumber of data items unbundled, including those that did not match the\\nunbundling filter.Added a parent_type label that can be one of transaction or data_item to data item indexing metrics.Added a files_cleaned_total total Prometheus metric to enable monitoring of\\ncontiguous data cleanup.Added support for specifying the admin API via a file specified by the ADMIN_API_KEY_FILE environment variable.Added experimental support for posting chunks in a non-blocking way to\\nsecondary nodes specified via a comma separate list in the SECONDARY_CHUNK_POST_URLS environment variable.Changed Renamed the parent_type lable to contiguous_data_type on bundle metrics\\nto more accurately reflect the meaning of the label.Reduced the maximum time to refresh the ArNS name list to 10 seconds to\\nminimize delays in ArNS availability after a new name is registered.Changed /ar-io/admin/queue-bundle to wait for bundles rows to be written\\nto the DB before responding to ensure that errors that occur due to DB\\ncontention are not silently ignored.Data items are now flushed even when block indexing is ped. This allows\\nfor indexing batches of data items using the admin API with block indexing\\ndisabled.Adjust services in docker-compose to use unless-ped as their restart\\npolicy. This guards against missing restarts in the case where service\\ncontainers exit with a success status even when they shouldn't.Fixed Added missing created_at field in blocked_names table.Fixed broken ArNS undername resolution.[Release 22] - 2024-12-18 Added Added the ability to block and unblock ArNS names (e.g., to comply with hosting provider TOS). To block a name, POST { \\\"name\\\": \\\"\\\" } to /ar-io/admin/block-name. To unblock a name, POST { \\\"name\\\": \\\"\\\" } to /ar-io/admin/unblock-name.Changed Return an HTTP 429 response to POSTs to /ar-io/admin/queue-bundle when the bundle data import queue is full so that scripts queuing bundles can wait rather than overflowing it.Fixed Adjust ArNS length limit from <= 48 to <= 51 to match the limit enforced by the AO process.[Release 21] - 2024-12-05 Added Added a ClickHouse auto-import service. When enabled, it calls the Parquet export API, imports the exported Parquet into ClickHouse, moves the Parquet files to an imported subdirectory, and deletes data items in SQLite up to where the Parquet export ended. To use it, run Docker Compose with the clickhouse profile, set the CLICKHOUSE_URL to http://clickhouse:8123, and ensure you have set an ADMIN_KEY. Using this configuration, the core service will also combine results from ClickHouse and SQLite when querying transaction data via GraphQL. Note: if you have a large number of data items in SQLite, the first export and subsequent delete may take an extended period. Also, this functionality is considered experimental. We expect there are still bugs to be found in it and we may make breaking changes to the ClickHouse schema in the future. If you choose to use it in production (not yet recommended), we suggest backing up copies of the Parquet files found in data/parquet/imported so that they can be reimported if anything goes wrong or future changes require it.Added a background data verification process that will attempt to recompute data roots for bundles and compare them to data roots indexed from Arweave nodes. When the data roots match, all descendant data items will be marked as verified. This enables verification of data initially retrieived from sources, like other gateways, that serve contiguous data instead of verifiable chunks. Data verification can be enabled by setting the ENABLE_BACKGROUND_DATA_VERIFICATION environment variable to true. The interval between attempts to verify batches of bundles is configurable using the BACKGROUND_DATA_VERIFICATION_INTERVAL_SECONDS environment variable.Added a CHUNK_POST_MIN_SUCCESS_COUNT environment variable to configure how many Arweave nodes must accept a chunk before a chunk broadcast is considered successful.Added arweave_chunk_post_total and arweave_chunk_broadcast_total Prometheus metrics to respectively track the number of successful chunk POSTs to Arweave nodes and the number of chunks successfully broadcast.When resolving ArNS names, the entire list of names is now cached instead of individually checking whether each name exists. This reduces the load on AO CUs since the entire list can be reused across multiple requests for different names. Note: due to the default 5 minute interval between name list refreshes, newly registered may now take longer to resolver after initial registration. We intend to make further caching refinements to address this in the future.Added support for multiple prioritized trusted gateways configurable by setting the TRUSTED_GATEWAYS_URLS environment variable to a JSON value containing a mapping of gateway hosts to priorities. Data requests are sent to other gateways in ascending priority order. If multiple gateways share the same priority, all the gateways with the same priority are tried in a random order before continuing on to the next priority.Added support for caching contiguous data in S3. It is enabled by default when the AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_CONTIGUOUS_DATA_PREFIX environment variables are set.Changed trusted-gateway was changed to trusted-gateways in ON_DEMAND_RETRIEVAL_ORDER and BACKGROUND_RETRIEVAL_ORDER.Renamed the S3 contiguous environment variables - AWS_S3_BUCKET to AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_PREFIX to AWS_S3_CONTIGUOUS_DATA_PREFIX.[Release 20] - 2024-11-15 Added Exposed the core service chunk POST endpoint via Envoy. It accepts a Arweave data chunk and broadcasts it to either the comma separated list of URLs specified by the CHUNK_POST_URLs environment variable or, if none are specified, the /chunk path on URL specified by the TRUST_GATEWAY_URL environment variable.Added a X-AR-IO-Root-Transaction-Id HTTP header to data responses containing the root base layer transaction ID for the ID in question if it's been indexed.Added a X-AR-IO-Data-Item-Data-Offset HTTP header containing the offset of the data item relative to the root bundle base layer transaction for it. In conjunction with X-AR-IO-Root-Transaction-Id, it enables retrieving data for data item IDs from base layer data using first a HEAD request to retrieve the root ID and data offset followed by a range request into the root bundle. This greatly increases the likelihood of retriving data item data by ID since only an index into the base layer and Arweave chunk availability is needed for this access method to succeed.Added an experimental ClickHouse service to docker-compose.yaml (available via the clickhouse profile). This will be used as a supplemental GraphQL DB in upcoming releases.Added a data item indexing healthcheck that can be enabled by setting the RUN_AUTOHEAL environment variable to true. When enabled, it will restart the core service if no data items have been indexed since the value specified by the MAX_EXPECTED_DATA_ITEM_INDEXING_INTERVAL_SECONDS environment variable.[Release 19] - 2024-10-21 Fixed Adjusted data item flushing to use the bundle DB worker instead of the core DB worker to prevent write contention and failed flushes under heavy unbundling load.Added Added X-AR-IO-Digest, X-AR-IO-Stable, X-AR-IO-Verified, and ETag headers. X-AR-IO-Digest contains a base64 URL encoded representation of the SHA-256 hash of the data item data. It may be empty if the gateway has not previously cached the data locally. X-AR-IO-Stable contains either true or false depending on whether the associated Arweave transaction is more than 18 blocks old or not. X-AR-IO-Verified contains either true if the gateway has verified the data root of the L1 transaction or the L1 root parent of the data item or false if it has not. ETag contains the same value a X-AR-IO-Digest and is used to improve HTTP caching efficiency.Added support for using a different data source for on-demand and background data retrieval. Background data retrieval is used when unbundling. The background retrieval data source order is configurable using the BACKGROUND_RETRIEVAL_ORDER environment variable and defaults to chunks,s3,trusted-gateway,tx-data. Priority is given to chunk retrieval since chunks are verifiable.Added an /ar-io/admin/export-parquet/status to support monitoring of in-progress Parquet export status.Added sqlite_in_flight_ops Prometheus metric with worker (core, bundles, data, or moderation) and role (read or write) labels to support monitoring the number of in-flight DB operations.Added experimental Grafana and Prometheus based observability stack. See the \\\"Monitoring and Observability\\\" section of the README for more details.Changed Bundle data is now retrieved as chunks from Arweave nodes by default so that data roots can be compared against the chain (see entry about background retrieval above).Changed observer configuration to use 8 instead of 5 chosen names. These are combined with 2 names prescribed from the contract for a total of 10 names observed each epoch to provide increased ArNS observation coverage.Verification status is set on data items when unbundling a parent that has already been verified.[Release 18] - 2024-10-01 Fixed Improved performance of data attributes query that was preventing data.db WAL flushing.Added Added WAL sqlite_wal_checkpoint_pages Prometheus metric to help monitor WAL flushing.Added a POST /ar-io/admin/export-parquet endpoint that can be used to export the contents of the SQLite3 core and bundle DBs as Parquet. To trigger an export, POST JSON containing outputDir, startHeight, endHeight, and maxFileRows keys. The resulting Parquet files can then be queried directly using DuckDB or loaded into another system (e.g. ClickHouse). Scripts will be provided to help automate the latter in a future release.Added ARNS_RESOLVER_OVERRIDE_TTL_SECONDS that can be used to force ArNS names to refresh before their TTLs expire.Added a GET /ar-io/resolver/:name endpoint that returns an ArNS resolution for the given name.Changed Removed ArNS resolver service in favor of integrated resolver. If a standalone resolver is still desired, the core service can be run with the START_WRITERS environment variable set to false. This will disable indexing while preserving resolver functionality.Deduplicated writes to data.db to improve performance and reduce WAL growth rate.[Release 17] - 2024-09-09 Notes This release includes a LONG RUNNING MIGRATION. Your node may appear unresponsive while it is running. It is best to wait for it to complete. If it fails or is interrupted, removing your SQLite DBs (in data/sqlite by default) should resolve the issue, provided you are willing to lose your GraphQL index and let your node rebuild it.Fixed Use the correct environment variable to populate WEBHOOK_BLOCK_FILTER in docker-compose.yaml.Don't cache data regions retrieved to satisfy range requests to avoid unnecessary storage overhead and prevent inserting invalid ID to hash mappings into the data DB.Added Added a new ClickHouse based DB backend. It can be used in combination with the SQLite DB backend to enable batch loading of historical data from Parquet. It also opens up the possibility of higher DB performance and scalability. In its current state it should be considered a technology preview. It won't be useful to most users until we either provide Parquet files to load into it or automate flushing of the SQLite DB to it (both are planned in future release). It is not intended to be standalone solution. It supports bulk loading and efficient GraphQL querying of transactions and data items, but it relies on SQLite (or potentially another OLTP in the future) to index recent data. These limitations allow greatly simplified schema and query construction. Querying the new ClickHouse DB for transaction and data items via GraphQL is enabled by setting the CLICKHOUSE_URL environment variable.Added the ability to skip storing transaction signatures in the DB by setting WRITE_TRANSACTION_DB_SIGNATURES to false. Missing signatures are fetched from the trusted Arweave node when needed for GraphQL results.Added a Redis backed signature cache to support retrieving optimistically indexed data item signatures in GraphQL queries when writing data items signatures to the DB has been disabled.Added on-demand and composite ArNS resolvers. The on-demand resolver fetches results directly from an AO CU. The composite resolver attempts resolution in the order specified by the ARNS_RESOLVER_PRIORITY_ORDER environment variable (defaults to on-demand,gateway).Added a queue_length Prometheus metric to fasciliate monitoring queues and inform future optimizations Added SQLite WAL cleanup worker to help manage the size of the data.db-wal file. Future improvements to data.db usage are also planned to further improve WAL management.Changed Handle data requests by ID on ArNS sites. This enables ArNS sites to use relative links to data by ID.Replaced ARNS_RESOLVER_TYPE with ARNS_RESOLVER_PRIORITY_ORDER (defaults to on-demand,gateway).Introduced unbundling back pressure. When either data item data or GraphQL indexing queue depths are more than the value specified by the MAX_DATA_ITEM_QUEUE_SIZE environment variable (defaults to 100000), unbundling is d until the queues length falls bellow that threshold. This prevents the gateway from running out of memory when the unbundling rate exceeds the indexing rate while avoiding wasteful bundle reprocessing.Prioritized optimistic data item indexing by inserting optimistic data items at the front of the indexing queues.Prioritized nested bundle indexing by inserting nested bundles at the front of the unbundling queue.[Release 16] - 2024-08-09 Fixed Fixed promise leak caused by missing await when saving data items to the DB.Modified ArNS middleware to not attempt resolution when receiving requests for a different hostname than the one specified by ARNS_ROOT_HOST.Added Added support for returning Content-Encoding HTTP headers based on user specified Content-Encoding tags.Added isNestedBundle filter enables that matches any nested bundle when indexing. This enables composite unbundling filters that match a set of L1 tags and bundles nested under them.Added ability to skip writing ANS-104 signatures to the DB and load them based on offsets from the data instead. This significantly reduces the size of the bundles DB. It can be enabled by setting the WRITE_ANS104_DATA_ITEM_DB_SIGNATURES environment variable to false.Added data_item_data_indexed_total Prometheus counter to count data items with data attributes indexed.Changed Queue data attributes writes when serving data rather than writing them syncronously.Reduced the default data indexer count to 1 to lessen the load on the data DB.Switched a number of overly verbose info logs to debug level.Removed docker-compose on-failure restart limits to ensure that services restart no matter how many times they fail.Modified the data_items_indexed_total Prometheus counter to count data items indexed for GraphQL querying instead of data attributes.Increased aggressiveness of contiguous data cleanup. It now s 5 seconds instead of 10 seconds per batch and runs every 4 hours instead of every 24 hours.[Release 15] - 2024-07-19 Fixed Fixed query error that was preventing bundles from being marked as fully imported in the database.Added Adjusted data item indexing to record data item signature types in the DB. This helps distinguish between signatures using different key formats, and will enable querying by signature type in the future.Adjusted data item indexing to record offsets for data items within bundles and signatures and owners within data items. In the future this will allow us to avoid saving owners and signatures in the DB and thus considerably reduce the size of the bundles DB.Added ARNS_CACHE_TTL_MS environment variable to control the TTL of ARNS cache entries (defaults to 1 hour).Added support for multiple ranges in a single HTTP range request.Added experimental chunk POST endpoint that broadcasts chunks to the comma-separate list of URLS in the CHUNK_BROADCAST_URLS environment variable. It is available at /chunk on the internal gateway service port (4000 by default) but is not yet exposed through Envoy.Added support for running an AO CU adjacent to the gateway (see README.md for details).Added X-ArNS-Process-Id to ArNS resolved name headers.Added a set of AO_... environment variables for specifying which AO URLs should be used (see docker-compose.yaml for the complete list). The AO_CU_URL is of particular use since the core and resolver services only perform AO reads and only the CU is needed for reads.Changed Split the monolithic docker-compose.yaml into docker-compose.yaml, docker-compose.bundler.yaml, and docker-compose.ao.yaml (see README for details).Replaced references to 'docker-compose' with 'docker compose' in the docs since the former is mostly deprecated.Reduce max fork depth from 50 to 18 inline to reflect Arweave 2.7.2 protocol changes.Increased the aggressiveness of bundle reprocessing by reducing reprocessing interval from 10 minutes to 5 minutes and raising reprocessing batch size from 100 to 1000.Use a patched version of Litestream to work around insufficient S3 multipart upload size in the upstream version.[Release 14] - 2024-06-26 Fixed Correctly handle manifest index after paths.[Release 13] - 2024-06-24 Added Added support for optimistically reading data items uploaded using the integrated Turbo bundler via the LocalStack S3 interface.Added X-AR-IO-Origin-Node-Release header to outbound data requests.Added hops, origin, and originNodeRelease query params to outbound data requests.Added support for fallback in v0.2 manifests that is used if no path in the manifest is matched.Changed Updated Observer to read prescribed names from and write observations to the ar.io AO network process.Updated Resolver to read from the ar.io AO network process.Fixed Modified optimistic indexing of data items to use a null parent_id when inserting into the DB instead of a placeholder value. This prevents unexpected non-null bundledIn values in GraphQL results for optimistically indexed data items.Modified GraphQl query logic to require an ID for single block GraphQL queries. Previously queries missing an ID were returning an internal SQLite error. This represents a small departure from arweave.net's query logic which returns the latest block for these queries. We recommend querying blocks instead of block in cases where the latest block is desired.Adjusted Observer health check to reflect port change to 5050.Security Modified docker-compose.yaml to only expose Redis, PostgreSQL, and LocalStack ports internally. This protects gateways that neglect to deploy behind a firewall, reverse proxy, or load balancer.[Release 12] - 2024-06-05 Added Added /ar-io/admin/queue-data-item endpoint for queuing data item headers for indexing before the bundles containing them are processed. This allows trusted bundlers to make their data items quickly available to be queried via GraphQL without having to wait for bundle data submission or unbundling.Added experimental support for retrieving contiguous data from S3. See AWS_* environment variables documentation for configuration details. In conjuction with a local Turbo bundler this allows optimistic bundle (but not yet data item) retrieval.Add experimental support for fetching data from gateway peers. It can be enabled by adding ario-peer to ON_DEMAND_RETRIEVAL_ORDER. Note: do not expect this work reliably yet! This functionality is in active development and will be improved in future releases.Add import_attempt_count to bundle records to enable future bundle import retry optimizations.Changed Removed version from docker-compose.yaml to avoid warnings with recent versions of docker-compose.Switched default observer port from 5000 to 5050 to avoid conflict on OS X. Since Envoy is used to provide external access to the observer API this should have no user visible effect.[Release 11] - 2024-05-21 Added Added arweave_tx_fetch_total Prometheus metric to track counts of transaction headers fetched from the trusted node and Arweave network peers.Changed Revert to using unnamed bind mounts due to cross platform issues with named s.[Release 10] - 2024-05-20 Added Added experimental support for streaming SQLite backups to S3 (and compatible services) using Litestream. Start the service using the docker-compose \\\"litestream\\\" profile to use it, and see the AR_IO_SQLITE_BACKUP_* environment variables documentation for further details.Added /ar-io/admin/queue-bundle endpoint for queueing bundles for import for import before they're in the mempool. In the future this will enable optimistic indexing when combined with a local trusted bundler.Added support for triggering webhooks when blocks are imported matching the filter specified by the WEBHOOK_BLOCK_FILTER environment variable.Added experimental support for indexing transactions and related data items from the mempool. Enable it by setting ENABLE_MEMPOOL_WATCHER to 'true'.Made on-demand data caching circuit breakers configurable via the GET_DATA_CIRCUIT_BREAKER_TIMEOUT_MS environment variable. This allows gateway operators to decide how much latency they will tolerate when serving data in exchange for more complete data indexing and caching.Rename cache header from X-Cached to X-Cache to mimic typical CDN practices.Add X-AR-IO-Hops and X-AR-IO-Origin headers in preparation for future peer-to-peer functionality.Upgrade to Node.js v20 and switch to native test runner.[Release 9] - 2024-04-10 Added Added experimental Farcaster Frames support, enabling simple Arweave based Frames with button navigation. Transaction and data item data is now served under /local/farcaster/frame/. /local is used as a prefix to indicate this functionality is both experimental and local to a particular gateway rather than part of the global gateway API. Both GET and POST requests are supported.Added an experimental local ArNS resolver. When enabled it removes dependence on arweave.net for ArNS resolution! Enable it by setting RUN_RESOLVER=TRUE, TRUSTED_ARNS_RESOLVER_TYPE=resolver, and TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 in your .env file.Added an X-Cached header to data responses to indicate when data is served from the local cache rather than being retrieved from an external source. This is helpful for interfacing with external systems, debugging, and end-to-end testing.Save hashes for unbundled data items during indexing. This enables reduction in data storage via hash based deduplication as well as more efficient peer-to-peer data retrieval in the future.[Release 8] - 2024-03-14 Added Added GraphQL SQL query debug logging to support trouble-shooting and performance optimization.Added support for indexing data items (not GraphQL querying) based solely on tag name. (example use case: indexing all IPFS CID tagged data items).Changes Observer data sampling now uses randomized ranges to generate content hashes.Reference gateway ArNS resolutions are now cached to improve report generation performance.Contract interactions are now tested before posting using dryWrite to avoid submitting interactions that would fail./ar-io/observer/info now reports INVALID for wallets that fail to load.Fixed Fix data caching failure caused by incorrect method name in getData circuit breakers.Fix healthcheck when ARNS_ROOT_HOST includes a subdomain.[Release 7] - 2024 - 02 - 14 Added Add support for notifying other services of transactions and data items using webhooks (see README for details).Add support for filter negation (particularly useful for excluding large bundles from indexint).Improve unbundling throughput by decoupling data fetching from unbundling.Add Envoy and core service ARM builds.Changed Improve resouce cleanup and shutdown behavior.Don't save Redis data to disk by default to help prevent memory issues on startup for small gateways.Reduce the amount of data sampled from large files by the observer.Ensure block poa2 field is not chached to reduce memory consumption.[Release 6] - 2024-01-29 Fixed Update observer to improve reliability of contract state synchronization and evaluation.[Release 5] - 2024-01-25 Added Added transaction offset indexing to support future data retrieval capabilities.Enabled IPv6 support in Envoy config.Added ability to configure observer report generation interval via the REPORT_GENERATION_INTERVAL_MS environmental variable. (Intended primarily for development and testing) Changed Updated observer to properly handle FQDN conflicts.Renamed most created_at columns to index to indexed_at for consistency and clarity.Fixed Updated LMDB version to remove Buffer workaround and fix occasional block cache errors.[Release 4] - 2024-01-11 Added Added circuit breakers around data index access to reduce impact of DB access contention under heavy requests loads.Added support for configuring data source priority via the ON_DEMAND_RETRIEVAL_ORDER environment variable.Updated observer to a version that retrieves epoch start and duration from contract state.Changed Set the Redis max memory eviction policy to allkeys-lru.Reduced default Redis max memory from 2GB to 256MB.Improved predictability and performance of GraphQL queries.Eliminated unbundling worker threads when filters are configured to skip indexing ANS-104 bundles.Reduced the default number of ANS-104 worker threads from 2 to 1 when unbundling is enabled to conserve memory.Increased nodejs max old space size to 8GB when ANS-104 workers > 1.Fixed Adjusted paths for chunks indexed by data root to include the full data root.[Release 3] - 2023-12-05 Added Support range requests (PR 61, PR 64) Note: serving multiple ranges in a single request is not yet supported.Release number in /ar-io/info response.Redis header cache implementation (PR 62).New default header cache (replaces old FS cache).LMDB header cache implementation (PR 60).Intended for use in development only.Enable by setting CHAIN_CACHE_TYPE=lmdb.Filesystem header cache cleanup worker (PR 68).Enabled by default to cleanup old filesystem cache now that Redis is the new default.Support for parallel ANS-104 unbundling (PR 65).Changed Used pinned container images tags for releases.Default to Redis header cache when running via docker-compose.Default to LMDB header cache when running via yarn start.Fixed Correct GraphQL pagination for transactions with duplicate tags.\",\"domain\":\"ario\",\"relevanceScore\":12,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ario-llms.txt\"},{\"content\":\"# 65. Glossary - ARIO Docs\\n\\nDocument Number: 65\\nSource: https://docs.ar.io/glossary\\nWords: 1405\\nQuality Score: 0.465\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nMany novel terms and acronyms are used by the Arweave ecosystem as well as some new ones introduced by AR.IO. The list below is intended to serve as a non-exhaustive reference of those terms. For a comprehensive glossary of permaweb-specific terminology, check out the permaweb glossary section:AO Computer (AO):The AO Computer is an actor-oriented machine on the Arweave network, creating a unified computing environment across diverse nodes. It supports many parallel processes through an open message-passing layer, linking independent processes into a cohesive system, similar to how websites are interconnected via hyperlinks.Arweave Name System (ArNS):a decentralized and censorship-resistant naming system enabled by AR.IO gateways which connects friendly names to permaweb applications, pages, data or identities.Arweave Name Token (ANT), \\\"Name Token\\\":A an AO Computer based token, that is connected to each registered ArNS Name. Each ANT gives the owner the ability to update the subdomains and Arweave Transaction IDs used by the registered name as well as transfer ownership and other functions.Arweave Network Standards (ANS):Drafts and finalized standards for data formats, tag formats, data protocols, custom gateway features and anything that is built on top the Arweave Network. Specific standards are denoted by an associated number, e.g., ANS-###.Base Layer Transaction:refers to one of up to 1,000 transactions that make up a single Arweave block. A base layer transaction may contain bundled data items.Bundle, bundling:an Arweave concept introduced in ANS-104 that allows for a way of writing multiple independent data transactions into one base layer transaction. Bundled transactions contain multiple independent transactions, called data items, wrapped into one larger transaction. This offers two major network benefits:A scaling solution for increasing the throughput of uploads to the Arweave network,Allows delegation of payment for an upload to a third party, while maintaining the identity and signature of the person who created the upload, without them needing to have a wallet with funds.Bundled Data Item (BDI):A data item / transaction nested within an ANS-104 bundled transaction.Bundler:A third-party service and gateway feature that bundles data files on a user's behalf.Chunk:A chunk is a unit of data that is stored on the Arweave network. It represents a piece of a larger file that has been split into smaller, manageable segments for efficient storage and retrieval.Decentralized, decentralization, etc:A nonbinary, many axis scale enabling a system or platform to be: permissionless, trustless, verifiable, transparent, open-source, composable, resilient, and censorship resistant. Ultimately, something that is decentralized is not prone to single points of failure or influence.Epoch:a specific duration (e.g., one day) during which network activities and evaluations are conducted. It serves as a key time frame for processes such as observation duties, performance assessments, and reward distributions within the network's protocols.Gateway:A node operating on the Arweave network that provides services for reading from, writing to, and indexing the data stored on the permaweb. Sometimes referred to as \\\"permaweb nodes\\\".Gateway Address Registry (GAR):a decentralized directory maintained in the AR.IO smart contract. It serves as the authoritative list of all registered gateways on the AR.IO Network. The registry provides detailed metadata about each gateway to facilitate discovery, health monitoring, and data sharing among apps, users and other infrastructure. The GAR is designed to be easily queryable, sortable, and filterable by end users and clients, allowing for tailored selections based on various criteria to meet specific use cases.Indexing:The act of organizing transaction data tags into queryable databases.Layer 2 Infrastructure:Layer 2 refers to the technology / infrastructure stack built \\\"above\\\" a base layer. In this use, the AR.IO Network would be considered Layer 2 infrastructure to the base Arweave protocol.Manifest (aka Path Manifest, Arweave Manifest):Special \\\"aggregate\\\" files uploaded to Arweave that map user-definable sub-paths with other Arweave transaction IDs. This allows users to create logical groups of content, for example a directory of related files, or the files and assets that make up a web page or application. Instead of having to manually collate these assets, manifests group them together so that an entire website or app can be launched from a single manifest file. Gateways can interpret this structure, so that users can then reference individual transactions by their file name and/or path.Mempool:Short for \\\"memory pool,\\\" is a component of Arweave mining nodes that temporarily stores valid transactions that have been broadcasted to the network but have not yet been added to a block.Message:An interaction with an AO Process, including action and tags. Every interaction with AO takes the form of a message.Miner (aka Arweave Node):A node operating on the Arweave network responsible for data storage and recall.Native Address:The way public addresses are commonly (or by spec) represented in their native blockchain. Arweave keys are 43 character base64url representations of the public key, while Ethereum keys use a different hashing algorithm and start with 0x etc.Normalized Address:43 character base64url representation of the sha256 hash of a public key. Public keys for other chains can be normalized by this representation.Observer:A gateway selected to evaluate the performance of peer gateways in resolving ArNS names. Observers assess and report on the operational efficacy of other gateways.Optimistic Indexing:Indexing transaction or data item headers before the associated L1 transaction has been accepted and confirmed in a chain block.Owner:Generally, the public key of the signer.Owner Address:The normalized address of the owner Period:Refers to a predefined time span (e.g., a day) that serves as a cycle for network activities such as dynamic pricing. It is a fundamental unit of time for operational and protocol processes within the network.Permanent Cloud Network:A decentralized network that securely stores, distributes, and serves data and applications in a timeless, tamper-proof, and universally accessible way. Unlike traditional clouds, it ensures data permanence and user sovereignty by eliminating reliance on centralized providers and creating a resilient, censorship-resistant infrastructure.Permaweb:The permaweb is the permanent and decentralized web of files and applications built on top of Arweave.Process:Process: A decentralized computation unit in the AO framework, enabling scalable, parallel execution via message-passing. Each process maintains its own state, interacts asynchronously, and is permanently stored on Arweave for transparency and immutability.Process ID (PID):Every process in AO is assigned a unique immutable identifier code.Protocol Balance:The primary sink and source of ARIO tokens circulating through the AR.IO Network. This balance is akin to a central vault or wallet programmatically encoded into the network's smart contract from which ArNS revenue is accumulated and incentive rewards are distributed.Protocol Rewards:ARIO Token incentive rewards distributed by the protocol to the network's eligible users and gateway operators.Public Key:The publicly known keys for a signer (wallet). Public keys are different byte lengths depending on the signer type (e.g. Arweave vs. Ethereum (ECDSA), vs Solana, etc.) Seeding:Refers to the act of propagating new data throughout the network. Miner nodes seed Arweave base layer transaction data to other miners, while gateways ensure that the transactions they receive reach the Arweave nodes. Both gateways and Arweave nodes seed base layer transactions and data chunks.Staking (of tokens):Refers to the process of locking ARIO tokens into a protocol-facilitated vault, temporarily removing them from circulation until unlocked. This action represents an opportunity cost for the gateway operator and serves as a motivator to prioritize the network's collective interests.Stake Redelegation:The process by which stakers move their delegated tokens from one gateway to another.Stake Redemption:A feature allowing stakers to use their staked tokens for ArNS-related activities, such as purchasing names, extending leases, or increasing undername capacity.Transaction ID (txID):Every transaction and data file uploaded to Arweave is assigned a unique identifier code known as the Transaction ID. These txID's can be referenced by users to easily locate and retrieve files.Trust-minimization:Relates to enacting network security by minimizing the number of entities and the degree to which they must be trusted to achieve reliable network interactions. A network with trust-minimizing mechanisms means that it has reduced exposure to undesirable third-party actions and built-in incentives to reward good behavior while punishing bad behavior.Vault:Token vaults are protocol level mechanisms used to contain staked tokens over time. Each vault contains a starting timestamp, ending timestamp (if applicable), along with a balance of tokens.Wayfinder Protocol:The Wayfinder protocol provides applications with a pattern for dynamically switching / routing between network gateways. It also allows for abstraction of top level domain names from Arweave data and verifies the responses from AR.IO Gateways. It forms the basis of the ar:// schema, so users can seamlessly access ArNS names, Arweave base layer transactions, and bundled data items without the user providing a top-level domain.Permaweb Glossary For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below:\",\"domain\":\"ario\",\"relevanceScore\":11,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ario-llms.txt\"},{\"content\":\"# 13. ArNS - Arweave Name System  Cooking with the Permaweb\\n\\nDocument Number: 13\\nSource: https://cookbook.arweave.net/concepts/arns.html\\nWords: 527\\nQuality Score: 0.520\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nArNS - Arweave Name System Overview The Arweave Name System (ArNS) is the phonebook of the PermaWeb.It is a decentralized and censorship-resistant naming system that is enabled by AR.IO Gateways and used to connect friendly names to PermaWeb apps, pages and data.This system works similarly to traditional DNS, where a user can purchase a name in a registry and DNS Name servers resolve these names to IP addresses.With ArNS, the registry is decentralized, permanent and stored on Arweave and each AR.IO gateway acts as both cache and name resolver. Users can register a name within the ArNS Registry, like \\\"my-name\\\" and set a pointer to any Arweave Transaction ID. AR.IO Gateways will resolve that name as one of their own subdomains, eg. https://laserilla.arweave.net and proxy all requests to the associated Arweave Transaction ID. Each registered name can also have under names associated with it that each point to an Arweave Transaction ID, like https://v1_laserilla.arweave.net, giving even more flexibility and control to its owner.The ArNS Registry ArNS uses the Smartweave protocol manage its name records. Each record, or name, is leased by a user and tied to an ANT token. You can register multiple ArNS names to a single ANT, but you cannot register multiple ANTs to a single ArNS name - the gateways wouldn't know where to point the routing ID.ArNS names can be up to 32 characters, including numbers [0-9], letters [a-z], and dashes [-]. The dashes cannot be trailing dashes, e.g. -myname.ANTs (Arweave Name Tokens) ANTs are a crucial part of the ArNS ecosystem - they are the actual key to owning an ArNS name. When you register an ArNS name to an ANT, the ANT then becomes the transfer method for that name. The ArNS registry does not care who owns the ANT, it simply knows what name ANT it belongs to.Within ANTs you can build out whatever functionality you wish, within the scope ArNS registry approved source code transaction list. Up to and including NFT's, PST's, DAO's, or full on applications.Under_Names Undernames are records held and managed by your ANT (Arweave Name Token). These records can be created and managed without even owning an ARNS name, and will be transferred along with the ant when sent to a new owner. Likewise if your ArNS name expires, and you register your ANT to a new ArNS name, all your undername will remain intact.Example: you own oldName.arweave.net.then: You create the undername \\\"my\\\" - my_oldName.arweave.net.then: oldName.arweave.net expires, and you register newName.arweave.net to your ANT.now: my_ undername is accessable on newName - my_newName.arweave.net.Below is an example of an ANT contract State:{\\nbalances:{ QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ : 1 },\\ncontroller: \\\"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\\\",\\nevolve: null,\\nname: \\\"ArDrive OG Logo\\\",\\nowner: \\\"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\\\",\\nrecords:{\\n@:{ transactionId: \\\"xWQ7UmbP0ZHDY7OLCxJsuPCN3wSUk0jCTJvOG1etCRo\\\" },\\nundername1:{ transactionId: \\\"usOLUmbP0ZHDY7OLCxJsuPCN3wSUk0jkdlvOG1etCRo\\\" }\\n},\\nticker:\\\"ANT-ARDRIVE-OG-LOGO\\\"\\n} the base \\\"@\\\" record is the initial routing id for the ANT. if you registered 'my-name' to this ANT, and tried to access it via my-name.arweave.net, you would be redirected to the @ record's transactionId.if you tried to access undername1_my-name.arweave.net, you would get 'undername1's transactionId.ANT's, in theory, have an UNLIMITED number of undernames. However, how many will be served depends on which tier is used with your ArNS name.\",\"domain\":\"arweave\",\"relevanceScore\":10,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/arweave-llms.txt\"},{\"content\":\"# 22. Querying Arweave with GraphQL  Cooking with the Permaweb\\n\\nDocument Number: 22\\nSource: https://cookbook.arweave.net/guides/querying-arweave/queryingArweave.html\\nWords: 822\\nQuality Score: 0.500\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nQuerying Arweave with GraphQL Arweave provides a simple way of querying for transactions and filtering them by tags. Arweave GraphQL-compatible indexing services provide endpoints users can post GraphQL queries to, and also provide a ground for trying queries.GraphQL is a flexible query language that services can use to build a customized data schema for clients to query. GraphQL also allows clients to specify which elements of the available data structure they would like to see in the results.Public Indexing Services arweave.net graphql the original graphql endpoint, managed by ar.io goldsky search service a public service specifically optimized for search using a superset of the graphql syntax, managed by goldsky ar.io decentralized indexing A decentralized network for indexing services. Currently in testing with L1 transactions available.knn3 arseeding indexing, one for arseeding trading can real-time query service.Executing a GraphQL Query To query arweave we’ll need to access it through an indexing service that supports GraphQL. Use one of the GraphQL grounds listed above to get started!Copy and paste in the following query query {\\ntransactions(tags: [{\\nname: \\\"App-Name\\\",\\nvalues: [\\\"PublicSquare\\\"]\\n}])\\n{\\nedges {\\nnode {\\nid\\ntags {\\nname\\nvalue\\n}\\n}\\n}\\n}\\n} If you’re not familiar with GraphQL it can seem a little overwhelming at first but once you know the structure, it’s fairly easy to read and understand.query { ( ) { } } In the example query we pasted our is transactions but we could also query for blocks. A full description of Arweave's GraphQL schema is written up in the Arweave GraphQL Guide. The guide refers to the filter criteria as “Query Structures” and the complete data structure definition of transactions and blocks as “Data Structures”.When it comes to the , the thing to note is that you can specify a subset of the complete data structure you’re interested in. For example, the complete data structure for a transactions schema is listed here.In our case we’re interested in the id and complete list of tags for any transaction matching our filter criteria.Hit the big “” button in the middle of the ground to run the query. You’ll notice we get back a list of transactions in the results data structure we specified in our original query.If you’re new to blockchains this is unexpected, we haven’t built anything, why do these results exist? It turns out, the “PublicSquare”: “App-Name” tag we’ve filtered for has been in use for a while.Arweave protocol's founder, Sam Williams, proposed the transaction format a few years ago in a github code snippet. Since then builders in the ecosystem have been building on and around it, experimenting, posting transactions with those tags.Back to querying Arweave. You’ll notice in the GraphQL results that there are no readable post messages, just tags and information about posts.This is because the GraphQL indexing service is concerned with indexing and retrieving header data for transactions and blocks but not their associated data.To get the data of a transaction we need to look it up using another HTTP endpoint.https://arweave.net/ Copy and paste one of the id’s in your query results and modify the above link, appending the id. It should look something like this… https://arweave.net/eaUAvulzZPrdh6_cHwUYV473OhvCumqT3K7eWI8tArk The result of navigating to that URL in the browser (HTTP GET) would be retrieving the content of the post (stored in the transactions data). In this example it’s… Woah that's pretty cool 😎 (For a complete listing arweave HTTP endpoints visit the HTTP API documentation.) Posting a Query From JavasScript Posting a GraphQL query from javascript isn't much different than posting it in the ground.First install the arweave-js package for easy access to a GraphQL endpoint.npm install --save arweave Then enter a slightly more advanced version of the example query from above and await the results of posting it.import Arweave from 'arweave';\\n// initialize an arweave instance\\nconst arweave = Arweave.init({});\\n// create a query that selects tx data the first 100 tx with specific tags\\nconst queryObject = {\\nquery:\\n`{\\ntransactions(\\nfirst:100,\\ntags: [\\n{\\nname: \\\"App-Name\\\",\\nvalues: [\\\"PublicSquare\\\"]\\n},\\n{\\nname: \\\"Content-Type\\\",\\nvalues: [\\\"text/plain\\\"]\\n}\\n]\\n)\\n{\\nedges {\\nnode {\\nid\\ntags {\\nname\\nvalue\\n}\\n}\\n}\\n}\\n}`\\n};\\nconst results = await arweave.api.post('/graphql', queryObject);Multiple Queries It is possible to post multiple queries in a single round-trip to the GraphQL endpoint. This example queries the name transaction (each as a separate query) for two wallet addresses using the now obsolete (replaced by ar-profile) but still permanent arweave-id protocol.query {\\naccount1: transactions(first: 1, owners:[\\\"89tR0-C1m3_sCWCoVCChg4gFYKdiH5_ZDyZpdJ2DDRw\\\"],\\ntags: [\\n{\\nname: \\\"App-Name\\\",\\nvalues: [\\\"arweave-id\\\"]\\n},\\n{\\nname: \\\"Type\\\",\\nvalues: [\\\"name\\\"]\\n}\\n]\\n) {\\nedges {\\nnode {\\nid\\nowner {\\naddress\\n}\\n}\\n}\\n}\\naccount2: transactions(first: 1, owners:[\\\"kLx41ALBpTVpCAgymxPaooBgMyk9hsdijSF2T-lZ_Bg\\\"],\\ntags: [\\n{\\nname: \\\"App-Name\\\",\\nvalues: [\\\"arweave-id\\\"]\\n},\\n{\\nname: \\\"Type\\\",\\nvalues: [\\\"name\\\"]\\n}\\n]\\n) {\\nedges {\\nnode {\\nid\\nowner {\\naddress\\n}\\n}\\n}\\n}\\n} Resources Arweave GQL Reference ArDB package ar-gql package Search Indexing Service\",\"domain\":\"arweave\",\"relevanceScore\":10,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/arweave-llms.txt\"},{\"content\":\"# 31. Warp WriteInteractions  Cooking with the Permaweb\\n\\nDocument Number: 31\\nSource: https://cookbook.arweave.net/guides/smartweave/warp/write-interactions.html\\nWords: 353\\nQuality Score: 0.484\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\n⚠️ Deprecation Notice This document is deprecated and may contain outdated information.To call a function on a SmartWeave contract, you can create a transaction known as a SmartWeave action. This action includes the function name and the necessary input parameters for the function on the SmartWeave contract. You can create a SmartWeave action using the contract.writeInteraction function.Code import { WarpFactory } from 'warp-contracts'\\nconst warp = WarpFactory.forMainnet()\\nconst STAMP_PROTOCOL = 'FMRHYgSijiUNBrFy-XqyNNXenHsCV0ThR4lGAPO4chA'\\nasync function doStamp() {\\nconst result = await warp.contract(STAMP_PROTOCOL)\\n.connect('use_wallet')\\n.writeInteraction({\\nfunction: 'stamp',\\ntimestamp: Date.now(),\\ntransactionId: 'zQhANphTO0DOsaWXhExylUD5cBN3a6xWvfn5ZCpmCVY'\\n})\\nconsole.log(result)\\n} When calling writeInteraction, you need to pass your input parameters, these are the parameters the contract is expecting to receive.WARNING Since SmartWeave contracts are evaluated in a lazy flow, you do not know if your interaction ran successfully until you evaluate the contract to the current state. Use Warp readState to access the contract and determine if the interaction was applied successfully.Dry Write DryWrite allows you to test and verify an interaction on the current state without actually executing it on the permaweb. This feature allows you to simulate the interaction locally and ensure that it will be successful before applying it.import { WarpFactory } from 'warp-contracts'\\nconst warp = WarpFactory.forMainnet()\\nconst STAMP_PROTOCOL = 'FMRHYgSijiUNBrFy-XqyNNXenHsCV0ThR4lGAPO4chA'\\nasync function doStamp() {\\nconst result = await warp.contract(STAMP_PROTOCOL)\\n.connect('use_wallet')\\n.dryWrite({\\nfunction: 'stamp',\\ntimestamp: Date.now(),\\ntransactionId: 'zQhANphTO0DOsaWXhExylUD5cBN3a6xWvfn5ZCpmCVY'\\n})\\nconsole.log(result)\\n} WARNING One thing to note when using dry writes, is that the entire state needs to be evaluated locally for contacts that use readState or internalWrites. This can result in a slow performing process.Optimized for speed By default, writeInteractions are submitted to the Warp Sequencer and bundled and posted to Arweave. You can post directly to Arweave by disabling bundling.const result = await contract.writeInteraction({\\nfunction: 'NAME_OF_YOUR_FUNCTION',\\n...\\n}, { disableBundling: true }) Summary The SmartWeave Protocol allows for the modification of dynamic data on an immutable, append-only storage system using writeInteractions. These interactions enable trustless and permissionless communication with SmartWeave contracts. The Warp SDK provides developers with a user-friendly API for interacting with the SmartWeave Protocol and its writeInteractions feature.For additional resources:Warp SDK https://github.com/warp-contracts/warp Warp Docs https://warp.cc\",\"domain\":\"arweave\",\"relevanceScore\":10,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/arweave-llms.txt\"},{\"content\":\"# 8. AO Processes  Cookbook\\n\\nDocument Number: 8\\nSource: https://cookbook_ao.arweave.net/welcome/ao-processes.html\\nWords: 442\\nQuality Score: 0.520\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nAO Processes AO Processes are persistent, programmable smart contracts that live inside the AO computer. Embodying the actor model from Erlang that inspired AO, these processes operate as independent computational units that have their own state and communicate with each other through message passing. This architecture makes them ideal for creating autonomous agents and complex decentralized applications.What are AO Processes?Following the actor model, each AO Process functions as an independent actor within the system, executing code—typically written in Lua—in response to messages it receives. Three core characteristics define them:Stateful: Each process has its own private state and memory, which persist across interactions.Persistent: All processes and their entire message history are permanently stored on Arweave.Generative: Processes can dynamically spawn new processes, enabling complex and evolving systems.AO Processes and the Actor Model The actor model provides several key benefits for process-based development, enabling naturally concurrent and resilient systems. By treating every process as an isolated \\\"actor,\\\" it simplifies development and enhances fault tolerance. Key advantages include:Concurrency & Isolation: Processes execute independently and are isolated from each other, enabling parallelism and preventing cascading failures.Message-Passing: All communication happens exclusively through asynchronous messages, simplifying interactions.Location Transparency & Fault Tolerance: Processes can interact without knowing each other's physical location on the network, and the system can continue operating even if individual processes fail.AOS: The Operating System for AO Processes AOS (AO Operating System) is an abstraction layer designed to simplify interaction with AO Processes. It provides developers with a powerful shell interface for sending commands, tools for managing process state, and a set of libraries for common functionalities, all contributing to a more streamlined development experience.Use Cases for AO Processes The persistent and concurrent nature of AO Processes makes them ideal for a wide range of decentralized applications. Here are a few examples:Autonomous Agents & Bots: Imagine a price-monitoring bot that tracks token prices across different decentralized exchanges (DEXs) and executes arbitrage trades automatically. AO makes it possible to build entire marketplaces for such agents, like Marketverse.Decentralized Finance (DeFi): You could build automated market makers (AMMs) or lending protocols where account balances and token reserves are tracked persistently within the process's state. A live example of this is Dexi, a decentralized exchange built on AO.On-Chain Games & Social Platforms: AO Processes can power fully on-chain games where the game state (like er positions or inventory) is managed by one or more processes, like the space strategy game Stargrid. They're also perfect for decentralized chat applications or social networks where user profiles, posts, and interactions are censorship-resistant.Now that you understand the capabilities of AO Processes, the next step is to dive into Hyperbeam, the high-performance network that powers them.\",\"domain\":\"ao\",\"relevanceScore\":10,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ao-llms.txt\"},{\"content\":\"# 25. Exposing Process State to HyperBEAM  Cookbook\\n\\nDocument Number: 25\\nSource: https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/exposing-process-state.html\\nWords: 578\\nQuality Score: 0.501\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nExposing Process State to HyperBEAM HyperBEAM introduces a powerful feature for exposing parts of a process's state for immediate reading over HTTP. This improves performance for web frontends and data services by replacing the need for dryrun calls, which were a known bottleneck on legacynet.The Patch Device The ~patch@1.0 device is the mechanism that allows AO processes to make parts of their internal state readable via direct HTTP GET requests.How it Works Exposing state is a four-step process involving your process and HyperBEAM:Process Logic: From your process (e.g., in Lua or WASM), send an outbound message to the ~patch@1.0 device.Patch Message Format: The message must include device and cache tags.HyperBEAM Execution: HyperBEAM's dev_patch module processes this message, mapping the key-value pairs from the cache table to a URL path.HTTP Access: The exposed data is then immediately available via a standard HTTP GET request to the process's endpoint.HyperBEAMGET /~process@1.0/compute/cache/ Initial State Sync (Optional) To make data available immediately on process creation, you can patch its initial state. A common pattern is to use a flag to ensure this sync only runs once, as shown in this example for a token's Balances and TotalSupply.lua-- Place this logic at the top level of your process script,\\n-- outside of specific handlers, so it runs on load.\\nBalances = { token1 = 100, token2 = 200 } -- A table of balances\\nTotalSupply = 1984 -- A single total supply value\\n-- 1. Initialize Flag:\\n-- Initializes a flag if it doesn't exist.\\nInitialSync = InitialSync or 'INCOMPLETE'\\n-- 2. Check Flag:\\n-- Checks if the sync has already run.\\nif InitialSync == 'INCOMPLETE' then\\n-- 3. Patch State:\\n-- The `Send` call patches the state, making it available at endpoints like:\\n-- /cache/balances\\n-- /cache/totalsupply\\nSend({ device = 'patch@1.0', cache = { balances = Balances, totalsupply = TotalSupply } })\\n-- 4. Update Flag:\\n-- Updates the flag to prevent the sync from running again.\\nInitialSync = 'COMPLETE'\\nprint(\\\"Initial state sync complete. Balances and TotalSupply patched.\\\")\\nend This pattern makes essential data queryable upon process creation, boosting application responsiveness.Example (Lua in aos) This handler exposes a currentstatus key that can be read via HTTP after the PublishData action is called.Avoiding Key Conflicts Keys in the cache table become URL path segments. To avoid conflicts with reserved HyperBEAM paths, use descriptive, specific keys. Avoid using reserved keywords such as:For instance, prefer a key like myappstate over a generic key like state.WARNING HTTP paths are case-insensitive. While the patch device stores keys with case sensitivity (e.g., MyKey vs mykey), HTTP access to paths like the following is ambiguous and may lead to unpredictable results.To prevent conflicts, always use lowercase keys in your cache table (e.g., mykey, usercount).HyperBEAMGET /~process@1.0/cache/mykey Key Points Path Structure: Data is exposed at a path structured like this, where is a key from your cache table:HyperBEAM/~process@1.0/cache/ Data Types: Basic data types like strings and numbers work best. Complex objects may require serialization.compute vs now: Accessing patched data can be done via two main paths:The compute endpoint serves the last known value quickly, while now may perform additional computation to get the most recent state.Read-Only Exposure: Patching is for efficient reads and does not replace your process's core state management logic.Using the patch device enables efficient, standard HTTP access to your process state, seamlessly connecting decentralized logic with web applications.Now that you know how to expose static state, learn how to perform on-the-fly computations on that state by reading dynamic state.\",\"domain\":\"ao\",\"relevanceScore\":10,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ao-llms.txt\"},{\"content\":\"# 5. Wayfinder Protocol - ARIO Docs\\n\\nDocument Number: 5\\nSource: https://docs.ar.io/concepts/wayfinder\\nWords: 1088\\nQuality Score: 0.563\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nOverview The Wayfinder protocol is a URI scheme designed to translate requests for Arweave content into https:// requests. Essentially, Wayfinder allows for transforming traditional Arweave URLs like https://arweave.net/long-txid into more concise and user-friendly forms such as ar://txid or ar://arns-name. When combined with the AR.IO WayFinder browser extension, the request can be directed to any number of functional AR.IO Gateways to serve the content.Browser Integration The Wayfinder Protocol is currently facilitated via the WayFinder App or internal application integration. The intention is to lead popular web browsers like Chrome and Brave towards a direct integration of the Wayfinder Protocol, similar to recent integrations of the ipfs:// protocol. Such integration would remove the need for a client-side extension and boost developers' confidence in embedding Wayfinder Protocol URLs in their websites.Internal Application Integration Certain websites or apps may want to resolve Arweave Transaction ID's (TxId) internally. In these scenarios, they can process the Wayfinder Protocol internally without depending on browser support or the WayFinder App. A prime example is opensea.io. Opensea, an NFT marketplace, frequently imports NFT metadata from external sources. If metadata employs the Wayfinder Protocol, Opensea internally resolves these, presenting content without redirecting users through an https:// link.There are two main approaches to resolving Wayfinder Protocol URLs:Convert Wayfinder into a request directed at a predefined Arweave gateway.Retrieve a list of active AR.IO Gateways from the GAR by reading the contract state, or other available resources, and then fetch content from a gateway on the list.Each strategy has its benefits and challenges, necessitating careful evaluation based on specific use cases.Benefits of Wayfinder Over Hardcoded Gateway Links Using the Wayfinder Protocol offers several advantages over hardcoded links to a specific gateway:Flexibility: Wayfinder links can be routed through any available AR.IO Gateway, ensuring content remains accessible even if a specific gateway is down or congested.Decentralization: By not being tied to a single gateway, the Wayfinder Protocol embodies the decentralized spirit of the web, reducing potential censorship points.Ease of Maintenance: Developers and content creators don't need to modify links if a gateway changes its URL or becomes unavailable. The WayFinder extension handles routing to an active gateway.Consistency: Users always receive the same content, regardless of the gateway used, ensuring a consistent user experience.Use Cases Decentralized Web Hosting with Flexible Access With Wayfinder, not only can websites be hosted on the Arweave network, but their accessibility is also enhanced. By using the Wayfinder Protocol, web developers can ensure that if a specific AR.IO Gateway is down, the content can still be accessed through another gateway, offering a more reliable and resilient user experience.Digital Archives and Preservation with Enhanced Sharing Digitally archiving public domain works, especially in light of events like \\\"banned books week\\\", becomes more efficient with Wayfinder. Historical institutions or enthusiasts can easily share specific Wayfinder links to documents or media. Unlike hardcoded links which might break if a specific gateway goes offline, Wayfinder ensures that the content remains consistently accessible.Media Sharing Platforms with Consistent Content Delivery For platforms hosting user-generated content, the Wayfinder Protocol provides not just decentralized hosting but also a guarantee of content delivery. Even if a content piece becomes viral and one gateway gets congested, Wayfinder ensures that users can still access the content through another gateway, providing a seamless experience.Decentralized Applications (DApps) with Reliable Front-End Accessibility DApps, while benefiting from Arweave's permanent hosting, can further ensure their front-end remains consistently accessible to users by using Wayfinder. If a DApp's front-end is accessed frequently, causing strain on one gateway, Wayfinder can help ensure the load is distributed, and the DApp remains online and functional.How it Works Transaction ID To access content tied to an Arweave Transaction ID (TxId), simply append the TxId to ar://:ar://qI19W6spw-kzOGl4qUMNp2gwFH2EBfDXOFsjkcNyK9A Inputting this into a WayFinder-equipped browser will route your request through the right AR.IO Gateway, translating it as per your Routing Method settings.ArNS Fetching content via an Arweave Name System (ArNS) name is straightforward. Attach the ArNS name to ar://:ar://good-morning The Wayfinder protocol, along with the WayFinder App, discerns between TxIds and ArNS names. Once the suitable https:// request is formulated, the chosen gateway translates the ArNS name based on the ArNS aoComputer contract.Wayfinder App The AR.IO WayFinder App is a browser extension designed to facilitate the resolving of ar:// urls.v0.0.10 As of v0.0.10, Wayfinder supports the resolution of TXT records to Arweave content on top level domains. This innovative feature leverages DNS TXT records to associate Arweave transaction IDs with human-readable domain names, facilitating intuitive and memorable access to permaweb content. By simply entering an ar:// URL with a domain name, the Wayfinder App resolves the corresponding Arweave transaction ID through DNS TXT records, redirecting users directly to the content hosted on the Arweave network.Setup: Owners of a domain can set a TXT record for that domain following the format ARTX . Wayfinder Redirection: With a TXT record set properly, whenever a user (who has Wayfinder installed) enters an ar:// URL containing a domain name (e.g., ar://example.com), the Wayfinder App performs a DNS lookup for that TXT record in order to redirect to the Arweave content. The lookup is completed through a secure DNS-over-HTTPS query to ensure privacy and integrity.Dynamic Content Resolution: After retrieving the TXT record, the Wayfinder App extracts that Arweave transaction ID and dynamically redirects the user to the content on the permaweb. This process is transparent to the user, providing a seamless experience as if accessing a traditional website.Key Features Gasless: TXT records can be set without any onchain transactions that would require gas fees.Easy Integration: Domain owners can easily link their permaweb content to their domains, making it accessible through a simple ar:// URL.Dyncamic Content Access: Content links can be updated in real-time through DNS TXT records, without requiring any changes to the ar:// URL itself.Enhanced User Experience: Offers users a familiar and easy-to-remember way to access permaweb content, leveraging standard web domain names.Security and Privacy: Secure DNS-over-HTTPS queries for DNS lookups protect user privacy and enhances security.Use Cases Branded Content Access: Companies and individuals can brand their permaweb content, making it accessible through their domain, enhancing brand visibility and user trust.Dynamic Content Updates: Domain owners can easily update what Permaweb content their AR:// URL resolves to, which is ideal for frequently updated resources like documents, blogs, and application interfaces.Educational and Informational Resources: Educational institutions and information providers can make their resources permanently available on the permaweb, accessible through simple, memorable URLs.This feature marks a significant advancement in making decentralized content more accessible and user-friendly, bridging the gap between traditional internet usability and the permaweb's permanence and censorship-resistant nature.\",\"domain\":\"ario\",\"relevanceScore\":10,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ario-llms.txt\"},{\"content\":\"# 26. Introduction - ARIO Docs\\n\\nDocument Number: 26\\nSource: https://docs.ar.io/introduction\\nWords: 591\\nQuality Score: 0.505\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nTL;DR AR.IO seeks to create a decentralized and incentivized cloud network aimed at attracting more gateways to the Arweave network therefore making the permanent web more accessible to all.\\nAt the core of AR.IO's incentivization mechanism is the ARIO Token, a utility token used for joining the network, payments, gateway accountability, and protocol incentives.\\nThe network features modular and composable gateway infrastructure in addition to the Arweave Name System (ArNS) – a system for assigning friendly domain names to permanent data.What is AR.IO AR.IO is the world's first permanent cloud network, providing the infrastructure to ensure data, applications, and digital identities are timeless, tamper-proof, and universally accessible.\\nBuilt on the foundation of the Arweave storage network, AR.IO forms a global ecosystem of gateways, protocols, and services that connect users to the permaweb – a web where information is permanent and free from centralized control.The AR.IO Network is an open, distributed, and ownerless system, supported by operators, developers, and end-users from around the world.\\nIt's decentralized nodes, known as AR.IO Gateways, act as \\\"Permanent Cloud Service Providers\\\" delivering the critical services needed to read, write, index and query data stored on the permaweb.\\nThese gateways provide a unified, resilient interface between users and the permaweb, featuring a permanent domain name system and seamless, location-independent access to permanent storage and applications.Gateways operate using standardized protocols to maintain consistency across the network.\\nThey also engage in an observation and reporting protocol to monitor performance and ensure accountability, helping to maintain a healthy and reliable ecosystem.The AR.IO Network is powered by a utility token, ARIO, which drives the network's functionality and accessibility.\\nARIO serves as a currency for services such as the Arweave Name System (ArNS), staking to join the network as a gateway operator, delegated staking, and as rewards for contributing to the network's performance and reliability.Together, these elements form the backbone of a permanent cloud network designed to preserve data and expand the possibilities of the web.Why AR.IO?Arweave (a Layer 1 blockchain network) offers scalable and permanent onchain data storage in a sustainable manner.\\nIt does this by incentivizing miner nodes through a tokenomic endowment model which ensures data is globally stored and replicated for hundreds of years without the need for continual payment or maintenance by its uploader.However, the Arweave protocol does not incorporate all the needs of modern applications like data indexing, querying, retrieval, and other vital services.\\nConsequently, over the past few years, infrastructure services have been independently developed and deployed to meet the demands of the permaweb at scale.\\nUsers and apps have come to rely on these gateway utilities, but they are closed source, have complex codebases, and are expensive to operate.Arweave does not offer any tokenomic incentives to offset the expenses associated with operating a gateway, which has led to the community's reliance on a single centrally controlled gateway subsidized for the betterment of the network: arweave.net.\\nWhile arweave.net currently caches and indexes the entire weave with a high quality of service, it is a single bottleneck and point of failure for the whole ecosystem.AR.IO seeks to reduce the barriers of entry and attract more gateway operators to the permaweb with the goal of further enhancing its overall health, resiliency, and functionality through decentralized mechanisms that are as trustless as possible.The solution will be applied in two directions:By reducing gateway overhead costs with open source, efficient, modular networked architecture.By creating an economic incentive layer with the ARIO Token.The overall goal of this white paper is to present the framework for a healthy and sustainable decentralized gateway network.\",\"domain\":\"ario\",\"relevanceScore\":10,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/ario-llms.txt\"},{\"content\":\"# 3. Github Action  Cooking with the Permaweb\\n\\nDocument Number: 3\\nSource: https://cookbook.arweave.net/guides/deployment/github-action.html\\nWords: 537\\nQuality Score: 0.587\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nGithub Action WARNING This guide is for educational purposes only, and you should use to learn options of how you might want to deploy your application. In this guide, we are trusting a 3rd party resource github owned by microsoft to protect our secret information, in their documentation they encrypt secrets in their store using libsodium sealed box, you can find more information about their security practices here. https://docs.github.com/en/actions/security-guides/encrypted-secrets Github Actions are CI/CD pipelines that allows developers to trigger automated tasks via events generated from the github workflow system. These tasks can be just about anything, in this guide we will show how you can use github actions to deploy your permaweb application to the permaweb using Irys and ArNS.TIP This guide requires understanding of github actions, and you must have some ArNS Test Tokens, go to https://ar.io/arns/ for more details.WARNING This guide does not include testing or any other checks you may want to add to your production workflow.Create deploy script A deploy script is a script that does the heavy lifting of deploying your application, we will use @irys/sdk and warp-contracts to publish our application and register the newly published application on ArNS.Install deploy dependencies npm install --save-dev @permaweb/arx\\nnpm install --save-dev warp-contracts\\nnpm install --save-dev arweave Create deploy.mjs file import Arx from \\\"@permaweb/arx\\\";\\nimport { WarpFactory, defaultCacheOptions } from \\\"warp-contracts\\\";\\nimport Arweave from \\\"arweave\\\";\\nconst ANT = \\\"[YOUR ANT CONTRACT]\\\";\\nconst DEPLOY_FOLDER = \\\"./dist\\\";\\nconst TURBO_NODE = \\\"https://turbo.ardrive.io\\\";\\nconst jwk = JSON.parse(Buffer.from(process.env.PERMAWEB_KEY, \\\"base64\\\").toString(\\\"utf-8\\\"));\\nconst arweave = Arweave.init({ host: \\\"arweave.net\\\", port: 443, protocol: \\\"https\\\" });\\nconst arx = new Arx({ url: TURBO_NODE, token: \\\"arweave\\\", key: jwk });\\nconst warp = WarpFactory.custom(arweave, defaultCacheOptions, \\\"mainnet\\\").useArweaveGateway().build();\\nconst contract = warp.contract(ANT).connect(jwk);\\n// upload folder\\nconst result = await arx.uploadFolder(DEPLOY_FOLDER, {\\nindexFile: \\\"index.html\\\",\\n});\\n// update ANT\\nawait contract.writeInteraction({\\nfunction: \\\"setRecord\\\",\\nsubDomain: \\\"@\\\",\\ntransactionId: result.id,\\n});\\nconsole.log(\\\"Deployed Cookbook, please wait 20 - 30 minutes for ArNS to update!\\\");Add script to package.json Create a new script property called deploy, call the build script, then call node deploy.mjs in the value of the scripts deploy property.package.json...\\n\\\"scripts\\\": {\\n\\\"dev\\\": \\\"vuepress dev src\\\",\\n\\\"build\\\": \\\"vuepress build src\\\",\\n\\\"deploy\\\": \\\"yarn build && node deploy.mjs\\\"\\n},\\n...Create github action Create a deploy.yml file in the .github/workflows folder, this file instructs github actions to deploy when a push event is triggered on the main branch.name: publish\\non:\\npush:\\nbranches:\\n- \\\"main\\\"\\njobs:\\npublish:\\nruns-on: ubuntu-latest\\nsteps:\\n- uses: actions/checkout@v2\\n- uses: actions/setup-node@v1\\nwith:\\nnode-version: 18.x\\n- run: yarn\\n- run: yarn deploy\\nenv:\\nKEY: ${{ secrets.PERMAWEB_KEY }} Summary In the project repo, go to the settings and secrets, add a new secret to the repostiory, this secret will be called PERMAWEB_KEY for this project. The value of the secret should be the base64 encode string of the deployment wallet.base64 -i wallet.json | pbcopy In order for this deployment to work, you will need to fund this wallets Irys account, make sure there is some $AR in the wallet you will be using, not much, maybe.5 AR, then use the Irys cli to fund.arx fund 250000000000 -w wallet.json -t arweave WARNING Keep this wallet low on funds and only use it for this project.🎉 You have setup a github action to completely automate your deploy to permaweb!\",\"domain\":\"arweave\",\"relevanceScore\":9,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/arweave-llms.txt\"},{\"content\":\"# 6. Atomic Tokens  Cooking with the Permaweb\\n\\nDocument Number: 6\\nSource: https://cookbook.arweave.net/guides/atomic-tokens/intro.html\\nWords: 355\\nQuality Score: 0.567\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nAtomic Tokens ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.What is an Atomic Token?Check out the concept Creating an Atomic Token INFORMATION For this example, we are using a SWT Contract Source that is already published on the network. x0ojRwrcHBmZP20Y4SY0mgusMRx-IYTjg5W8c3UFoNs - example.ts import Irys from '@irys/sdk'\\nimport { WarpFactory } from 'warp-contracts'\\nasync function main() {\\nconst wallet = JSON.parse(await import('fs')\\n.then(fs => fs.readFileSync('./wallet.json', 'utf-8')))\\nconst irys = new Irys({ 'https://node2.irys.xyz', 'arweave', wallet })\\nconst warp = WarpFactory.forMainnet()\\nconst data = `# Hello Permaweb!\\n`\\nconst tags = [\\n{ name: 'Content-Type', value: 'text/html' },\\n// ANS-110 Tags\\n{ name: 'Type', value: 'web-page' },\\n{ name: 'Title', value: 'My first permaweb page' },\\n{ name: 'Description', value: 'First permaweb page by Anon' },\\n{ name: 'Topic:Noob', value: 'Noob' },\\n// SmartWeave Contract\\n{ name: 'App-Name', value: 'SmartWeaveContract' },\\n{ name: 'App-Version', value: '0.3.0' },\\n{ name: 'Contract-Src', value: 'x0ojRwrcHBmZP20Y4SY0mgusMRx-IYTjg5W8c3UFoNs' },\\n{\\nname: 'Init-State', value: JSON.stringify({\\nbalances: {\\n'cHB6D8oNeXxbQCsKcmOyjUX3UkL8cc3FbJmzbaj3-Nc': 1000000\\n},\\nname: 'AtomicToken',\\nticker: 'ATOMIC-TOKEN',\\npairs: [],\\ncreator: 'cHB6D8oNeXxbQCsKcmOyjUX3UkL8cc3FbJmzbaj3-Nc',\\nsettings: [['isTradeable', true]]\\n})\\n}\\n]\\nconst { id } = await irys.upload(data, { tags })\\nawait warp.createContract.register(id, 'node2')\\nconsole.log('Atomic Token: ', id)\\n}\\nmain() In this example, we are creating a data-item and uploading the item to the bundler network service. Then we are registering our contract with the Warp sequencer. By using bundler to publish our data-item and registering with the Warp sequencer, our data is immediately available on the gateway service and our contract is immediately able to accept interactions.Run Example npm install @irys/sdk warp-contracts\\nnpm install typescript ts-node\\nnpx ts-node example.ts INFORMATION ANS-110 is an Asset Discovery Specification to allow for composability with the Permaweb Application ecosystem.Summary This is a simple example of deploying an Atomic Asset, for more detailed examples check out: https://atomic-assets.arweave.dev Working with Tokens SmartWeave Contracts can not hold AR the native coin of the Arweave Network. AR is used to purchase storage for data on the Arweave Network and it can be transferred from a source wallet to a target wallet on the Arweave network, but it can not be held in a SmartWeave contract.\",\"domain\":\"arweave\",\"relevanceScore\":9,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/arweave-llms.txt\"},{\"content\":\"# 10. Warp (SmartWeave) SDK - Deploying Contracts  Cooking with the Permaweb\\n\\nDocument Number: 10\\nSource: https://cookbook.arweave.net/guides/smartweave/warp/deploying-contracts.html\\nWords: 726\\nQuality Score: 0.527\\nExtraction Method: defuddle_semantic\\nExtraction Reason: defuddle_returned_html_semantic_extracted\\n\\nWarp (SmartWeave) SDK - Deploying Contracts ⚠️ Deprecation Notice This document is deprecated and may contain outdated information.SmartWeave Contracts are created by posting two transactions to the network, a Source Transaction and a Initial State Transaction, the source transaction contains the source code the contract will use to determine the current state. The initial state transaction provides a contract identifer to reference as well as the initial seed data the contract should use as the starting point to evaluate the current state. The current state is calculated by accessing actions that are transactions written to the network that contain input parameters to execute using the evaluated and instantiated source code. Warp Contracts can be created using many different languages and can be evaluated using the Warp SDK. This guide will show the many different ways you can deploy a Warp Contract.TIP If you would like to Warp SmartWeaveContracts, checkout the Warp Academy! https://academy.warp.cc/ As of Warp version 1.3.0 you willl need a plugin to deploy contracts with Warp. This plugin will enable you to add different wallet signatures.import { DeployPlugin, InjectedArweaveSigner } from 'warp-contracts-plugin-deploy'\\nimport { WarpFactory } from 'warp-contracts'\\nconst warp = WarpFactory.forMainnet().use(new DeployPlugin())\\n...\\nfunction deploy(initState, src) {\\nif (window.arweaveWallet) {\\nawait window.arweaveWallet.connect(['ACCESS_ADDRESS', 'SIGN_TRANSACTION', 'ACCESS_PUBLIC_KEY', 'SIGNATURE']);\\n}\\nconst userSigner = new InjectedArweaveSigner(window.arweaveWallet);\\nawait userSigner.setPublicKey();\\nreturn warp.deploy({\\nwallet: userSigner,\\nsrc,\\ninitState: JSON.stringify(initState)\\n})\\n} The Four ways to deploy a Warp SmartWeave Contract There are 4 ways you can deploy a SmartWeaveContract via the Warp SDK, these options handle different use cases that a developer may encounter.Need to deploy the contract with the source at the same time Need to deploy a contract where the source is already on the permaweb Need to deploy a contract through the sequencer and point it to some data using a path manifest Need to deploy a contract via Irys and register that contract on the sequencer TIP For more information about Warp deployments check out the github Readme for the project. https://github.com/warp-contracts/warp#deployment.WARNING This project is in rapid development, so the documentation here could be out of data quickly, if you discover it is out of date, please let us know on the Permaweb Cookbook Discord Channel.Examples TIP By default all deploy functions are published to Arweave via Irys, each option has a flag that can be set to not use Irys, but it can take many confirmations for the network to fully confirm the transaction.deploy Deploys contract plus source code to Warp Sequencer, to Irys (L2), to Arweave.const { contractTxId, srcTxId } = await warp.deploy({\\nwallet,\\ninitState,\\ndata: { \\\"Content-Type\\\": \\\"text/html\\\", body: \\\"# Hello World\\n\\\" },\\nsrc: contractSrc,\\ntags: [{ name: \\\"AppName\\\", value: \\\"HelloWorld\\\" }],\\n});wallet - should be Arweave keyfile (wallet.json) parsed as a JSON object implementing the JWK Interface or the string 'use_wallet' initState - is a stringified JSON object data - is optional if you want to write data as part of your deployment src - is the string or Uint8Array value of the source code for the contract tags - is an array of name/value objects {name: string, value: string}[], deployFromSourceTx Already have the source on the permaweb? Then deployFromSourceTx is your tool of choice! With the permaweb you never have to worry about data changing so re-using source code for contracts is a no brainer.const { contractTxId, srcTxId } = await warp.deployFromSourceTx({\\nwallet,\\ninitState,\\nsrcTxId: \\\"SRC_TX_ID\\\",\\n});deployBundled Uses Warp Gateway Sequencer's endpoint to upload a raw data item to Irys and index it.import { createData } from \\\"arbundles\\\";\\nconst dataItem = createData(\\nJSON.stringify({\\nmanifest: \\\"arweave/paths\\\",\\nversion: \\\"1.0\\\",\\nindex: {\\npath: \\\"index.html\\\",\\n},\\npaths: {\\n\\\"index.html\\\": {\\nid: \\\"cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI\\\",\\n},\\n},\\n}),\\n{ tags: [{ \\\"Content-Type\\\": \\\"application/x.arweave-manifest+json\\\" }] },\\n);\\nconst { contractTxId } = await warp.deployBundled(dataItem.getRaw());register Uses Warp Gateway Sequencer's endpoint to index a contract that has been uploaded with Irys.Summary Why are there so many options to deploy contracts? These methods exist to reduce duplication, enable advanced contract interactions, and allow for flexibility for testing and usage of the smartweave protocol. The permaweb is very unique in its architecture, it provides a feature where you can deploy both digital data and the contract to manage that data generating the same transaction identifier. The result is dynamic data paired with an immutable set of data. Deploying contracts is just one piece of the Warp SDK, to learn more keep reading this guide!\",\"domain\":\"arweave\",\"relevanceScore\":9,\"isFullDocument\":true,\"type\":\"permaweb_docs\",\"url\":\"https://fuel_permawebllms.permagate.io/arweave-llms.txt\"}],\"success\":true,\"totalResults\":20}",
        "type" : "text"
      }
    ]
  },
  "expected" : [
    {
      "callsTool" : {
        "arguments" : {
          "query" : "arlink"
        },
        "name" : "queryPermawebDocs"
      }
    }
  ]
}